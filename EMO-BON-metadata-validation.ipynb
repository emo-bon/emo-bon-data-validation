{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c01fa68b-1e71-4a5d-a68a-77b4d6e810f7",
   "metadata": {},
   "source": [
    "# Pydantic validation framework for the EMO BON observatory and other metadata log sheets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bb619d-5b92-4bee-b1a5-b3f0cb364f70",
   "metadata": {},
   "source": [
    "- **pydantic** Data validation using Python type hints.\n",
    "    - [pypi](https://pypi.org/project/pydantic/)\n",
    "    - [Documentation](https://docs.pydantic.dev/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bca054-8458-4137-ae94-b960bbe0ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Weird stuff from JupyterHub after I moved modules and notebooks around:\n",
    "# For some reasong CWD is /src/scratch even though this notebook is in /srv/scratch/emo-bon-validation\n",
    "# The terminal also show us to be in /srv/scratch/emo-bon-validation\n",
    "# So...\n",
    "if os.getcwd() == \"/srv/scratch\":\n",
    "    os.chdir(\"./emo-bon-data-validation\")\n",
    "print(f\"CWD is {os.getcwd()}\")\n",
    "\n",
    "import importlib\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pydantic\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f0a524-4ac9-4840-ba77-b031296619fa",
   "metadata": {},
   "source": [
    "##### Init the directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d4afacf-09d3-47dc-80cc-e00d3e9232c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Init dirs and paths, write csv files\n",
    "    # Init the validation classes dir if needed\n",
    "    # Note that __init__.py will need be edited manually to import the validators\n",
    "    # e.g from .observatories import Model as observatoriesModel\n",
    "    validation_classes_path = \"./validation_classes\"\n",
    "    if True:\n",
    "        if not os.path.exists(validation_classes_path):\n",
    "            os.mkdir(validation_classes_path)\n",
    "            Path(os.path.join(validation_classes_path, \"__init__.py\")).touch()\n",
    "            os.mkdir(raw_files_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e601478-27ea-404a-8371-1405051ccf22",
   "metadata": {},
   "source": [
    "## Governance data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e34558-fb7c-4000-8e8b-24ad71df7eab",
   "metadata": {},
   "source": [
    "#### Read each of the governance CSV files into a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ba717-c9a9-43a0-b34e-8b7461952135",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "github_path = \"https://raw.githubusercontent.com/emo-bon/governance-data/main/\"\n",
    "file_names = [\n",
    "    \"logsheets.csv\",  # contain the URLs of the googlesheets that are the logsheets\n",
    "    \"observatories.csv\",  # contain information about each observatory\n",
    "    # \"organisations.csv\",         # contain information about the organisations in EMO BON\n",
    "    # \"planned_events.csv\"         # contains information about planned EMO BON events (this file is only used by humans, not by any actions) - DONT CARE\n",
    "    # \"ro-crate-metadata.json\"     # IGNORE\n",
    "]\n",
    "\n",
    "dfs = {}\n",
    "for f in file_names:\n",
    "    df = pd.read_csv(os.path.join(github_path, f))\n",
    "    print(f\"This is info() for {df.info()}\")\n",
    "    dfs[f] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2484615f-cab2-48a2-8ca7-999bab0051e3",
   "metadata": {},
   "source": [
    "#### Validate Governance tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8dc50e0-2d84-4508-8d82-67b90397c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from validation_classes import observatoriesModel, logsheetsModel\n",
    "\n",
    "validator_class_paths = {\n",
    "    \"logsheets.csv\": logsheetsModel,\n",
    "    \"observatories.csv\": observatoriesModel,\n",
    "}\n",
    "validation_classes_path = \"./validation_classes\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa819fa6-c492-481f-bc81-218057699342",
   "metadata": {},
   "source": [
    "##### Observatories table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af3dfa6-8d4c-4749-b9d1-bc00834e1a3b",
   "metadata": {},
   "source": [
    "The observatories validator mostly changes the column names to make them consistent (and spelled correctly), removes blank strings (\"   \") from cells, and reformats the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aa2840b-9b25-4155-bd43-34cea9c81de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"observatories.csv\"\n",
    "data = dfs[file_name]  # dfs is dict of pandas df's\n",
    "validator = validator_class_paths[file_name]\n",
    "data_records = data.to_dict(orient=\"records\")\n",
    "validated_rows = [validator(**row).model_dump() for row in data_records]\n",
    "\n",
    "ndf = pd.DataFrame.from_records(validated_rows, index=\"observatory_id\")\n",
    "ndf.to_csv(os.path.join(\"governance\", \"observatories_validated.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ef5ade-e3f4-4742-b1ba-41233e93e80c",
   "metadata": {},
   "source": [
    "##### Logsheets table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b9e1b9e-8a05-4bf2-aceb-55c5c740e77d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cdir = os.getcwd()\n",
    "file_name = \"logsheets.csv\"\n",
    "data = dfs[file_name]  # dfs is dict of pandas df's\n",
    "validator = validator_class_paths[file_name]\n",
    "data_records = data.to_dict(orient=\"records\")\n",
    "validated_rows = [validator(**row).model_dump() for row in data_records]\n",
    "\n",
    "ndf = pd.DataFrame.from_records(validated_rows, index=\"observatory_id\")\n",
    "ndf.to_csv(os.path.join(\"governance\", \"logsheets_validated.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fc138c-c96f-4349-843c-ad02276b5f43",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Validate logsheets from \"water column\" and \"soft sediments\" for the \"sampling\" and \"measured\" tables\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b0343f-e36b-4685-ac17-e90a9e53f12d",
   "metadata": {},
   "source": [
    "##### Pulls from the raw Google Sheets does a \"lax\" validation where we correct/coerce everything to a consistent type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b41f0d-7d88-46e4-9789-442036904749",
   "metadata": {},
   "source": [
    "### !!! A note about Pandas and integer fields with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b45ba77-48d0-422c-940c-a0c1b396f16a",
   "metadata": {},
   "source": [
    "Pandas will read a raw CSV file and try to determine the type while doing so. If it finds an integer column with missing values they will be NaN's, which of course are floats. Consequently, the default action here is to read the column as a floats by coercing the integer values to float to match the NaNs - this is rarely what you want.\n",
    "\n",
    "However, Pandas does have a [nullable integer type](https://pandas.pydata.org/docs/user_guide/integer_na.html) - `pandas.Int64Dtype()` or it's string alias `\"Int64\"`. You can force Pandas to use this type when reading the CSV file using `pandas.read_csv('file.csv', dtype={\"<int field with missing values>\": \"Int64\"})` the NaN's will be changed to [pandas.NA types](https://pandas.pydata.org/docs/reference/api/pandas.NA.html#pandas.NA).\n",
    "\n",
    "For clarity: this is NOT what happens below: we let the validators deal with it, which coerce the now floats back into ints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95e6614-c9ff-470d-8363-8f37a08d34ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import pydantic\n",
    "from validation_classes import samplingModel, measuredModel\n",
    "\n",
    "\n",
    "def get_sheet(\n",
    "    sheet_type: str, sheet_link: str, format_type: str = \"json\"\n",
    ") -> pd.core.frame.DataFrame:\n",
    "    \"\"\"Returns a Pandas dataframe of the 'sampling' or 'measured' sheets\n",
    "    from the observatories' Google Sheets.\n",
    "\n",
    "    CSV has a problem with the word \"blank\" in the replicated field.\n",
    "    But none of the others work because of the header (not sure why only\n",
    "    CSV doesnt have the header.\n",
    "\n",
    "    TODO sort out the header parsing from the Google Sheet response.\n",
    "    \"\"\"\n",
    "    sampling_sheet_base = sheet_link.split(\"/edit\")[0]\n",
    "    if format_type == \"json\":\n",
    "        # should return json\n",
    "        sampling_sheet_suffix = f\"/gviz/tq?tqx=sheet={sheet_type}\"\n",
    "        sample_sheet_link = sampling_sheet_base + sampling_sheet_suffix\n",
    "        print(f\"Sample sheet link: {sample_sheet_link}\")\n",
    "        df = pd.read_json(sample_sheet_link)\n",
    "\n",
    "    elif format_type == \"csv\":\n",
    "        sampling_sheet_suffix = f\"/gviz/tq?tqx=out:csv&sheet={sheet_type}\"\n",
    "        sample_sheet_link = sampling_sheet_base + sampling_sheet_suffix\n",
    "        print(f\"Sample sheet link: {sample_sheet_link}\")\n",
    "\n",
    "        # Note that even if we force the replicate field to be a string, it doesnt\n",
    "        # recognise \"blank\" as a string, it's still None\n",
    "        # df = pd.read_csv(sample_sheet_link, encoding='utf-8', dtype={\"replicate\": str})\n",
    "        # If we force them to ints the NaNs become NA as noted above\n",
    "        # df = pd.read_csv(sample_sheet_link, encoding='utf-8', dtype={\"replicate\": int})\n",
    "\n",
    "        # Here we don't force ints with NaNs to \"Int64\", but\n",
    "        # let the later validator coerce floats and ints to string | None\n",
    "        df = pd.read_csv(sample_sheet_link, encoding=\"utf-8\")\n",
    "\n",
    "    elif format_type == \"excel\":\n",
    "        sampling_sheet_suffix = f\"/gviz/tq?tqx=out:tsv-xlsx&sheet={sheet_type}\"\n",
    "        sample_sheet_link = sampling_sheet_base + sampling_sheet_suffix\n",
    "        print(f\"Sample sheet link: {sample_sheet_link}\")\n",
    "        df = pd.read_excel(sample_sheet_link, engine=\"openpyxl\")\n",
    "    elif format_type == \"json\":\n",
    "        sampling_sheet_suffix = f\"/gviz/tq?tqx=out:json&sheet={sheet_type}\"\n",
    "        sample_sheet_link = sampling_sheet_base + sampling_sheet_suffix\n",
    "        print(f\"Sample sheet link: {sample_sheet_link}\")\n",
    "        df = pd.read_json(sample_sheet_link)\n",
    "    elif format_type == \"tsv\":\n",
    "        sampling_sheet_suffix = f\"/gviz/tq?tqx=out:tsv&sheet={sheet_type}\"\n",
    "        sample_sheet_link = sampling_sheet_base + sampling_sheet_suffix\n",
    "        print(f\"Sample sheet link: {sample_sheet_link}\")\n",
    "        df = pd.read_csv(sample_sheet_link, sep=\"\\t\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unrecognised {format_type=}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_sample_sheets(\n",
    "    sampling_strategy: str, sheet_type: str, addresses: list[str, str]\n",
    ") -> None:\n",
    "\n",
    "    for observatory in addresses:\n",
    "        observatory_id, sheet_link = observatory\n",
    "        # print(f\"Observatory_id {observatory_id} sheet_link {sheet_link}\")\n",
    "        if not isinstance(sheet_link, str):\n",
    "            # print(f\"This is the sheet_link type {type(sheet_link)}\")\n",
    "            if isinstance(sheet_link, float):\n",
    "                if math.isnan(sheet_link):\n",
    "                    print(\n",
    "                        f\"Observatory {observatory_id} lacks valid sheet URL for {sampling_strategy}\"\n",
    "                    )\n",
    "                    continue\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Unknown link {sheet_link} to observatory {observatory_id}\"\n",
    "                )\n",
    "        else:\n",
    "\n",
    "            if observatory_id == \"Plenzia\":\n",
    "                continue  # Sheets not publically available\n",
    "            # UMF soft_sed has two source_mat_ids\n",
    "            if sampling_strategy == \"soft_sediment\" and observatory_id == \"UMF\":\n",
    "                continue\n",
    "\n",
    "            # if observatory_id not in [\"OSD74\", \"AAOT\"]:\n",
    "            #     continue\n",
    "            # if sampling_strategy != \"water_column\":\n",
    "            #     continue\n",
    "\n",
    "            print(\n",
    "                f\"\\n\\nProcessing {observatory_id=} - {sampling_strategy=} - {sheet_type=}\"\n",
    "            )\n",
    "            # Assuming either 'sampling' or 'measured' for sheet_type\n",
    "            df = get_sheet(sheet_type, sheet_link, format_type=\"csv\")\n",
    "\n",
    "            data_records_all = df.to_dict(orient=\"records\")\n",
    "\n",
    "            # Many sheets have partially filled rows\n",
    "            # The source_mat_id is auto-formatted and the PRIMARY_KEY\n",
    "            # Therefore filter records on source_mat_id\n",
    "            def filter_on_source_mat_id(d):\n",
    "                # Bergen has it as source_material_id\n",
    "                try:\n",
    "                    value = d[\"source_mat_id\"]\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        value = d[\"source_material_id\"]\n",
    "                    except KeyError:\n",
    "                        raise ValueError(\"Cannot find source_mat_id field\")\n",
    "                if isinstance(value, float):\n",
    "                    if math.isnan(value):\n",
    "                        return False\n",
    "                elif value is None:\n",
    "                    return False\n",
    "                # Remove mis-formatted\n",
    "                elif len(value.split(\"_\")) < 6:\n",
    "                    return False\n",
    "                # Edge case of this otherwise blank entry having 6 \"bits\"\n",
    "                elif value == \"EMOBON_VB_Wa_230509_um_\":\n",
    "                    return False\n",
    "                else:\n",
    "                    return True\n",
    "\n",
    "            data_records_filtered = list(\n",
    "                filter(filter_on_source_mat_id, data_records_all)\n",
    "            )\n",
    "\n",
    "            if len(data_records_all) > len(data_records_filtered):\n",
    "                print(\n",
    "                    f\"Discarded {len(data_records_all) - len(data_records_filtered)} records leaving {len(data_records_filtered)}.\"\n",
    "                )\n",
    "\n",
    "            validator = validator_classes[sheet_type]\n",
    "            validated_rows = [\n",
    "                validator(**row).model_dump() for row in data_records_filtered\n",
    "            ]\n",
    "\n",
    "            save_dir = \"./logsheets\"\n",
    "            outfile_name = f\"{observatory_id}_{sampling_strategy}_{sheet_type}_validated.csv\"\n",
    "            ndf = pd.DataFrame.from_records(\n",
    "                validated_rows, index=\"source_mat_id\"\n",
    "            )\n",
    "            ndf.to_csv(os.path.join(save_dir, outfile_name))\n",
    "            print(f\"Written {os.path.join(save_dir, outfile_name)}\")\n",
    "\n",
    "\n",
    "validator_classes = {\"sampling\": samplingModel, \"measured\": measuredModel}\n",
    "# Get list of observatory ids\n",
    "df = pd.read_csv(\"./governance/observatories_validated.csv\")\n",
    "observatory_ids = [id[0] for id in df[[\"observatory_id\"]].values.tolist()]\n",
    "print(f\"{observatory_ids=}\")\n",
    "# Get list of all URL links to sampling sheets\n",
    "# NB  you cant use a \"with\" closure here when reading the Pandas df\n",
    "governance_logsheets_validated_csv = \"./governance/logsheets_validated.csv\"\n",
    "df = pd.read_csv(governance_logsheets_validated_csv)\n",
    "water_column_sheet_addresses = df[\n",
    "    [\"observatory_id\", \"water_column\"]\n",
    "].values.tolist()\n",
    "soft_sediment_sheet_addresses = df[\n",
    "    [\"observatory_id\", \"soft_sediment\"]\n",
    "].values.tolist()\n",
    "\n",
    "parse_sample_sheets(\"water_column\", \"sampling\", water_column_sheet_addresses)\n",
    "parse_sample_sheets(\"soft_sediment\", \"sampling\", soft_sediment_sheet_addresses)\n",
    "parse_sample_sheets(\"water_column\", \"measured\", water_column_sheet_addresses)\n",
    "parse_sample_sheets(\"soft_sediment\", \"measured\", soft_sediment_sheet_addresses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa1b378-bbd4-4403-ae00-37c5b5ba8b82",
   "metadata": {},
   "source": [
    "# Validate the Observatory sheets the EMO-BON-Metadata Google Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a1f67c-e5a8-445d-ac90-1ad4bad88180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the Observatory sheets in the EMO-BON-Metadata Google Sheets\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from pathlib import Path, PurePath\n",
    "from validation_classes import observatoryModel\n",
    "\n",
    "\n",
    "def get_sheet(sheet_link: str) -> pd.core.frame.DataFrame:\n",
    "    \"\"\"Returns a Pandas dataframe of the 'observatory' sheets\n",
    "    from the observatories' Google Sheets.\n",
    "    \"\"\"\n",
    "    print(f\"Sheet link: {sheet_link}\")\n",
    "    sampling_sheet_base = sheet_link.split(\"/edit\")[0]\n",
    "    sampling_sheet_suffix = f\"/gviz/tq?tqx=out:csv&sheet=observatory\"\n",
    "    sample_sheet_link = sampling_sheet_base + sampling_sheet_suffix\n",
    "    print(f\"Sample sheet link: {sample_sheet_link}\")\n",
    "    df = pd.read_csv(sample_sheet_link, encoding=\"utf-8\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_sample_sheets(\n",
    "    sampling_strategy: str, sheet_type: str, addresses: list[str, str]\n",
    ") -> None:\n",
    "\n",
    "    for observatory in addresses:\n",
    "        observatory_id, sheet_link = observatory\n",
    "        # print(f\"Observatory_id {observatory_id} sheet_link {sheet_link}\")\n",
    "        if not isinstance(sheet_link, str):\n",
    "            # print(f\"This is the sheet_link type {type(sheet_link)}\")\n",
    "            if isinstance(sheet_link, float):\n",
    "                if math.isnan(sheet_link):\n",
    "                    print(\n",
    "                        f\"Observatory {observatory_id} lacks valid sheet URL for {sampling_strategy}\"\n",
    "                    )\n",
    "                    continue\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Unknown link {sheet_link} to observatory {observatory_id}\"\n",
    "                )\n",
    "        else:\n",
    "\n",
    "            if observatory_id == \"Plenzia\":\n",
    "                continue  # Sheets not publically available\n",
    "            # UMF soft_sed has two source_mat_ids\n",
    "            if sampling_strategy == \"soft_sediment\" and observatory_id == \"UMF\":\n",
    "                continue\n",
    "\n",
    "            # if observatory_id not in [\"OSD74\", \"AAOT\"]:\n",
    "            #     continue\n",
    "            # if sampling_strategy != \"water_column\":\n",
    "            #     continue\n",
    "\n",
    "            print(\n",
    "                f\"\\n\\nProcessing {observatory_id=} - {sampling_strategy=} - {sheet_type=}\"\n",
    "            )\n",
    "            df: pd.core.frame.DataFrame = get_sheet(sheet_link)\n",
    "\n",
    "            # Note there is only one row per sheet\n",
    "            data_records_all = df.to_dict(orient=\"records\")\n",
    "\n",
    "            # pprint(data_records_all)\n",
    "\n",
    "            # Get the obs_id from the only row\n",
    "            obs_id = data_records_all[0][\"obs_id\"]\n",
    "            assert (\n",
    "                observatory_id == obs_id\n",
    "            ), f\"Error: {observatory_id=} != {obs_id=}\"\n",
    "\n",
    "            if len(data_records_all) != 1:\n",
    "                raise RuntimeError(f\"Error: {len(data_records_all)} != 1\")\n",
    "\n",
    "            validated_rows = [\n",
    "                observatoryModel(**row).model_dump() for row in data_records_all\n",
    "            ]\n",
    "\n",
    "            save_dir = Path(\"./logsheets\")\n",
    "            outfile_name = Path(\n",
    "                f\"{observatory_id}_{sampling_strategy}_{sheet_type}_validated.csv\"\n",
    "            )\n",
    "            ndf = pd.DataFrame.from_records(validated_rows, index=\"obs_id\")\n",
    "            out_file = PurePath(save_dir, outfile_name)\n",
    "            ndf.to_csv(out_file)\n",
    "            print(f\"Written {out_file}\")\n",
    "\n",
    "\n",
    "# Get list of observatory ids\n",
    "# df = pd.read_csv(\"./governance/observatories_validated.csv\")\n",
    "# observatory_ids = [id[0] for id in df[[\"observatory_id\"]].values.tolist()]\n",
    "# print(f\"{observatory_ids=}\")\n",
    "\n",
    "# Get list of all URL links to sampling sheets\n",
    "# NB  you cant use a \"with\" closure here when reading the Pandas df\n",
    "df = pd.read_csv(Path(\"./governance/logsheets_validated.csv\"))\n",
    "water_column_sheet_addresses = df[\n",
    "    [\"observatory_id\", \"water_column\"]\n",
    "].values.tolist()\n",
    "soft_sediment_sheet_addresses = df[\n",
    "    [\"observatory_id\", \"soft_sediment\"]\n",
    "].values.tolist()\n",
    "\n",
    "parse_sample_sheets(\"water_column\", \"observatory\", water_column_sheet_addresses)\n",
    "parse_sample_sheets(\n",
    "    \"soft_sediment\", \"observatory\", soft_sediment_sheet_addresses\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3f629d-1b29-46ad-8c81-80084f865c48",
   "metadata": {},
   "source": [
    "# Write the combined Observatory table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0068dff2-16f9-4406-b777-7bf8805894d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path, PurePath\n",
    "\n",
    "FILE_PATH = Path(\"./logsheets\")\n",
    "\n",
    "\n",
    "def filter_on_sheet_type(csv_file):\n",
    "    if Path(csv_file).stem.split(\"_\")[-2] == \"observatory\":\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "csv_files = [f for f in os.listdir(FILE_PATH) if f.split(\".\")[1] == \"csv\"]\n",
    "observatory_files = list(filter(filter_on_sheet_type, csv_files))\n",
    "# print(f\"{observatory_files}\")\n",
    "\n",
    "frames = []\n",
    "for obs in observatory_files:\n",
    "    df = pd.read_csv(PurePath(FILE_PATH, obs))\n",
    "    df[\"env_package\"].replace(\"water\", \"water_column\", inplace=True)\n",
    "    df[\"env_package\"].replace(\"sediment\", \"soft_sediment\", inplace=True)\n",
    "    frames.append(df)\n",
    "\n",
    "\n",
    "#        for col in df:\n",
    "#            if col == rank:\n",
    "#                continue\n",
    "#            for i, row_value in df[col].items():\n",
    "#                df.loc[i, col] = numpy.sqrt(row_value)\n",
    "\n",
    "combined_df = pd.concat(frames)\n",
    "outfile_name = f\"Observatory_combined_logsheets_validated3.csv\"\n",
    "combined_df.to_csv(outfile_name, index=False)\n",
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c0a641-6fa8-48d5-b3d3-0785beaf2f68",
   "metadata": {},
   "source": [
    "# Combined meta-table for each observatory of all validated logsheets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3cbb42-7bd0-4a45-9ee4-a6d30ce44758",
   "metadata": {},
   "source": [
    "The source_mat_id is the unique key or identifier that links the records in the Batch run_information sheets ([run-information-batch-001.csv](https://raw.githubusercontent.com/emo-bon/sequencing-data/main/shipment/batch-001/run-information-batch-001.csv) and [run-information-batch-002.csv](https://raw.githubusercontent.com/emo-bon/sequencing-data/main/shipment/batch-002/run-information-batch-002.csv)) to the sampling events in the \"sampling\" and \"measured\" sheets of the observatory logsheets (Google Sheets) (e.g. [ESC68N](https://docs.google.com/spreadsheets/d/11_Eu0W1-sDiuzKx1cIl6YuxjRHmWezN6u9v3Ly8JZ3A/edit?gid=0#gid=0)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33792f86-8da9-4cd5-9986-29a80aedfbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Check\n",
    "# EMOBON_BPNS_Wa_211223_3um_1\n",
    "# EMOBON_BPNS_Wa_211223_3um_2\n",
    "# EMOBON_BPNS_Wa_211223_0.2um_1\n",
    "# EMOBON_BPNS_Wa_211223_0.2um_2\n",
    "# in the combined sheet\n",
    "\n",
    "# TODO there are 25 missing source_mat_ids because of the \"blank_1\" problem - this should be\n",
    "# auto corrected if the replicate column is auto-filled to text not auto there is an open issue\n",
    "# on the github page\n",
    "\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import difflib\n",
    "import pandas as pd\n",
    "import validators\n",
    "import collections\n",
    "from pprint import pprint\n",
    "\n",
    "LOGSHEETS_PATH = \"./logsheets\"\n",
    "# These are real duplicates\n",
    "KNOWN_DUPLICATES = [\n",
    "    \"EMOBON_ROSKOGO_Wa_210618_3um_1\",\n",
    "    \"EMOBON_PiEGetxo_Wa_210824_3um_blank\",\n",
    "]\n",
    "\n",
    "\n",
    "def get_observatory_data() -> list[str, str, str]:\n",
    "    # Get list of observatory_ids\n",
    "    df = pd.read_csv(\"./governance/logsheets_validated.csv\")\n",
    "    observatory_data = df[\n",
    "        [\"observatory_id\", \"water_column\", \"soft_sediment\"]\n",
    "    ].values.tolist()\n",
    "    return observatory_data\n",
    "\n",
    "\n",
    "def get_all_refcodes() -> dict[str, str]:\n",
    "    batch1_run_info_path = \"https://raw.githubusercontent.com/emo-bon/sequencing-data/main/shipment/batch-001/run-information-batch-001.csv\"\n",
    "    batch2_run_info_path = \"https://raw.githubusercontent.com/emo-bon/sequencing-data/main/shipment/batch-002/run-information-batch-002.csv\"\n",
    "    # Get list of batch1 <source_mat_id>, <ref_code>'s\n",
    "    df = pd.read_csv(batch1_run_info_path)\n",
    "    refcodes = {}\n",
    "    for i in df[[\"source_material_id\", \"ref_code\"]].values.tolist():\n",
    "        assert i[0] not in refcodes, f\"{i[0]} maybe duplicated\"\n",
    "        refcodes.update(dict([i]))\n",
    "    # Get list of batch2 <source_mat_id>, <ref_code>'s\n",
    "    df = pd.read_csv(batch2_run_info_path)\n",
    "    b2_refcodes = {}\n",
    "    for i in df[[\"source_material_id\", \"ref_code\"]].values.tolist():\n",
    "        assert i[0] not in refcodes, f\"{i[0]} maybe duplicated\"\n",
    "        b2_refcodes.update(dict([i]))\n",
    "    refcodes.update(b2_refcodes)\n",
    "    return refcodes\n",
    "\n",
    "\n",
    "def parse_observatory_sample_type(\n",
    "    observatory_id: str,\n",
    "    obs_refcodes: dict[str, str],\n",
    "    sampling_type: str,\n",
    "    save_table: bool = False,\n",
    "    verbose: bool = True,\n",
    ") -> list[dict[str, str]]:\n",
    "    \"\"\"An observatory is an EMBRC station and it has an ID\n",
    "    Each observatory may take either or both of the \"water_column\" and \"soft_sediment\" sampling types\n",
    "    Each sampling type has both a \"sampling\" and \"measured\" sheet\n",
    "\n",
    "    This function returns a list of sampling events each of which is a dict with key/value pairs for each field and value\n",
    "    \"\"\"\n",
    "\n",
    "    sampling_data_filename = (\n",
    "        f\"{observatory_id}_{sampling_type}_sampling_validated.csv\"\n",
    "    )\n",
    "    measured_data_filename = (\n",
    "        f\"{observatory_id}_{sampling_type}_measured_validated.csv\"\n",
    "    )\n",
    "\n",
    "    sampling_data = pd.read_csv(\n",
    "        os.path.join(LOGSHEETS_PATH, sampling_data_filename)\n",
    "    )\n",
    "    measured_data = pd.read_csv(\n",
    "        os.path.join(LOGSHEETS_PATH, measured_data_filename)\n",
    "    )\n",
    "\n",
    "    sampling_events = sampling_data.to_dict(orient=\"records\")\n",
    "    measured_events = measured_data.to_dict(orient=\"records\")\n",
    "\n",
    "    # To be returned\n",
    "    source_mat_ids_from_combined_events = []  # List of all source_mat_ids\n",
    "    all_sampling_source_mat_ids = sampling_data[\"source_mat_id\"].values.tolist()\n",
    "    missing_measured_but_refcode_present = (\n",
    "        0  # Shouldn't happen if sheet is not broken\n",
    "    )\n",
    "    duplicates_ignored_counter = 0  # How many known duplicate source_mat_ids in the sheets are we ignoring\n",
    "    combined_events = []  # List of all sampling/measured events\n",
    "\n",
    "    # Internal\n",
    "    no_refcode_counter = (\n",
    "        0  # Sampling events without a refcode ie not sent to sequencing\n",
    "    )\n",
    "    source_mat_ids_in_sampling_with_refcode_missing_from_measured = (\n",
    "        []\n",
    "    )  # Again shouldn't happen\n",
    "    refcodes_in_run_info = []  # List of refcodes matched to source_mat_ids\n",
    "\n",
    "    for sampling_event in sampling_events:\n",
    "\n",
    "        # Checking consistency\n",
    "        # Does this sampling event have a ref_code\n",
    "        # If yes, then it should have both a sampling and measured sheet\n",
    "        # If no, we can ignore it\n",
    "        event_mat_id = sampling_event[\"source_mat_id\"]\n",
    "\n",
    "        # Two source_mat_id's in the Batch 1 & 2 run_informations match duplicate sampling events\n",
    "        # in the sampling sheets - here we ignore those two:\n",
    "        if event_mat_id in KNOWN_DUPLICATES:\n",
    "            duplicates_ignored_counter += 1\n",
    "            # print(f\"IGNORING DUP: {event_mat_id}\")\n",
    "            continue\n",
    "\n",
    "        # Hack for HCMR-1 replicates which are \"blank1\" and \"blank2\" in the sampling sheet\n",
    "        # become just blank in the measured and run_information sheet from where we get the refcodes\n",
    "        if event_mat_id == \"EMOBON_HCMR-1_Wa_210917_3um_blank1\":\n",
    "            event_mat_id = \"EMOBON_HCMR-1_Wa_210917_3um_blank\"\n",
    "        if event_mat_id == \"EMOBON_HCMR-1_Wa_210917_0.2um_blank1\":\n",
    "            event_mat_id = \"EMOBON_HCMR-1_Wa_210917_0.2um_blank\"\n",
    "\n",
    "        try:\n",
    "            refcode = obs_refcodes[event_mat_id]\n",
    "        except KeyError:\n",
    "            no_refcode_counter += 1\n",
    "            # OK so has not been sent to sequencing; ignore\n",
    "            continue\n",
    "\n",
    "        event_measured = False\n",
    "        for measured_event in measured_events:\n",
    "\n",
    "            try:\n",
    "                measured_event[\"source_mat_id\"]\n",
    "            except KeyError:\n",
    "                print(f\"Key error: {measured_event}\")\n",
    "                raise KeyError\n",
    "                # Should not happen\n",
    "            if measured_event[\"source_mat_id\"] == event_mat_id:\n",
    "                event_measured = copy.deepcopy(measured_event)\n",
    "                break\n",
    "\n",
    "        if not event_measured:\n",
    "            # sampling sheet source_mat_id has ref_code in run_information\n",
    "            # but the corresponding measured sheet lacks the same sources_mat_id\n",
    "            # This shouldn't happen unless the auto-formatting of the 'source_mat_id'\n",
    "            # field in the 'measured' sheet is broken - which is exactly what happened.\n",
    "            missing_measured_but_refcode_present += 1\n",
    "            source_mat_ids_in_sampling_with_refcode_missing_from_measured.append(\n",
    "                event_mat_id\n",
    "            )\n",
    "            continue\n",
    "        else:\n",
    "            sampling_event[\"ref_code\"] = refcode  # key to sequence data\n",
    "            sampling_event[\"obs_id\"] = observatory_id  # key to observatory data\n",
    "            if refcode in refcodes_in_run_info:\n",
    "                raise ValueError(\n",
    "                    f\"Error: {refcode=} match more that one sampling event \"\n",
    "                    f\"with the {source_mat_id=}\"\n",
    "                )\n",
    "            else:\n",
    "                refcodes_in_run_info.append(refcode)\n",
    "                # Delete the now duplicated source_mat_id in the combined event\n",
    "                del event_measured[\"source_mat_id\"]\n",
    "                source_mat_ids_from_combined_events.append(event_mat_id)\n",
    "                sampling_event.update(event_measured)\n",
    "                combined_events.append(sampling_event)\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Observatory {observatory_id}-{sampling_type} has {len(sampling_events)} sampling events.\\n\"\n",
    "            f\"{no_refcode_counter} have no ref_code (i.e. they were not sent for sequencing), \\n\"\n",
    "            f\"{missing_measured_but_refcode_present} 'sampling' events have a refcode but no \"\n",
    "            f\"'measured' data with the corresponding source_mat_id: \\n\"\n",
    "            # f\"{source_mat_ids_in_sampling_with_refcode_missing_from_measured} \\n\"\n",
    "            f\"A total of {len(combined_events)} sampling events with refcode and measured sheet were found.\\n\"\n",
    "        )\n",
    "\n",
    "    # Did we find all the sampling events?\n",
    "    se = len(sampling_events)\n",
    "    ce = len(combined_events)\n",
    "    mmbrcp = missing_measured_but_refcode_present\n",
    "    nrc = no_refcode_counter\n",
    "    dc = duplicates_ignored_counter\n",
    "    assert se == (ce + nrc + mmbrcp + dc), (\n",
    "        f\"Something is a foot: len(sampling_events) {se} != \"\n",
    "        f\"(len(combined_events) {ce} + no_refcode_counter {nrc} \"\n",
    "        f\"missing_measured_but_refcode_present {mmbrcp}) \"\n",
    "        f\"known duplicates ignored was {dc}\"\n",
    "    )\n",
    "\n",
    "    if len(combined_events) != 0 and save_table:\n",
    "        save_dir = \"./transformed\"\n",
    "        outfile_name = (\n",
    "            f\"{observatory_id}_{sampling_type}_combined_validated.csv\"\n",
    "        )\n",
    "        ndf = pd.DataFrame.from_records(combined_events, index=\"source_mat_id\")\n",
    "        ndf.to_csv(os.path.join(save_dir, outfile_name))\n",
    "\n",
    "    return (\n",
    "        source_mat_ids_from_combined_events,\n",
    "        all_sampling_source_mat_ids,\n",
    "        missing_measured_but_refcode_present,\n",
    "        duplicates_ignored_counter,\n",
    "        combined_events,\n",
    "    )\n",
    "\n",
    "\n",
    "def validate_observatories(\n",
    "    observatory_data, obs_refcodes, save_table=False\n",
    ") -> tuple[list, list, int, int, list]:\n",
    "\n",
    "    # To be returned\n",
    "    all_source_mat_ids_from_combined_events: list[str] = []\n",
    "    all_source_mat_ids_from_sheets: list[str] = []\n",
    "    all_missing_measured_but_refcode_present: int = 0\n",
    "    all_duplicates_ignored: int = 0\n",
    "    all_combined_events: list[list[str, ...]] = []\n",
    "\n",
    "    for observatory_id, water_column, soft_sediment in observatory_data:\n",
    "\n",
    "        if observatory_id == \"Plenzia\":\n",
    "            continue  # Data not public\n",
    "        if observatory_id == \"UMF\" and soft_sediment:\n",
    "            continue  # Broken sheet\n",
    "\n",
    "        # Surely there is a better way to do this\n",
    "        observatories_present = []\n",
    "        if validators.url(water_column):\n",
    "            observatories_present.append(\"water_column\")\n",
    "        if validators.url(soft_sediment):\n",
    "            observatories_present.append(\"soft_sediment\")\n",
    "\n",
    "        for sampling_strategy in observatories_present:\n",
    "\n",
    "            r = parse_observatory_sample_type(\n",
    "                observatory_id, obs_refcodes, sampling_strategy, save_table\n",
    "            )\n",
    "\n",
    "            all_source_mat_ids_from_combined_events.extend(r[0]),\n",
    "            all_source_mat_ids_from_sheets.extend(r[1]),\n",
    "            # Cannot use += on tuple unpacking\n",
    "            all_missing_measured_but_refcode_present = (\n",
    "                all_missing_measured_but_refcode_present + r[2]\n",
    "            )\n",
    "            all_duplicates_ignored = all_duplicates_ignored + r[3]\n",
    "            all_combined_events.extend(r[4])\n",
    "\n",
    "    return (\n",
    "        all_source_mat_ids_from_combined_events,\n",
    "        all_source_mat_ids_from_sheets,\n",
    "        all_missing_measured_but_refcode_present,\n",
    "        all_duplicates_ignored,\n",
    "        all_combined_events,\n",
    "    )\n",
    "\n",
    "\n",
    "############ VALIDATE OBSERVATORIES ###################################################\n",
    "\n",
    "# Get observatory data and ref_codes\n",
    "observatory_data: list[str, str, str] = get_observatory_data()\n",
    "obs_refcodes: dict[str, str] = get_all_refcodes()\n",
    "\n",
    "result = validate_observatories(observatory_data, obs_refcodes, save_table=True)\n",
    "\n",
    "all_source_mat_ids_from_combined_events = result[0]\n",
    "all_source_mat_ids_from_sheets = result[1]\n",
    "all_missing_measured_but_refcode_present = result[2]\n",
    "all_duplicates_ignored = result[3]\n",
    "all_combined_events = result[4]\n",
    "\n",
    "############## REPORT STATISTICS ########################################################\n",
    "\n",
    "############### REFCODES ####################################\n",
    "print(f\"There are {len(obs_refcodes)} total ref_codes assigned\")\n",
    "# We are ignoring both of the duplicates but it's only 1 record expected missing from total_combined events\n",
    "\n",
    "############## DUPLICATES ###################################\n",
    "total_dups_ignored = int(all_duplicates_ignored / 2)\n",
    "print(\n",
    "    f\"A total of {total_dups_ignored} sequencing event in the Batch 1 & 2 run_information sheets \"\n",
    "    f\"have refcodes that match source_mat_ids in the sample sheets that have duplicate entries\"\n",
    ")\n",
    "print(\n",
    "    f\"Total number of combined sampling events with ref_codes: {len(all_combined_events)}\"\n",
    ")\n",
    "duplicates = [\n",
    "    source_mat_id\n",
    "    for source_mat_id, count in collections.Counter(\n",
    "        all_source_mat_ids_from_sheets\n",
    "    ).items()\n",
    "    if count > 1\n",
    "]\n",
    "print(\n",
    "    f\"Total number of all_source_mat_ids_from_sheets: {len(all_source_mat_ids_from_sheets)}\"\n",
    "    f\" of which {len(duplicates)} were duplicates\"\n",
    ")\n",
    "\n",
    "############# SOURCE_MAT_IDS ################################\n",
    "missing_source_mat_ids = []\n",
    "for source_mat_id in obs_refcodes:\n",
    "    # source_mat_ids are the keys in the refcode dict of the run_information\n",
    "    # print(f\"refcode from run_information: {refcode}\")\n",
    "\n",
    "    # Hack for HCMR-1\n",
    "    if source_mat_id == \"EMOBON_HCMR-1_Wa_210917_3um_blank\":\n",
    "        source_mat_id = \"EMOBON_HCMR-1_Wa_210917_3um_blank1\"\n",
    "    if source_mat_id == \"EMOBON_HCMR-1_Wa_210917_0.2um_blank\":\n",
    "        source_mat_id = \"EMOBON_HCMR-1_Wa_210917_0.2um_blank1\"\n",
    "\n",
    "    if source_mat_id not in all_source_mat_ids_from_sheets:\n",
    "        # print(f\"source_mat_id {source_mat_id} is missing from the sampling sheets\")\n",
    "\n",
    "        # Get close matches to missing source_mat_id\n",
    "        matches = difflib.get_close_matches(\n",
    "            source_mat_id, all_source_mat_ids_from_sheets, n=3\n",
    "        )\n",
    "        missing_source_mat_ids.append([source_mat_id, matches])\n",
    "\n",
    "if missing_source_mat_ids:\n",
    "    print(\n",
    "        \"\\n\\nThe missing source_mat_ids that are in the run information sheets are:\"\n",
    "    )\n",
    "    for missing in missing_source_mat_ids:\n",
    "        join = \" \".join(missing[1])\n",
    "        print(\n",
    "            f\"Missing source_mat_id is {missing[0]} close matches are \\n\\t {join}\"\n",
    "        )\n",
    "\n",
    "    for missing in missing_source_mat_ids:\n",
    "        print(missing)\n",
    "\n",
    "print(\n",
    "    f\"A total of {len(missing_source_mat_ids)} source_mat_ids \"\n",
    "    f\"in the batch 1 & 2 run information sheets are missing from the \"\n",
    "    f\"observatory sampling sheets\"\n",
    ")\n",
    "\n",
    "missing = False  # CAUTION: THIS SHOULD BE ZERO!\n",
    "counter = 0\n",
    "# TODO: take all_source_mat_ids_from_combined_events directly from all_combined_events rather than having\n",
    "# a separate list\n",
    "for source_mat_id in all_source_mat_ids_from_combined_events:\n",
    "    # source_mat_ids are the keys in the refcode dict of the run_information\n",
    "    # print(f\"source_mat_id: {source_mat_id}\")\n",
    "    if source_mat_id not in obs_refcodes:\n",
    "        # print(f\"source_mat_id {source_mat_id} is missing from the run_information\")\n",
    "        counter += 1\n",
    "        missing = True\n",
    "if missing:\n",
    "    print(\n",
    "        f\"ERROR: A total of {counter} source_mat_ids \"\n",
    "        f\"in the sampling sheets that also have refcodes are missing from the \"\n",
    "        f\"Batch 1 & 2 run information sheets\"\n",
    "    )\n",
    "\n",
    "##################### SUMMARY #################################\n",
    "total = len(all_combined_events) + len(missing_source_mat_ids)\n",
    "print(\n",
    "    f\"\\nTotal combined_events: {len(all_combined_events)} \\n\"\n",
    "    f\"Missing source_mat_ids in combined events: {len(missing_source_mat_ids)} \\n\"\n",
    "    f\"EMO BON ref_codes in run_information sheets: {len(obs_refcodes)} \\n\"\n",
    "    f\"Events ignored due to duplications in sampling sheets: {total_dups_ignored} \\n\"\n",
    "    f\"{len(all_combined_events)} + {len(missing_source_mat_ids)} = {len(obs_refcodes)} - {total_dups_ignored}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b5e06b-a1b8-4661-afd2-3ae2a42b3103",
   "metadata": {},
   "source": [
    "# Create the single metadata table from the combined observatories tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b0522c-c143-42f4-8d0f-db82b0e56e3f",
   "metadata": {},
   "source": [
    "Combined observatories metadata tables are in ./transformed and have the file name format:\n",
    "`<observatory_id>_<sampling_strategy>_combined_validated.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c07f7b5-9c18-4e11-9ee0-2e8061603b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"./transformed\"\n",
    "csv_files = [f for f in os.listdir(FILE_PATH) if f.split(\".\")[1] == \"csv\"]\n",
    "\n",
    "number_of_rows = 0\n",
    "frames = []\n",
    "for obs in csv_files:\n",
    "    front = obs.split(\"_combined_validated.csv\")[0]\n",
    "    obs_id, strategy = front.split(\"_\", 1)\n",
    "    # print(f\"{obs_id=} -> - {strategy=}\")\n",
    "    df = pd.read_csv(\n",
    "        os.path.join(FILE_PATH, obs), dtype={\"tax_id\": \"Int64\"}\n",
    "    )  # See note above\n",
    "    for index, event in df.iterrows():\n",
    "        # This is the name given in the Observatory sheet Google Spreadsheet\n",
    "        df.loc[index, \"env_package\"] = strategy\n",
    "        number_of_rows += 1\n",
    "    frames.append(df)\n",
    "\n",
    "print(f\"{number_of_rows=}\")\n",
    "combined_df = pd.concat(frames)\n",
    "today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "outfile_name = f\"Batch1and2_combined_logsheets_{today}.csv\"\n",
    "# combined_df.set_index(\"source_mat_id\")\n",
    "combined_df.to_csv(outfile_name, index=False)\n",
    "combined_df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
