{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae637b6-a712-4ec7-b2cb-4d8c11be9feb",
   "metadata": {},
   "source": [
    "### Pulls from the raw Obseravatory Google Sheets, does \"lax\" (default), \"strict\" and \"semi-strict\" validation depending on constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f2f9e9-b4db-4429-b30a-faa4bbf05c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "    \n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pydantic import ValidationError\n",
    "from pprint import pprint\n",
    "from validation_classes import samplingModel, measuredModel, samplingModelStrict, samplingModelSemiStrict\n",
    "\n",
    "############################ CAUTION #############################################\n",
    "STRICT      = False  # As defined by Ioulia, dates corrected, NA's removed etc\n",
    "SEMI_STRICT = True   # As defined by Ioulia but not checking for mandatory fields\n",
    "                     # ints and str coerced to floats when possible\n",
    "##################################################################################\n",
    "\n",
    "def parse_sample_sheets(sampling_strategy: str,\n",
    "                        sheet_type: str,\n",
    "                        addresses: pd.core.frame.DataFrame,\n",
    "                       ) -> None:\n",
    "    \n",
    "    for observatory in addresses:\n",
    "        observatory_id, sheet_link = observatory\n",
    "        #print(f\"Observatory_id {observatory_id} sheet_link {sheet_link}\")\n",
    "        if not isinstance(sheet_link, str):\n",
    "            #print(f\"This is the sheet_link type {type(sheet_link)}\")\n",
    "            if isinstance(sheet_link, float): \n",
    "                if math.isnan(sheet_link):\n",
    "                    print(f\"Observatory {observatory_id} lacks valid sheet URL {sheet_link}\")\n",
    "                    continue\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown link {sheet_link} to observatory {observatory_id}\")\n",
    "        else:\n",
    "\n",
    "            if observatory_id == \"Plenzia\": continue # Sheets not publically available\n",
    "            # UMF soft_sed has two source_mat_ids\n",
    "            if sampling_strategy == \"soft_sediment\" and observatory_id == \"UMF\":\n",
    "                continue\n",
    "    \n",
    "            print(f\"Processing {observatory_id}...\")\n",
    "            sampling_sheet_base = sheet_link.split(\"/edit\")[0]\n",
    "            sampling_sheet_suffix = \"/gviz/tq?tqx=out:csv&sheet=%s\"\n",
    "            sample_sheet_link = sampling_sheet_base + sampling_sheet_suffix % sheet_type\n",
    "            print(f\"Sample sheet link: {sample_sheet_link}\")\n",
    "            df = pd.read_csv(sample_sheet_link, encoding='utf-8')\n",
    "            data_records_all = df.to_dict(orient=\"records\")\n",
    "    \n",
    "            # Many sheets have partially filled rows\n",
    "            # The source_mat_id is manually curated and the PRIMARY_KEY\n",
    "            # Therefore filter records on source_mat_id\n",
    "            def filter_on_source_mat_id(d):\n",
    "                # Bergen has it as source_material_id\n",
    "                try:\n",
    "                    value = d[\"source_mat_id\"]\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        value = d[\"source_material_id\"]\n",
    "                    except KeyError:\n",
    "                        raise ValueError(\"Cannot find source_mat_id field\")\n",
    "                if isinstance(value, float):\n",
    "                    if math.isnan(value):\n",
    "                        return False\n",
    "                elif value is None:\n",
    "                    return False\n",
    "                # Remove mis-formatted\n",
    "                elif len(value.split(\"_\")) < 6:\n",
    "                    return False\n",
    "                #Edge case of this otherwise blank entry having 6 \"bits\"\n",
    "                elif value == \"EMOBON_VB_Wa_230509_um_\":\n",
    "                    return False \n",
    "                else:\n",
    "                    return True\n",
    "                \n",
    "            data_records_filtered = list(filter(filter_on_source_mat_id, data_records_all))\n",
    "    \n",
    "            if len(data_records_all) > len(data_records_filtered):\n",
    "                print(f\"Discarded {len(data_records_all) - len(data_records_filtered)} records leaving {len(data_records_filtered)}.\")\n",
    "\n",
    "            if STRICT:\n",
    "                model_type = f\"{sheet_type}_strict\"\n",
    "            elif SEMI_STRICT:\n",
    "                model_type = f\"{sheet_type}_semistrict\"\n",
    "            else:\n",
    "                model_type = sheet_type\n",
    "\n",
    "            validator = validator_classes[model_type]\n",
    "\n",
    "            #validated_rows = [validator(**row).model_dump() for row in data_records_filtered]\n",
    "            validated_rows = []\n",
    "            errors: List[List[str:List[Dict]]] = [] # where each error is the inner Dict\n",
    "            for row in data_records_filtered:\n",
    "                try:\n",
    "                    vr = validator(**row)\n",
    "                except ValidationError as e:\n",
    "                    if observatory_id == \"Bergen\":\n",
    "                        errors.append([(row[\"source_material_id\"], e.errors())])\n",
    "                    else:\n",
    "                        errors.append([(row[\"source_mat_id\"], e.errors())])\n",
    "                else:\n",
    "                    validated_rows.append(vr.model_dump())\n",
    "\n",
    "            if errors:\n",
    "                # errors is a list of lists where each inner list is a dict of row errors\n",
    "                # where each isof key = source_mat_id and values is list of dicts each of which\n",
    "                # is an error:\n",
    "                #List[List[str:List[Dict]]]\n",
    "                total_number_errors = sum([len(row[1]) for e in errors for row in e])\n",
    "                print(f\"Errors were found... {total_number_errors} in total\")\n",
    "                save_dir = \"./validation_errors\"\n",
    "                outfile_name_pk = f\"{observatory_id}_{sampling_strategy}_{model_type}_ERRORS.pickle\"\n",
    "                out_path_pk = os.path.join(save_dir, outfile_name_pk)\n",
    "                with open(out_path_pk, \"wb\") as f:\n",
    "                    pickle.dump(errors, f, pickle.HIGHEST_PROTOCOL)\n",
    "                outfile_name_log = f\"{observatory_id}_{sampling_strategy}_{model_type}_ERRORS.log\"\n",
    "                out_path_log = os.path.join(save_dir, outfile_name_log)                \n",
    "                with open(out_path_log, \"w\") as f:\n",
    "                    pprint(errors, f)\n",
    "            else:\n",
    "                assert len(validated_rows) == len(data_records_filtered), \"Not sure what happenned, but len(validated_rows) != len(data_filtered_records)\"\n",
    "                print(\"All records passed!\")\n",
    "            \n",
    "                #for record in validated_rows:\n",
    "                #    for field in record:\n",
    "                #        print(f\"Record {field} has value {record[field]} is type {type(record[field])}\")\n",
    "\n",
    "                if not STRICT and not SEMI_STRICT:\n",
    "                    save_dir = \"./logsheets\"\n",
    "                    outfile_name = f\"{observatory_id}_{sampling_strategy}_{model_type}_validated.csv\"\n",
    "                    ndf = pd.DataFrame.from_records(validated_rows, index=\"source_mat_id\")\n",
    "                    ndf.to_csv(os.path.join(save_dir, outfile_name))\n",
    "                    print(f\"Written {os.path.join(save_dir, outfile_name)}\")\n",
    "\n",
    "\n",
    "validator_classes = {\"sampling\": samplingModel,\n",
    "                     \"measured\": measuredModel,\n",
    "                     \"sampling_strict\": samplingModelStrict,\n",
    "                     \"sampling_semistrict\": samplingModelSemiStrict\n",
    "                    }\n",
    "\n",
    "# Get list of all URL links to sampling sheets\n",
    "# NB  you cant use a \"with\" closure here when reading the Pandas df\n",
    "governance_logsheets_validated_csv = \"./governance/logsheets_validated.csv\"\n",
    "df = pd.read_csv(governance_logsheets_validated_csv)\n",
    "water_column_sheet_addresses = df[[\"observatory_id\", \"water_column\"]].values.tolist()\n",
    "soft_sediment_sheet_addresses  = df[[\"observatory_id\", \"soft_sediment\"]].values.tolist()\n",
    "del df\n",
    "\n",
    "parse_sample_sheets(\"water_column\", \"sampling\", water_column_sheet_addresses)\n",
    "\n",
    "#parse_sample_sheets(\"water_column\", \"sampling\", water_column_sheet_addresses)\n",
    "#parse_sample_sheets(\"soft_sediment\", \"sampling\", soft_sediment_sheet_addresses)\n",
    "#parse_sample_sheets(\"water_column\", \"measured\", water_column_sheet_addresses)\n",
    "#parse_sample_sheets(\"soft_sediment\", \"measured\", soft_sediment_sheet_addresses)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
