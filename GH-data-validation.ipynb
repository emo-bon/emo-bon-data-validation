{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54fa567d-e682-4aff-bb5d-959c437212d6",
   "metadata": {},
   "source": [
    "### Pulls sheets from Github after QC curation by EMBRC, does \"lax\", \"strict\", and \"semi-strict\" validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ae275a-eb87-4218-a735-93faee7ddee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "    \n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "from urllib.request import HTTPError\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "from pydantic import ValidationError\n",
    "from pprint import pprint\n",
    "from validation_classes import (samplingModelGithub,               # lax validator for EMO-BON Github repository\n",
    "                                samplingModelGithubStrict,         # strict validator for EMO-BON Github repository\n",
    "                                samplingModelGithubSemiStrict      # semi-strict validator for EMO-BON Github repository                  \n",
    "                               )\n",
    "class SamplingStrategy(Enum):\n",
    "    WATER    = \"water\"    # Originally water_column\n",
    "    SEDIMENT = \"sediment\" # Originally soft_sediment\n",
    "\n",
    "class SheetType(Enum):\n",
    "    SAMPLING = \"sampling\"\n",
    "    MEASURED = \"measured\"\n",
    "\n",
    "# Not all observatories have \"transformed\" sheets on GH, but may have \"raw\"\n",
    "# Of course the types in the fields are different to difficult to validate\n",
    "# with a single validator - best just to ignore the raw sheets\n",
    "USE_RAW = False\n",
    "\n",
    "############################ CAUTION #############################################\n",
    "# As defined by Ioulia, dates corrected, NA's removed etc\n",
    "STRICT      = False\n",
    "\n",
    "# As defined by Ioulia but not checking for mandatory fields\n",
    "# ints and str coerced to floats when possible\n",
    "SEMI_STRICT = False  \n",
    "##################################################################################\n",
    "\n",
    "def get_sheet_from_github(observatory_id: SheetType,\n",
    "                          sampling_strategy: SamplingStrategy, \n",
    "                          sheet_type: str) -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    Here we pull the \"sampling\" or \"measured\" sheets from Github. These are the curated\n",
    "    sheets downloaded by the Github actions and hopefully do not have the errors that\n",
    "    the CSV's pulled directly from Google Sheets had (e.g. the word \"blank\" magically\n",
    "    disappering from the \"replicate\" field.)\n",
    "\n",
    "    Github paths look like:\n",
    "    https://raw.githubusercontent.com/emo-bon/observatory-umf-crate/main/logsheets/transformed/sediment_measured.csv\n",
    "    https://raw.githubusercontent.com/emo-bon/observatory-bergen-crate/main/logsheets/raw/water_sampling.csv\n",
    "\n",
    "    The problem at 29-08-2024 is that the data are out of date.\n",
    "    \"\"\"\n",
    "\n",
    "    prefix     = \"https://raw.githubusercontent.com/emo-bon\"\n",
    "    obs_name   = f\"observatory-{observatory_id}-crate\"\n",
    "    inter_path = \"main/logsheets\"\n",
    "    dir_path   = \"transformed\"\n",
    "    sheet_name = f\"{sampling_strategy}_{sheet_type}.csv\"\n",
    "\n",
    "    print(f\"Processing {observatory_id}... {sheet_name}\")\n",
    "    github_addr = os.path.join(prefix, obs_name, inter_path, dir_path, sheet_name)\n",
    "    try:\n",
    "        df = pd.read_csv(github_addr)\n",
    "    except HTTPError:\n",
    "        # Some observatories don't yet have transformed sheets\n",
    "        if USE_RAW:\n",
    "            # Try for the raw sheets\n",
    "            print(\"Unable to find 'transformed' sheet, reading the 'raw' sheet\")\n",
    "            dir_path   = \"raw\"\n",
    "            github_addr = os.path.join(prefix, obs_name, inter_path, dir_path, sheet_name)\n",
    "            try:\n",
    "                df = pd.read_csv(github_addr)\n",
    "            except HTTPError:\n",
    "                raise ValueError(\"Unable to find transformed or raw sheet\")\n",
    "        else:\n",
    "            print(f\"Observatory {observatory_id} does not have a transformed {sheet_name} on GH\")\n",
    "            return None\n",
    "            \n",
    "    return df\n",
    "\n",
    "def filter_on_source_mat_id(d):\n",
    "    # Bergen has it as source_material_id on Google and Github\n",
    "    try:\n",
    "        value = d[\"source_mat_id\"]\n",
    "    except KeyError:\n",
    "        try:\n",
    "            value = d[\"source_material_id\"]\n",
    "        except KeyError:\n",
    "            raise ValueError(\"Cannot find source_mat_id field\")\n",
    "    if isinstance(value, float):\n",
    "        if math.isnan(value):\n",
    "            return False\n",
    "    elif value is None:\n",
    "        return False\n",
    "    # Remove mis-formatted\n",
    "    elif len(value.split(\"_\")) < 6:\n",
    "        return False\n",
    "    #Edge case of this otherwise blank entry having 6 \"bits\"\n",
    "    elif value == \"EMOBON_VB_Wa_230509_um_\":\n",
    "        return False \n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def parse_sample_sheets(sampling_strategy: str,\n",
    "                        sheet_type: str,\n",
    "                        addresses: pd.core.frame.DataFrame,\n",
    "                       ) -> None:\n",
    "    \n",
    "    for observatory in addresses:\n",
    "        observatory_id, sheet_link = observatory\n",
    "        #print(f\"ObSservatory_id {observatory_id} sheet_link {sheet_link}\")\n",
    "        if not isinstance(sheet_link, str):\n",
    "            #print(f\"This is the sheet_link type {type(sheet_link)}\")\n",
    "            if isinstance(sheet_link, float):\n",
    "                # Only OOB doesnt do water_column\n",
    "                # But most do not do soft-sediments\n",
    "                if math.isnan(sheet_link):\n",
    "                    print(f\"Observatory {observatory_id} does not have a {sampling_strategy} sampling strategy.\")\n",
    "                    continue\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown value \\'{sheet_link}\\' in {sampling_strategy} cell of {observatory_id}\")\n",
    "        else:\n",
    "\n",
    "            if observatory_id == \"Plenzia\": continue # Sheets not publically available\n",
    "\n",
    "            ################ CAUTION ##################\n",
    "            #if not observatory_id in [\"AAOT\"]: continue\n",
    "            \n",
    "            # UMF soft_sed has two source_mat_ids\n",
    "            if sampling_strategy == \"sediment\" and observatory_id == \"UMF\":\n",
    "                continue\n",
    "    \n",
    "            df = get_sheet_from_github(observatory_id, sampling_strategy, sheet_type)\n",
    "            if df is None:\n",
    "                continue\n",
    "            data_records_all = df.to_dict(orient=\"records\")\n",
    "    \n",
    "            # Many sheets have partially filled rows\n",
    "            # The source_mat_id is manually curated and the PRIMARY_KEY\n",
    "            # Therefore filter records on source_mat_id\n",
    "                \n",
    "            data_records_filtered = list(filter(filter_on_source_mat_id, data_records_all))\n",
    "    \n",
    "            if len(data_records_all) > len(data_records_filtered):\n",
    "                print(f\"Discarded {len(data_records_all) - len(data_records_filtered)} records leaving {len(data_records_filtered)}.\")\n",
    "\n",
    "            ################ CAUTION ##############\n",
    "            #continue\n",
    "\n",
    "            if STRICT:\n",
    "                model_type = f\"{sheet_type}_github_strict\"\n",
    "            elif SEMI_STRICT:\n",
    "                model_type = f\"{sheet_type}_github_semistrict\"\n",
    "            else:\n",
    "                model_type = f\"{sheet_type}_github\"\n",
    "\n",
    "            validator = validator_classes[model_type]\n",
    "            #print(f\"Using {validator} from {model_type}\")\n",
    "\n",
    "            #validated_rows = [validator(**row).model_dump() for row in data_records_filtered]\n",
    "            validated_rows = []\n",
    "            errors: List[List[str:List[Dict]]] = [] # where each error is the inner Dict\n",
    "            for row in data_records_filtered:\n",
    "                try:\n",
    "                    vr = validator(**row)\n",
    "                except ValidationError as e:\n",
    "                    if observatory_id == \"Bergen\":\n",
    "                        errors.append([(row[\"source_material_id\"], e.errors())])\n",
    "                    else:\n",
    "                        errors.append([(row[\"source_mat_id\"], e.errors())])\n",
    "                else:\n",
    "                    validated_rows.append(vr.model_dump())\n",
    "\n",
    "            if errors:\n",
    "                # errors is a list of lists where each inner list is a dict of row errors\n",
    "                # where each isof key = source_mat_id and values is list of dicts each of which\n",
    "                # is an error:\n",
    "                #List[List[str:List[Dict]]]\n",
    "                total_number_errors = sum([len(row[1]) for e in errors for row in e])\n",
    "                print(f\"Errors were found... {total_number_errors} in total\")\n",
    "                save_dir = \"./validation_errors_github\"\n",
    "                #outfile_name_pk = f\"{observatory_id}_{sampling_strategy}_{model_type}_ERRORS.pickle\"\n",
    "                #out_path_pk = os.path.join(save_dir, outfile_name_pk)\n",
    "                #with open(out_path_pk, \"wb\") as f:\n",
    "                #    pickle.dump(errors, f, pickle.HIGHEST_PROTOCOL)\n",
    "                outfile_name_log = f\"{observatory_id}_{sampling_strategy}_{model_type}_ERRORS.log\"\n",
    "                out_path_log = os.path.join(save_dir, outfile_name_log)                \n",
    "                with open(out_path_log, \"w\") as f:\n",
    "                    pprint(errors, f)\n",
    "            else:\n",
    "                assert len(validated_rows) == len(data_records_filtered), \\\n",
    "                    \"Not sure what happenned, but len(validated_rows) != len(data_filtered_records)\"\n",
    "                print(\"All records passed!\")\n",
    "            \n",
    "                #for record in validated_rows:\n",
    "                #    for field in record:\n",
    "                #        print(f\"Record {field} has value {record[field]} is type {type(record[field])}\")\n",
    "\n",
    "                if not STRICT and not SEMI_STRICT:\n",
    "                    save_dir = \"./logsheets_github\"\n",
    "                    outfile_name = f\"{observatory_id}_{sampling_strategy}_{model_type}_validated.csv\"\n",
    "                    ndf = pd.DataFrame.from_records(validated_rows, index=\"source_mat_id\")\n",
    "                    ndf.to_csv(os.path.join(save_dir, outfile_name))\n",
    "                    print(f\"Written {os.path.join(save_dir, outfile_name)}\")\n",
    "\n",
    "validator_classes = {\"sampling_github\"           : samplingModelGithub, \n",
    "                     \"sampling_github_strict\"    : samplingModelGithubStrict,\n",
    "                     \"sampling_github_semistrict\": samplingModelGithubSemiStrict\n",
    "                    }\n",
    "\n",
    "# Get list of all URL links to sampling sheets\n",
    "# NB  you cant use a \"with\" closure here when reading the Pandas df\n",
    "governance_logsheets_validated_csv = \"./governance/logsheets_validated.csv\"\n",
    "df = pd.read_csv(governance_logsheets_validated_csv)\n",
    "water_column_sheet_addresses = df[[\"observatory_id\", \"water_column\"]].values.tolist()\n",
    "soft_sediment_sheet_addresses  = df[[\"observatory_id\", \"soft_sediment\"]].values.tolist()\n",
    "del df\n",
    "\n",
    "parse_sample_sheets(\"water\", \"sampling\", water_column_sheet_addresses)\n",
    "parse_sample_sheets(\"sediment\", \"sampling\", soft_sediment_sheet_addresses)\n",
    "#parse_sample_sheets(\"water\", \"measured\", water_column_sheet_addresses)\n",
    "#parse_sample_sheets(\"sediment\", \"measured\", soft_sediment_sheet_addresses)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
