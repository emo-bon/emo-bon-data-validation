{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c01fa68b-1e71-4a5d-a68a-77b4d6e810f7",
   "metadata": {},
   "source": [
    "# Pydantic validation framework for the EMO BON observatory and other metadata log sheets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bb619d-5b92-4bee-b1a5-b3f0cb364f70",
   "metadata": {},
   "source": [
    "- **pydantic** Data validation using Python type hints.\n",
    "    - [pypi](https://pypi.org/project/pydantic/)\n",
    "    - [Documentation](https://docs.pydantic.dev/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5bca054-8458-4137-ae94-b960bbe0ad45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD is /srv/scratch/emo-bon-data-validation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Weird stuff from JupyterHub after I moved modules and notebooks around:\n",
    "# For some reasong CWD is /src/scratch even though this notebook is in /srv/scratch/emo-bon-validation\n",
    "# The terminal also show us to be in /srv/scratch/emo-bon-validation\n",
    "# So...\n",
    "if os.getcwd() == \"/srv/scratch\":\n",
    "    os.chdir(\"./emo-bon-data-validation\")\n",
    "print(f\"CWD is {os.getcwd()}\")\n",
    "\n",
    "import importlib\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f0a524-4ac9-4840-ba77-b031296619fa",
   "metadata": {},
   "source": [
    "##### Init the directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d4afacf-09d3-47dc-80cc-e00d3e9232c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    #Init dirs and paths, write csv files\n",
    "    # Init the validation classes dir if needed\n",
    "    # Note that __init__.py will need be edited manually to import the validators\n",
    "    # e.g from .observatories import Model as observatoriesModel\n",
    "    validation_classes_path = \"./validation_classes\"\n",
    "    if True:\n",
    "        if not os.path.exists(validation_classes_path):\n",
    "            os.mkdir(validation_classes_path)\n",
    "            Path(os.path.join(validation_classes_path, \"__init__.py\")).touch()\n",
    "            os.mkdir(raw_files_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e601478-27ea-404a-8371-1405051ccf22",
   "metadata": {},
   "source": [
    "## Governance data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e34558-fb7c-4000-8e8b-24ad71df7eab",
   "metadata": {},
   "source": [
    "#### Read each of the governance CSV files into a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "154ba717-c9a9-43a0-b34e-8b7461952135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18 entries, 0 to 17\n",
      "Data columns (total 9 columns):\n",
      " #   Column                               Non-Null Count  Dtype \n",
      "---  ------                               --------------  ----- \n",
      " 0   EMBRC Node                           18 non-null     object\n",
      " 1   EMBRC Site                           18 non-null     object\n",
      " 2   EMOBON_observatory_id                18 non-null     object\n",
      " 3   Water Column                         17 non-null     object\n",
      " 4   Soft sediment                        7 non-null      object\n",
      " 5   data_quality_control_threshold_date  18 non-null     object\n",
      " 6   data_quality_control_assignee        18 non-null     object\n",
      " 7   rocrate_profile_uri                  18 non-null     object\n",
      " 8   autogenerate                         18 non-null     int64 \n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 1.4+ KB\n",
      "This is info() for None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19 entries, 0 to 18\n",
      "Data columns (total 22 columns):\n",
      " #   Column                            Non-Null Count  Dtype  \n",
      "---  ------                            --------------  -----  \n",
      " 0   country_code                      19 non-null     object \n",
      " 1   country                           19 non-null     object \n",
      " 2   EMOBON_observatory_name           19 non-null     object \n",
      " 3   EMOBON_observatory_id             19 non-null     object \n",
      " 4   startdate                         19 non-null     object \n",
      " 5   enddate                           2 non-null      object \n",
      " 6   Water_Column                      19 non-null     object \n",
      " 7   Soft_Substrates                   19 non-null     object \n",
      " 8   Hard_Substrates                   19 non-null     object \n",
      " 9   water_site_latitude               18 non-null     float64\n",
      " 10  water_site_longitude              18 non-null     float64\n",
      " 11  sediment_site_latitude            9 non-null      float64\n",
      " 12  sediment_site_longtitude          9 non-null      float64\n",
      " 13  hard_substrates_site1_longitude   9 non-null      object \n",
      " 14  hard_substrates_site1_latitude    9 non-null      object \n",
      " 15  hard_substrates_site2_longtitude  3 non-null      float64\n",
      " 16  hard_substrates_site2_latitude    3 non-null      float64\n",
      " 17  contect person                    19 non-null     object \n",
      " 18  contact person email              19 non-null     object \n",
      " 19  ENA_accession_number_umbrella     19 non-null     object \n",
      " 20  ENA_accession_number_project      19 non-null     object \n",
      " 21  EMOBON_core                       19 non-null     object \n",
      "dtypes: float64(6), object(16)\n",
      "memory usage: 3.4+ KB\n",
      "This is info() for None\n"
     ]
    }
   ],
   "source": [
    "github_path = \"https://raw.githubusercontent.com/emo-bon/governance-data/main/\"\n",
    "file_names = [\n",
    "        \"logsheets.csv\",              # contain the URLs of the googlesheets that are the logsheets\n",
    "        \"observatories.csv\"           # contain information about each observatory\n",
    "        #\"organisations.csv\",         # contain information about the organisations in EMO BON\n",
    "        #\"planned_events.csv\"         # contains information about planned EMO BON events (this file is only used by humans, not by any actions) - DONT CARE\n",
    "        #\"ro-crate-metadata.json\"     # IGNORE\n",
    "        ]\n",
    "dfs = {}\n",
    "for f in file_names:\n",
    "    df = pd.read_csv(os.path.join(github_path, f))\n",
    "    print(f\"This is info() for {df.info()}\")\n",
    "    dfs[f] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2484615f-cab2-48a2-8ca7-999bab0051e3",
   "metadata": {},
   "source": [
    "#### Validate Governance tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8dc50e0-2d84-4508-8d82-67b90397c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from validation_classes import observatoriesModel, logsheetsModel\n",
    "validator_class_paths = {\"logsheets.csv\": logsheetsModel, \"observatories.csv\": observatoriesModel}\n",
    "validation_classes_path = \"./validation_classes\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa819fa6-c492-481f-bc81-218057699342",
   "metadata": {},
   "source": [
    "##### Observatories table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af3dfa6-8d4c-4749-b9d1-bc00834e1a3b",
   "metadata": {},
   "source": [
    "The observatories validator mostly changes the column names to make them consistent (and spelled correctly), removes blank strings (\"   \") from cells, and reformats the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aa2840b-9b25-4155-bd43-34cea9c81de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"observatories.csv\"\n",
    "data = dfs[file_name] # dfs is dict of pandas df's\n",
    "validator = validator_class_paths[file_name]\n",
    "data_records = data.to_dict(orient=\"records\")\n",
    "validated_rows = [validator(**row).model_dump() for row in data_records]\n",
    "\n",
    "ndf = pd.DataFrame.from_records(validated_rows, index=\"observatory_id\")\n",
    "ndf.to_csv(os.path.join(\"governance\", \"observatories_validated.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ef5ade-e3f4-4742-b1ba-41233e93e80c",
   "metadata": {},
   "source": [
    "##### Logsheets table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b9e1b9e-8a05-4bf2-aceb-55c5c740e77d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cdir = os.getcwd()\n",
    "file_name = \"logsheets.csv\"\n",
    "data = dfs[file_name] # dfs is dict of pandas df's\n",
    "validator = validator_class_paths[file_name]\n",
    "data_records = data.to_dict(orient=\"records\")\n",
    "validated_rows = [validator(**row).model_dump() for row in data_records]\n",
    "        \n",
    "ndf = pd.DataFrame.from_records(validated_rows, index=\"observatory_id\")\n",
    "ndf.to_csv(os.path.join(\"governance\", \"logsheets_validated.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fc138c-c96f-4349-843c-ad02276b5f43",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Logsheets from water column and soft sediments sampling and \"measured\" tables\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b0343f-e36b-4685-ac17-e90a9e53f12d",
   "metadata": {},
   "source": [
    "### Version 1 - pulls from Google Sheets only does \"lax\" validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c95e6614-c9ff-470d-8363-8f37a08d34ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "observatory_ids=['BPNS', 'EMT21', 'PiEGetxo', 'VB', 'OOB', 'ROSKOGO', 'HCMR-1', 'IUIEilat1', 'NRMCB', 'AAOT', 'Bergen', 'ESC68N', 'RFormosa', 'OSD74', 'MBAL4', 'STHVN', 'UMF', 'LMO', 'Plenzia']\n",
      "\n",
      "\n",
      "Processing observatory_id='ESC68N' - sampling_strategy='water_column' - sheet_type='sampling'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/11_Eu0W1-sDiuzKx1cIl6YuxjRHmWezN6u9v3Ly8JZ3A/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Discarded 114 records leaving 150.\n",
      "Written ./logsheets/ESC68N_water_column_sampling_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='Bergen' - sampling_strategy='water_column' - sheet_type='sampling'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1HuXHiUJICZrmCrJ4EZDyU5aSCMzDAc1cy_tne5YVPTg/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Written ./logsheets/Bergen_water_column_sampling_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='MBAL4' - sampling_strategy='water_column' - sheet_type='sampling'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1xfrqraPa0auQ1O-C9RUo68RhxrPCDWkVMCAUbj79AZI/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Written ./logsheets/MBAL4_water_column_sampling_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='BPNS' - sampling_strategy='water_column' - sheet_type='sampling'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1mEi4Bd2YR63WD0j54FQ6QkzcUw_As9Wilue9kaXO2DE/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Written ./logsheets/BPNS_water_column_sampling_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='ROSKOGO' - sampling_strategy='water_column' - sheet_type='sampling'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1BCu2pbuIS4f-yrw5Kw4uzfO9v6ryZJBtSF5WBLWG_-E/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Written ./logsheets/ROSKOGO_water_column_sampling_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='VB' - sampling_strategy='water_column' - sheet_type='sampling'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1kSlTcNfXaCQv7fIKVbBnt5PmRW3_H7Trryh4FJHyLkE/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Discarded 32 records leaving 824.\n",
      "Written ./logsheets/VB_water_column_sampling_validated.csv\n",
      "Observatory OOB lacks valid sheet URL for water_column\n",
      "\n",
      "\n",
      "Processing observatory_id='EMT21' - sampling_strategy='water_column' - sheet_type='sampling'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1RutVpduwL1mEed5jC2zLbs0DgkzAwZdsJ27fg3kb0Yg/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Written ./logsheets/EMT21_water_column_sampling_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='PiEGetxo' - sampling_strategy='water_column' - sheet_type='sampling'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1KGcishLP5eAn6ZCnixn5wZIez1Y9MgmYoch2s7aefKM/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Written ./logsheets/PiEGetxo_water_column_sampling_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='RFormosa' - sampling_strategy='water_column' - sheet_type='sampling'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/15f_yqrhvOr1MW-SHuv0JiKf0HRDy81uGZapJANxL54k/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Written ./logsheets/RFormosa_water_column_sampling_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='OSD74' - sampling_strategy='water_column' - sheet_type='sampling'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1VATpzmhwmk8sYkhEjCbDUZHRiYuNGpSRefhXthZRbfQ/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Discarded 28 records leaving 170.\n",
      "Written ./logsheets/OSD74_water_column_sampling_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='AAOT' - sampling_strategy='water_column' - sheet_type='sampling'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1hvLkBwiKTGTJDx19m_8e7qJ2lm9bwLLeVztMpxTLqnk/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Discarded 188 records leaving 400.\n",
      "Written ./logsheets/AAOT_water_column_sampling_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='NRMCB' - sampling_strategy='water_column' - sheet_type='sampling'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1F4AWv_seI-DQJ_Gp_N2ziwvGyjnc4KC92GGNXrJRek4/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Written ./logsheets/NRMCB_water_column_sampling_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='HCMR-1' - sampling_strategy='water_column' - sheet_type='sampling'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/13DcVK2mzSxMJoFydSBaIMmj7Td1_JapEvcY2bmZTyLc/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Written ./logsheets/HCMR-1_water_column_sampling_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='IUIEilat' - sampling_strategy='water_column' - sheet_type='sampling'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1qIwi1nZu4OBiriCzn1fEG1fyG8-3wSVvwzsMiH6JTwk/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Written ./logsheets/IUIEilat_water_column_sampling_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='UMF' - sampling_strategy='water_column' - sheet_type='sampling'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1-1VsUUbRtKselxu-y2BghRIrJdaf74SbmvK42ntvd20/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Written ./logsheets/UMF_water_column_sampling_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='LMO' - sampling_strategy='water_column' - sheet_type='sampling'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1AvQMYcS0tdNMw6Er8zUarQg1a_wrshhnkTS6RuI1FJQ/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Written ./logsheets/LMO_water_column_sampling_validated.csv\n",
      "Observatory ESC68N lacks valid sheet URL for soft_sediment\n",
      "Observatory Bergen lacks valid sheet URL for soft_sediment\n",
      "Observatory MBAL4 lacks valid sheet URL for soft_sediment\n",
      "\n",
      "\n",
      "Processing observatory_id='BPNS' - sampling_strategy='soft_sediment' - sheet_type='sampling'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1zc0bZdpl-Eoi35lI_5BGkElbscplyQRyNPLkSgeEyEQ/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Written ./logsheets/BPNS_soft_sediment_sampling_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='ROSKOGO' - sampling_strategy='soft_sediment' - sheet_type='sampling'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1M0ytBWbUDP0YthGtTVtdIaX2cQUxF_uNIJBHIXREoX8/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Discarded 66 records leaving 155.\n",
      "Written ./logsheets/ROSKOGO_soft_sediment_sampling_validated.csv\n",
      "Observatory VB lacks valid sheet URL for soft_sediment\n",
      "\n",
      "\n",
      "Processing observatory_id='OOB' - sampling_strategy='soft_sediment' - sheet_type='sampling'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1J469_ljfxM9NhOEyRmvWQNOXX7dujxBy1opU6ROmDv8/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Discarded 93 records leaving 133.\n",
      "Written ./logsheets/OOB_soft_sediment_sampling_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='EMT21' - sampling_strategy='soft_sediment' - sheet_type='sampling'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1sBB0x6h-prnUHMcOfms_7qSqTn6zeGVriitTysceoT0/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Discarded 129 records leaving 48.\n",
      "Written ./logsheets/EMT21_soft_sediment_sampling_validated.csv\n",
      "Observatory PiEGetxo lacks valid sheet URL for soft_sediment\n",
      "Observatory Plenzia lacks valid sheet URL for soft_sediment\n",
      "\n",
      "\n",
      "Processing observatory_id='RFormosa' - sampling_strategy='soft_sediment' - sheet_type='sampling'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/17rqrZ-qrDP77SpoEyBFO17wewp98a-tLBWTlBAn6ZCc/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Written ./logsheets/RFormosa_soft_sediment_sampling_validated.csv\n",
      "Observatory OSD74 lacks valid sheet URL for soft_sediment\n",
      "Observatory AAOT lacks valid sheet URL for soft_sediment\n",
      "\n",
      "\n",
      "Processing observatory_id='NRMCB' - sampling_strategy='soft_sediment' - sheet_type='sampling'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1a19q3w2sP7dae8HEzT5TdmfqhRQQd3GItFqiYxWJlxs/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Written ./logsheets/NRMCB_soft_sediment_sampling_validated.csv\n",
      "Observatory HCMR-1 lacks valid sheet URL for soft_sediment\n",
      "Observatory IUIEilat lacks valid sheet URL for soft_sediment\n",
      "Observatory LMO lacks valid sheet URL for soft_sediment\n",
      "\n",
      "\n",
      "Processing observatory_id='ESC68N' - sampling_strategy='water_column' - sheet_type='measured'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/11_Eu0W1-sDiuzKx1cIl6YuxjRHmWezN6u9v3Ly8JZ3A/gviz/tq?tqx=out:csv&sheet=measured\n",
      "Discarded 31 records leaving 120.\n",
      "Written ./logsheets/ESC68N_water_column_measured_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='Bergen' - sampling_strategy='water_column' - sheet_type='measured'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1HuXHiUJICZrmCrJ4EZDyU5aSCMzDAc1cy_tne5YVPTg/gviz/tq?tqx=out:csv&sheet=measured\n",
      "Written ./logsheets/Bergen_water_column_measured_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='MBAL4' - sampling_strategy='water_column' - sheet_type='measured'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1xfrqraPa0auQ1O-C9RUo68RhxrPCDWkVMCAUbj79AZI/gviz/tq?tqx=out:csv&sheet=measured\n",
      "Written ./logsheets/MBAL4_water_column_measured_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='BPNS' - sampling_strategy='water_column' - sheet_type='measured'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1mEi4Bd2YR63WD0j54FQ6QkzcUw_As9Wilue9kaXO2DE/gviz/tq?tqx=out:csv&sheet=measured\n",
      "Discarded 1 records leaving 320.\n",
      "Written ./logsheets/BPNS_water_column_measured_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='ROSKOGO' - sampling_strategy='water_column' - sheet_type='measured'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1BCu2pbuIS4f-yrw5Kw4uzfO9v6ryZJBtSF5WBLWG_-E/gviz/tq?tqx=out:csv&sheet=measured\n",
      "Written ./logsheets/ROSKOGO_water_column_measured_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='VB' - sampling_strategy='water_column' - sheet_type='measured'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1kSlTcNfXaCQv7fIKVbBnt5PmRW3_H7Trryh4FJHyLkE/gviz/tq?tqx=out:csv&sheet=measured\n",
      "Discarded 26 records leaving 754.\n",
      "Written ./logsheets/VB_water_column_measured_validated.csv\n",
      "Observatory OOB lacks valid sheet URL for water_column\n",
      "\n",
      "\n",
      "Processing observatory_id='EMT21' - sampling_strategy='water_column' - sheet_type='measured'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1RutVpduwL1mEed5jC2zLbs0DgkzAwZdsJ27fg3kb0Yg/gviz/tq?tqx=out:csv&sheet=measured\n",
      "Written ./logsheets/EMT21_water_column_measured_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='PiEGetxo' - sampling_strategy='water_column' - sheet_type='measured'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1KGcishLP5eAn6ZCnixn5wZIez1Y9MgmYoch2s7aefKM/gviz/tq?tqx=out:csv&sheet=measured\n",
      "Written ./logsheets/PiEGetxo_water_column_measured_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='RFormosa' - sampling_strategy='water_column' - sheet_type='measured'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/15f_yqrhvOr1MW-SHuv0JiKf0HRDy81uGZapJANxL54k/gviz/tq?tqx=out:csv&sheet=measured\n",
      "Written ./logsheets/RFormosa_water_column_measured_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='OSD74' - sampling_strategy='water_column' - sheet_type='measured'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1VATpzmhwmk8sYkhEjCbDUZHRiYuNGpSRefhXthZRbfQ/gviz/tq?tqx=out:csv&sheet=measured\n",
      "Written ./logsheets/OSD74_water_column_measured_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='AAOT' - sampling_strategy='water_column' - sheet_type='measured'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1hvLkBwiKTGTJDx19m_8e7qJ2lm9bwLLeVztMpxTLqnk/gviz/tq?tqx=out:csv&sheet=measured\n",
      "Written ./logsheets/AAOT_water_column_measured_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='NRMCB' - sampling_strategy='water_column' - sheet_type='measured'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1F4AWv_seI-DQJ_Gp_N2ziwvGyjnc4KC92GGNXrJRek4/gviz/tq?tqx=out:csv&sheet=measured\n",
      "Written ./logsheets/NRMCB_water_column_measured_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='HCMR-1' - sampling_strategy='water_column' - sheet_type='measured'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/13DcVK2mzSxMJoFydSBaIMmj7Td1_JapEvcY2bmZTyLc/gviz/tq?tqx=out:csv&sheet=measured\n",
      "Written ./logsheets/HCMR-1_water_column_measured_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='IUIEilat' - sampling_strategy='water_column' - sheet_type='measured'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1qIwi1nZu4OBiriCzn1fEG1fyG8-3wSVvwzsMiH6JTwk/gviz/tq?tqx=out:csv&sheet=measured\n",
      "Written ./logsheets/IUIEilat_water_column_measured_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='UMF' - sampling_strategy='water_column' - sheet_type='measured'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1-1VsUUbRtKselxu-y2BghRIrJdaf74SbmvK42ntvd20/gviz/tq?tqx=out:csv&sheet=measured\n",
      "Written ./logsheets/UMF_water_column_measured_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='LMO' - sampling_strategy='water_column' - sheet_type='measured'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1AvQMYcS0tdNMw6Er8zUarQg1a_wrshhnkTS6RuI1FJQ/gviz/tq?tqx=out:csv&sheet=measured\n",
      "Written ./logsheets/LMO_water_column_measured_validated.csv\n",
      "Observatory ESC68N lacks valid sheet URL for soft_sediment\n",
      "Observatory Bergen lacks valid sheet URL for soft_sediment\n",
      "Observatory MBAL4 lacks valid sheet URL for soft_sediment\n",
      "\n",
      "\n",
      "Processing observatory_id='BPNS' - sampling_strategy='soft_sediment' - sheet_type='measured'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1zc0bZdpl-Eoi35lI_5BGkElbscplyQRyNPLkSgeEyEQ/gviz/tq?tqx=out:csv&sheet=measured\n",
      "Discarded 32 records leaving 143.\n",
      "Written ./logsheets/BPNS_soft_sediment_measured_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='ROSKOGO' - sampling_strategy='soft_sediment' - sheet_type='measured'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1M0ytBWbUDP0YthGtTVtdIaX2cQUxF_uNIJBHIXREoX8/gviz/tq?tqx=out:csv&sheet=measured\n",
      "Written ./logsheets/ROSKOGO_soft_sediment_measured_validated.csv\n",
      "Observatory VB lacks valid sheet URL for soft_sediment\n",
      "\n",
      "\n",
      "Processing observatory_id='OOB' - sampling_strategy='soft_sediment' - sheet_type='measured'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1J469_ljfxM9NhOEyRmvWQNOXX7dujxBy1opU6ROmDv8/gviz/tq?tqx=out:csv&sheet=measured\n",
      "Discarded 14 records leaving 118.\n",
      "Written ./logsheets/OOB_soft_sediment_measured_validated.csv\n",
      "\n",
      "\n",
      "Processing observatory_id='EMT21' - sampling_strategy='soft_sediment' - sheet_type='measured'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1sBB0x6h-prnUHMcOfms_7qSqTn6zeGVriitTysceoT0/gviz/tq?tqx=out:csv&sheet=measured\n",
      "Written ./logsheets/EMT21_soft_sediment_measured_validated.csv\n",
      "Observatory PiEGetxo lacks valid sheet URL for soft_sediment\n",
      "Observatory Plenzia lacks valid sheet URL for soft_sediment\n",
      "\n",
      "\n",
      "Processing observatory_id='RFormosa' - sampling_strategy='soft_sediment' - sheet_type='measured'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/17rqrZ-qrDP77SpoEyBFO17wewp98a-tLBWTlBAn6ZCc/gviz/tq?tqx=out:csv&sheet=measured\n",
      "Discarded 1 records leaving 84.\n",
      "Written ./logsheets/RFormosa_soft_sediment_measured_validated.csv\n",
      "Observatory OSD74 lacks valid sheet URL for soft_sediment\n",
      "Observatory AAOT lacks valid sheet URL for soft_sediment\n",
      "\n",
      "\n",
      "Processing observatory_id='NRMCB' - sampling_strategy='soft_sediment' - sheet_type='measured'\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1a19q3w2sP7dae8HEzT5TdmfqhRQQd3GItFqiYxWJlxs/gviz/tq?tqx=out:csv&sheet=measured\n",
      "Written ./logsheets/NRMCB_soft_sediment_measured_validated.csv\n",
      "Observatory HCMR-1 lacks valid sheet URL for soft_sediment\n",
      "Observatory IUIEilat lacks valid sheet URL for soft_sediment\n",
      "Observatory LMO lacks valid sheet URL for soft_sediment\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "    \n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import pydantic\n",
    "from validation_classes import samplingModel, measuredModel\n",
    "\n",
    "def get_sheet(sheet_type: str, sheet_link: str, format_type: str = None) -> pd.core.frame.DataFrame:\n",
    "        \"\"\"Returns a Pandas dataframe of the 'sampling' or 'measured' sheets\n",
    "        from the observatories' Google Sheets.\n",
    "\n",
    "        CSV has a problem with the word \"blank\" in the replicated field.\n",
    "        But none of the others work :)\n",
    "        \"\"\"\n",
    "        sampling_sheet_base = sheet_link.split(\"/edit\")[0]\n",
    "        if format_type == None:\n",
    "            #should return json\n",
    "            sampling_sheet_suffix = f\"/gviz/tq?tqx=sheet={sheet_type}\"\n",
    "            sample_sheet_link = sampling_sheet_base + sampling_sheet_suffix\n",
    "            print(f\"Sample sheet link: {sample_sheet_link}\")\n",
    "            df = pd.read_json(sample_sheet_link)                   \n",
    "        elif format_type == \"csv\":\n",
    "            sampling_sheet_suffix = f\"/gviz/tq?tqx=out:csv&sheet={sheet_type}\"\n",
    "            sample_sheet_link = sampling_sheet_base + sampling_sheet_suffix\n",
    "            print(f\"Sample sheet link: {sample_sheet_link}\")\n",
    "            df = pd.read_csv(sample_sheet_link, encoding='utf-8')\n",
    "        elif format_type == \"excel\":\n",
    "            sampling_sheet_suffix = f\"/gviz/tq?tqx=out:tsv-xlsx&sheet={sheet_type}\"\n",
    "            sample_sheet_link = sampling_sheet_base + sampling_sheet_suffix\n",
    "            print(f\"Sample sheet link: {sample_sheet_link}\")\n",
    "            df = pd.read_excel(sample_sheet_link, engine='openpyxl')\n",
    "        elif format_type == \"json\": \n",
    "            sampling_sheet_suffix = f\"/gviz/tq?tqx=out:json&sheet={sheet_type}\"\n",
    "            sample_sheet_link = sampling_sheet_base + sampling_sheet_suffix\n",
    "            print(f\"Sample sheet link: {sample_sheet_link}\")\n",
    "            df = pd.read_json(sample_sheet_link)\n",
    "        elif format_type == \"tsv\": \n",
    "            sampling_sheet_suffix = f\"/gviz/tq?tqx=out:tsv&sheet={sheet_type}\"\n",
    "            sample_sheet_link = sampling_sheet_base + sampling_sheet_suffix\n",
    "            print(f\"Sample sheet link: {sample_sheet_link}\")\n",
    "            df = pd.read_csv(sample_sheet_link,  sep='\\t')            \n",
    "        else:\n",
    "            raise ValueError(f\"Unrecognised {format_type=}\")\n",
    "        return df\n",
    "\n",
    "def parse_sample_sheets(sampling_strategy: str, sheet_type: str, addresses: list[str, str]) -> None: \n",
    "    for observatory in addresses:\n",
    "        observatory_id, sheet_link = observatory\n",
    "        #print(f\"Observatory_id {observatory_id} sheet_link {sheet_link}\")\n",
    "        if not isinstance(sheet_link, str):\n",
    "            #print(f\"This is the sheet_link type {type(sheet_link)}\")\n",
    "            if isinstance(sheet_link, float): \n",
    "                if math.isnan(sheet_link):\n",
    "                    print(f\"Observatory {observatory_id} lacks valid sheet URL for {sampling_strategy}\")\n",
    "                    continue\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown link {sheet_link} to observatory {observatory_id}\")\n",
    "        else:\n",
    "\n",
    "            if observatory_id == \"Plenzia\": continue # Sheets not publically available\n",
    "            # UMF soft_sed has two source_mat_ids\n",
    "            if sampling_strategy == \"soft_sediment\" and observatory_id == \"UMF\":\n",
    "                continue\n",
    "            \n",
    "            #if observatory_id in [\"BPNS\", \"Bergen\", \"ESC68N\", \"MBAL4\",\n",
    "            #                     \"VB\", \"ROSKOGO\"]:\n",
    "            #[\"Bergen\", \"MBAL4\", \"ESC68N\", \n",
    "            #               \"BPNS\", \"VB\", \"ROSKOGO\",\n",
    "            #               \"EMT21\", \"PiEGetxo\", \"RFormosa\",\n",
    "            #              \"OSD74\", \"AAOT\", \"NRMCB\",\n",
    "            #              \"HCMR-1\", \"IUIEilat\", \"UMF\"]:\n",
    "            #    continue\n",
    "    \n",
    "            #if observatory_id != \"BPNS\":\n",
    "            #   continue\n",
    "    \n",
    "            print(f\"\\n\\nProcessing {observatory_id=} - {sampling_strategy=} - {sheet_type=}\")\n",
    "            # Assuming either 'sampling' or 'measured' for sheet_type\n",
    "            df = get_sheet(sheet_type, sheet_link, format_type=\"csv\")\n",
    "\n",
    "            # CSV sheets are missing the \"blank\" word in the \"replicate field of the 'sampling' sheets: fix\n",
    "            #if sheet_type == \"sampling\":\n",
    "            #    df = correct_replicate_field(df)\n",
    "            #break\n",
    "            \n",
    "            data_records_all = df.to_dict(orient=\"records\")\n",
    "    \n",
    "            # Many sheets have partially filled rows\n",
    "            # The source_mat_id is manually curated and the PRIMARY_KEY\n",
    "            # Therefore filter records on source_mat_id\n",
    "            def filter_on_source_mat_id(d):\n",
    "                # Bergen has it as source_material_id\n",
    "                try:\n",
    "                    value = d[\"source_mat_id\"]\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        value = d[\"source_material_id\"]\n",
    "                    except KeyError:\n",
    "                        raise ValueError(\"Cannot find source_mat_id field\")\n",
    "                if isinstance(value, float):\n",
    "                    if math.isnan(value):\n",
    "                        return False\n",
    "                elif value is None:\n",
    "                    return False\n",
    "                # Remove mis-formatted\n",
    "                elif len(value.split(\"_\")) < 6:\n",
    "                    return False\n",
    "                #Edge case of this otherwise blank entry having 6 \"bits\"\n",
    "                elif value == \"EMOBON_VB_Wa_230509_um_\":\n",
    "                    return False \n",
    "                else:\n",
    "                    return True\n",
    "                \n",
    "            data_records_filtered = list(filter(filter_on_source_mat_id, data_records_all))\n",
    "    \n",
    "            if len(data_records_all) > len(data_records_filtered):\n",
    "                print(f\"Discarded {len(data_records_all) - len(data_records_filtered)} records leaving {len(data_records_filtered)}.\")\n",
    "            \n",
    "            validator = validator_classes[sheet_type]\n",
    "            validated_rows = [validator(**row).model_dump() for row in data_records_filtered]\n",
    "    \n",
    "            #for record in validated_rows:\n",
    "            #    for field in record:\n",
    "            #        print(f\"Record {field} has value {record[field]} is type {type(record[field])}\")\n",
    "    \n",
    "            save_dir = \"./logsheets\"\n",
    "            outfile_name = f\"{observatory_id}_{sampling_strategy}_{sheet_type}_validated.csv\"\n",
    "            ndf = pd.DataFrame.from_records(validated_rows, index=\"source_mat_id\")\n",
    "            ndf.to_csv(os.path.join(save_dir, outfile_name))\n",
    "            print(f\"Written {os.path.join(save_dir, outfile_name)}\")\n",
    "\n",
    "validator_classes = {\"sampling\": samplingModel, \"measured\": measuredModel}\n",
    "#Get list of observatory ids\n",
    "df = pd.read_csv(\"./governance/observatories_validated.csv\")\n",
    "observatory_ids = [id[0] for id in df[[\"observatory_id\"]].values.tolist()]\n",
    "print(f\"{observatory_ids=}\")\n",
    "# Get list of all URL links to sampling sheets\n",
    "# NB  you cant use a \"with\" closure here when reading the Pandas df\n",
    "governance_logsheets_validated_csv = \"./governance/logsheets_validated.csv\"\n",
    "df = pd.read_csv(governance_logsheets_validated_csv)\n",
    "water_column_sheet_addresses = df[[\"observatory_id\", \"water_column\"]].values.tolist()\n",
    "soft_sediment_sheet_addresses  = df[[\"observatory_id\", \"soft_sediment\"]].values.tolist()\n",
    "    \n",
    "parse_sample_sheets(\"water_column\", \"sampling\", water_column_sheet_addresses)\n",
    "parse_sample_sheets(\"soft_sediment\", \"sampling\", soft_sediment_sheet_addresses)\n",
    "parse_sample_sheets(\"water_column\", \"measured\", water_column_sheet_addresses)\n",
    "parse_sample_sheets(\"soft_sediment\", \"measured\", soft_sediment_sheet_addresses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2e4fbb-ff53-4058-8fc9-90dfef6423b4",
   "metadata": {},
   "source": [
    "### Version 2: pulls from Google Sheets, does \"lax\" (default), \"strict\" and \"semi-strict\" validation depending on constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec0e469-3c57-4f9c-a41d-9a343a235bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "    \n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pydantic import ValidationError\n",
    "from pprint import pprint\n",
    "from validation_classes import samplingModel, measuredModel, samplingModelStrict, samplingModelSemiStrict\n",
    "\n",
    "############################ CAUTION #############################################\n",
    "STRICT      = False  # As defined by Ioulia, dates corrected, NA's removed etc\n",
    "SEMI_STRICT = True   # As defined by Ioulia but not checking for mandatory fields\n",
    "                     # ints and str coerced to floats when possible\n",
    "##################################################################################\n",
    "\n",
    "def parse_sample_sheets(sampling_strategy: str,\n",
    "                        sheet_type: str,\n",
    "                        addresses: pd.core.frame.DataFrame,\n",
    "                       ) -> None:\n",
    "    \n",
    "    for observatory in addresses:\n",
    "        observatory_id, sheet_link = observatory\n",
    "        #print(f\"Observatory_id {observatory_id} sheet_link {sheet_link}\")\n",
    "        if not isinstance(sheet_link, str):\n",
    "            #print(f\"This is the sheet_link type {type(sheet_link)}\")\n",
    "            if isinstance(sheet_link, float): \n",
    "                if math.isnan(sheet_link):\n",
    "                    print(f\"Observatory {observatory_id} lacks valid sheet URL {sheet_link}\")\n",
    "                    continue\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown link {sheet_link} to observatory {observatory_id}\")\n",
    "        else:\n",
    "\n",
    "            if observatory_id == \"Plenzia\": continue # Sheets not publically available\n",
    "            # UMF soft_sed has two source_mat_ids\n",
    "            if sampling_strategy == \"soft_sediment\" and observatory_id == \"UMF\":\n",
    "                continue\n",
    "    \n",
    "            print(f\"Processing {observatory_id}...\")\n",
    "            sampling_sheet_base = sheet_link.split(\"/edit\")[0]\n",
    "            sampling_sheet_suffix = \"/gviz/tq?tqx=out:csv&sheet=%s\"\n",
    "            sample_sheet_link = sampling_sheet_base + sampling_sheet_suffix % sheet_type\n",
    "            print(f\"Sample sheet link: {sample_sheet_link}\")\n",
    "            df = pd.read_csv(sample_sheet_link, encoding='utf-8')\n",
    "            data_records_all = df.to_dict(orient=\"records\")\n",
    "    \n",
    "            # Many sheets have partially filled rows\n",
    "            # The source_mat_id is manually curated and the PRIMARY_KEY\n",
    "            # Therefore filter records on source_mat_id\n",
    "            def filter_on_source_mat_id(d):\n",
    "                # Bergen has it as source_material_id\n",
    "                try:\n",
    "                    value = d[\"source_mat_id\"]\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        value = d[\"source_material_id\"]\n",
    "                    except KeyError:\n",
    "                        raise ValueError(\"Cannot find source_mat_id field\")\n",
    "                if isinstance(value, float):\n",
    "                    if math.isnan(value):\n",
    "                        return False\n",
    "                elif value is None:\n",
    "                    return False\n",
    "                # Remove mis-formatted\n",
    "                elif len(value.split(\"_\")) < 6:\n",
    "                    return False\n",
    "                #Edge case of this otherwise blank entry having 6 \"bits\"\n",
    "                elif value == \"EMOBON_VB_Wa_230509_um_\":\n",
    "                    return False \n",
    "                else:\n",
    "                    return True\n",
    "                \n",
    "            data_records_filtered = list(filter(filter_on_source_mat_id, data_records_all))\n",
    "    \n",
    "            if len(data_records_all) > len(data_records_filtered):\n",
    "                print(f\"Discarded {len(data_records_all) - len(data_records_filtered)} records leaving {len(data_records_filtered)}.\")\n",
    "\n",
    "            if STRICT:\n",
    "                model_type = f\"{sheet_type}_strict\"\n",
    "            elif SEMI_STRICT:\n",
    "                model_type = f\"{sheet_type}_semistrict\"\n",
    "            else:\n",
    "                model_type = sheet_type\n",
    "\n",
    "            validator = validator_classes[model_type]\n",
    "\n",
    "            #validated_rows = [validator(**row).model_dump() for row in data_records_filtered]\n",
    "            validated_rows = []\n",
    "            errors: List[List[str:List[Dict]]] = [] # where each error is the inner Dict\n",
    "            for row in data_records_filtered:\n",
    "                try:\n",
    "                    vr = validator(**row)\n",
    "                except ValidationError as e:\n",
    "                    if observatory_id == \"Bergen\":\n",
    "                        errors.append([(row[\"source_material_id\"], e.errors())])\n",
    "                    else:\n",
    "                        errors.append([(row[\"source_mat_id\"], e.errors())])\n",
    "                else:\n",
    "                    validated_rows.append(vr.model_dump())\n",
    "\n",
    "            if errors:\n",
    "                # errors is a list of lists where each inner list is a dict of row errors\n",
    "                # where each isof key = source_mat_id and values is list of dicts each of which\n",
    "                # is an error:\n",
    "                #List[List[str:List[Dict]]]\n",
    "                total_number_errors = sum([len(row[1]) for e in errors for row in e])\n",
    "                print(f\"Errors were found... {total_number_errors} in total\")\n",
    "                save_dir = \"./validation_errors\"\n",
    "                outfile_name_pk = f\"{observatory_id}_{sampling_strategy}_{model_type}_ERRORS.pickle\"\n",
    "                out_path_pk = os.path.join(save_dir, outfile_name_pk)\n",
    "                with open(out_path_pk, \"wb\") as f:\n",
    "                    pickle.dump(errors, f, pickle.HIGHEST_PROTOCOL)\n",
    "                outfile_name_log = f\"{observatory_id}_{sampling_strategy}_{model_type}_ERRORS.log\"\n",
    "                out_path_log = os.path.join(save_dir, outfile_name_log)                \n",
    "                with open(out_path_log, \"w\") as f:\n",
    "                    pprint(errors, f)\n",
    "            else:\n",
    "                assert len(validated_rows) == len(data_records_filtered), \"Not sure what happenned, but len(validated_rows) != len(data_filtered_records)\"\n",
    "                print(\"All records passed!\")\n",
    "            \n",
    "                #for record in validated_rows:\n",
    "                #    for field in record:\n",
    "                #        print(f\"Record {field} has value {record[field]} is type {type(record[field])}\")\n",
    "\n",
    "                if not STRICT and not SEMI_STRICT:\n",
    "                    save_dir = \"./logsheets\"\n",
    "                    outfile_name = f\"{observatory_id}_{sampling_strategy}_{model_type}_validated.csv\"\n",
    "                    ndf = pd.DataFrame.from_records(validated_rows, index=\"source_mat_id\")\n",
    "                    ndf.to_csv(os.path.join(save_dir, outfile_name))\n",
    "                    print(f\"Written {os.path.join(save_dir, outfile_name)}\")\n",
    "\n",
    "\n",
    "validator_classes = {\"sampling\": samplingModel,\n",
    "                     \"measured\": measuredModel,\n",
    "                     \"sampling_strict\": samplingModelStrict,\n",
    "                     \"sampling_semistrict\": samplingModelSemiStrict\n",
    "                    }\n",
    "\n",
    "# Get list of all URL links to sampling sheets\n",
    "# NB  you cant use a \"with\" closure here when reading the Pandas df\n",
    "governance_logsheets_validated_csv = \"./governance/logsheets_validated.csv\"\n",
    "df = pd.read_csv(governance_logsheets_validated_csv)\n",
    "water_column_sheet_addresses = df[[\"observatory_id\", \"water_column\"]].values.tolist()\n",
    "soft_sediment_sheet_addresses  = df[[\"observatory_id\", \"soft_sediment\"]].values.tolist()\n",
    "del df\n",
    "\n",
    "parse_sample_sheets(\"water_column\", \"sampling\", water_column_sheet_addresses)\n",
    "\n",
    "#parse_sample_sheets(\"water_column\", \"sampling\", water_column_sheet_addresses)\n",
    "#parse_sample_sheets(\"soft_sediment\", \"sampling\", soft_sediment_sheet_addresses)\n",
    "#parse_sample_sheets(\"water_column\", \"measured\", water_column_sheet_addresses)\n",
    "#parse_sample_sheets(\"soft_sediment\", \"measured\", soft_sediment_sheet_addresses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dbec45-35c7-41f6-a245-8a81cf547cf9",
   "metadata": {},
   "source": [
    "### Version 3: pulls sheets from Github after curation, does \"lax\", \"strict\", and \"semi-strict\" validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b732a61-afa4-4281-8acd-1a8ad3f46671",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "    \n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "from urllib.request import HTTPError\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "from pydantic import ValidationError\n",
    "from pprint import pprint\n",
    "from validation_classes import (samplingModelGithub,               # lax validator for EMO-BON Github repository\n",
    "                                samplingModelGithubStrict,         # strict validator for EMO-BON Github repository\n",
    "                                samplingModelGithubSemiStrict      # semi-strict validator for EMO-BON Github repository                  \n",
    "                               )\n",
    "class SamplingStrategy(Enum):\n",
    "    WATER    = \"water\"    # Originally water_column\n",
    "    SEDIMENT = \"sediment\" # Originally soft_sediment\n",
    "\n",
    "class SheetType(Enum):\n",
    "    SAMPLING = \"sampling\"\n",
    "    MEASURED = \"measured\"\n",
    "\n",
    "# Not all observatories have \"transformed\" sheets on GH, but may have \"raw\"\n",
    "# Of course the types in the fields are different to difficult to validate\n",
    "# with a single validator - best just to ignore the raw sheets\n",
    "USE_RAW = False\n",
    "\n",
    "############################ CAUTION #############################################\n",
    "# As defined by Ioulia, dates corrected, NA's removed etc\n",
    "STRICT      = False\n",
    "\n",
    "# As defined by Ioulia but not checking for mandatory fields\n",
    "# ints and str coerced to floats when possible\n",
    "SEMI_STRICT = False  \n",
    "##################################################################################\n",
    "\n",
    "def get_sheet_from_github(observatory_id: SheetType,\n",
    "                          sampling_strategy: SamplingStrategy, \n",
    "                          sheet_type: str) -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    Here we pull the \"sampling\" or \"measured\" sheets from Github. These are the curated\n",
    "    sheets downloaded by the Github actions and hopefully do not have the errors that\n",
    "    the CSV's pulled directly from Google Sheets had (e.g. the word \"blank\" magically\n",
    "    disappering from the \"replicate\" field.)\n",
    "\n",
    "    Github paths look like:\n",
    "    https://raw.githubusercontent.com/emo-bon/observatory-umf-crate/main/logsheets/transformed/sediment_measured.csv\n",
    "    https://raw.githubusercontent.com/emo-bon/observatory-bergen-crate/main/logsheets/raw/water_sampling.csv\n",
    "\n",
    "    The problem at 29-08-2024 is that the data are out of date.\n",
    "    \"\"\"\n",
    "\n",
    "    prefix     = \"https://raw.githubusercontent.com/emo-bon\"\n",
    "    obs_name   = f\"observatory-{observatory_id}-crate\"\n",
    "    inter_path = \"main/logsheets\"\n",
    "    dir_path   = \"transformed\"\n",
    "    sheet_name = f\"{sampling_strategy}_{sheet_type}.csv\"\n",
    "\n",
    "    print(f\"Processing {observatory_id}... {sheet_name}\")\n",
    "    github_addr = os.path.join(prefix, obs_name, inter_path, dir_path, sheet_name)\n",
    "    try:\n",
    "        df = pd.read_csv(github_addr)\n",
    "    except HTTPError:\n",
    "        # Some observatories don't yet have transformed sheets\n",
    "        if USE_RAW:\n",
    "            # Try for the raw sheets\n",
    "            print(\"Unable to find 'transformed' sheet, reading the 'raw' sheet\")\n",
    "            dir_path   = \"raw\"\n",
    "            github_addr = os.path.join(prefix, obs_name, inter_path, dir_path, sheet_name)\n",
    "            try:\n",
    "                df = pd.read_csv(github_addr)\n",
    "            except HTTPError:\n",
    "                raise ValueError(\"Unable to find transformed or raw sheet\")\n",
    "        else:\n",
    "            print(f\"Observatory {observatory_id} does not have a transformed {sheet_name} on GH\")\n",
    "            return None\n",
    "            \n",
    "    return df\n",
    "\n",
    "def filter_on_source_mat_id(d):\n",
    "    # Bergen has it as source_material_id on Google and Github\n",
    "    try:\n",
    "        value = d[\"source_mat_id\"]\n",
    "    except KeyError:\n",
    "        try:\n",
    "            value = d[\"source_material_id\"]\n",
    "        except KeyError:\n",
    "            raise ValueError(\"Cannot find source_mat_id field\")\n",
    "    if isinstance(value, float):\n",
    "        if math.isnan(value):\n",
    "            return False\n",
    "    elif value is None:\n",
    "        return False\n",
    "    # Remove mis-formatted\n",
    "    elif len(value.split(\"_\")) < 6:\n",
    "        return False\n",
    "    #Edge case of this otherwise blank entry having 6 \"bits\"\n",
    "    elif value == \"EMOBON_VB_Wa_230509_um_\":\n",
    "        return False \n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def parse_sample_sheets(sampling_strategy: str,\n",
    "                        sheet_type: str,\n",
    "                        addresses: pd.core.frame.DataFrame,\n",
    "                       ) -> None:\n",
    "    \n",
    "    for observatory in addresses:\n",
    "        observatory_id, sheet_link = observatory\n",
    "        #print(f\"ObSservatory_id {observatory_id} sheet_link {sheet_link}\")\n",
    "        if not isinstance(sheet_link, str):\n",
    "            #print(f\"This is the sheet_link type {type(sheet_link)}\")\n",
    "            if isinstance(sheet_link, float):\n",
    "                # Only OOB doesnt do water_column\n",
    "                # But most do not do soft-sediments\n",
    "                if math.isnan(sheet_link):\n",
    "                    print(f\"Observatory {observatory_id} does not have a {sampling_strategy} sampling strategy.\")\n",
    "                    continue\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown value \\'{sheet_link}\\' in {sampling_strategy} cell of {observatory_id}\")\n",
    "        else:\n",
    "\n",
    "            if observatory_id == \"Plenzia\": continue # Sheets not publically available\n",
    "\n",
    "            ################ CAUTION ##################\n",
    "            #if not observatory_id in [\"AAOT\"]: continue\n",
    "            \n",
    "            # UMF soft_sed has two source_mat_ids\n",
    "            if sampling_strategy == \"sediment\" and observatory_id == \"UMF\":\n",
    "                continue\n",
    "    \n",
    "            df = get_sheet_from_github(observatory_id, sampling_strategy, sheet_type)\n",
    "            if df is None:\n",
    "                continue\n",
    "            data_records_all = df.to_dict(orient=\"records\")\n",
    "    \n",
    "            # Many sheets have partially filled rows\n",
    "            # The source_mat_id is manually curated and the PRIMARY_KEY\n",
    "            # Therefore filter records on source_mat_id\n",
    "                \n",
    "            data_records_filtered = list(filter(filter_on_source_mat_id, data_records_all))\n",
    "    \n",
    "            if len(data_records_all) > len(data_records_filtered):\n",
    "                print(f\"Discarded {len(data_records_all) - len(data_records_filtered)} records leaving {len(data_records_filtered)}.\")\n",
    "\n",
    "            ################ CAUTION ##############\n",
    "            #continue\n",
    "\n",
    "            if STRICT:\n",
    "                model_type = f\"{sheet_type}_github_strict\"\n",
    "            elif SEMI_STRICT:\n",
    "                model_type = f\"{sheet_type}_github_semistrict\"\n",
    "            else:\n",
    "                model_type = f\"{sheet_type}_github\"\n",
    "\n",
    "            validator = validator_classes[model_type]\n",
    "            #print(f\"Using {validator} from {model_type}\")\n",
    "\n",
    "            #validated_rows = [validator(**row).model_dump() for row in data_records_filtered]\n",
    "            validated_rows = []\n",
    "            errors: List[List[str:List[Dict]]] = [] # where each error is the inner Dict\n",
    "            for row in data_records_filtered:\n",
    "                try:\n",
    "                    vr = validator(**row)\n",
    "                except ValidationError as e:\n",
    "                    if observatory_id == \"Bergen\":\n",
    "                        errors.append([(row[\"source_material_id\"], e.errors())])\n",
    "                    else:\n",
    "                        errors.append([(row[\"source_mat_id\"], e.errors())])\n",
    "                else:\n",
    "                    validated_rows.append(vr.model_dump())\n",
    "\n",
    "            if errors:\n",
    "                # errors is a list of lists where each inner list is a dict of row errors\n",
    "                # where each isof key = source_mat_id and values is list of dicts each of which\n",
    "                # is an error:\n",
    "                #List[List[str:List[Dict]]]\n",
    "                total_number_errors = sum([len(row[1]) for e in errors for row in e])\n",
    "                print(f\"Errors were found... {total_number_errors} in total\")\n",
    "                save_dir = \"./validation_errors_github\"\n",
    "                #outfile_name_pk = f\"{observatory_id}_{sampling_strategy}_{model_type}_ERRORS.pickle\"\n",
    "                #out_path_pk = os.path.join(save_dir, outfile_name_pk)\n",
    "                #with open(out_path_pk, \"wb\") as f:\n",
    "                #    pickle.dump(errors, f, pickle.HIGHEST_PROTOCOL)\n",
    "                outfile_name_log = f\"{observatory_id}_{sampling_strategy}_{model_type}_ERRORS.log\"\n",
    "                out_path_log = os.path.join(save_dir, outfile_name_log)                \n",
    "                with open(out_path_log, \"w\") as f:\n",
    "                    pprint(errors, f)\n",
    "            else:\n",
    "                assert len(validated_rows) == len(data_records_filtered), \\\n",
    "                    \"Not sure what happenned, but len(validated_rows) != len(data_filtered_records)\"\n",
    "                print(\"All records passed!\")\n",
    "            \n",
    "                #for record in validated_rows:\n",
    "                #    for field in record:\n",
    "                #        print(f\"Record {field} has value {record[field]} is type {type(record[field])}\")\n",
    "\n",
    "                if not STRICT and not SEMI_STRICT:\n",
    "                    save_dir = \"./logsheets_github\"\n",
    "                    outfile_name = f\"{observatory_id}_{sampling_strategy}_{model_type}_validated.csv\"\n",
    "                    ndf = pd.DataFrame.from_records(validated_rows, index=\"source_mat_id\")\n",
    "                    ndf.to_csv(os.path.join(save_dir, outfile_name))\n",
    "                    print(f\"Written {os.path.join(save_dir, outfile_name)}\")\n",
    "\n",
    "validator_classes = {\"sampling_github\"           : samplingModelGithub, \n",
    "                     \"sampling_github_strict\"    : samplingModelGithubStrict,\n",
    "                     \"sampling_github_semistrict\": samplingModelGithubSemiStrict\n",
    "                    }\n",
    "\n",
    "# Get list of all URL links to sampling sheets\n",
    "# NB  you cant use a \"with\" closure here when reading the Pandas df\n",
    "governance_logsheets_validated_csv = \"./governance/logsheets_validated.csv\"\n",
    "df = pd.read_csv(governance_logsheets_validated_csv)\n",
    "water_column_sheet_addresses = df[[\"observatory_id\", \"water_column\"]].values.tolist()\n",
    "soft_sediment_sheet_addresses  = df[[\"observatory_id\", \"soft_sediment\"]].values.tolist()\n",
    "del df\n",
    "\n",
    "parse_sample_sheets(\"water\", \"sampling\", water_column_sheet_addresses)\n",
    "parse_sample_sheets(\"sediment\", \"sampling\", soft_sediment_sheet_addresses)\n",
    "#parse_sample_sheets(\"water\", \"measured\", water_column_sheet_addresses)\n",
    "#parse_sample_sheets(\"sediment\", \"measured\", soft_sediment_sheet_addresses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c0a641-6fa8-48d5-b3d3-0785beaf2f68",
   "metadata": {},
   "source": [
    "# Create meta-table of all logsheets.sheets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2c5120-9f96-45a0-b9ce-bd3c79119864",
   "metadata": {},
   "source": [
    "### source_mat_id translations between the Batch 1 & 2 run_information log sheets and the observatory \"sample\" and \"measured\" sheets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3cbb42-7bd0-4a45-9ee4-a6d30ce44758",
   "metadata": {},
   "source": [
    "The source_mat_id is supposed to be the unique key or identifier that links the records in the Batch run_information sheets ([run-information-batch-001.csv](https://raw.githubusercontent.com/emo-bon/sequencing-data/main/shipment/batch-001/run-information-batch-001.csv) and [run-information-batch-002.csv](https://raw.githubusercontent.com/emo-bon/sequencing-data/main/shipment/batch-002/run-information-batch-002.csv)) to the sampling events in the \"sampling\" and \"measured\" sheets of the observatory logsheets (Google Sheets) (e.g. [ESC68N](https://docs.google.com/spreadsheets/d/11_Eu0W1-sDiuzKx1cIl6YuxjRHmWezN6u9v3Ly8JZ3A/edit?gid=0#gid=0)). However, it is not always the case (90 out of 277 in Batch 1 & 2). The source_mat_id in the sampling and measured sheets are auto-formated (=CONCATENATE(observatory!$A$2,\"_\",H2,\"_\",Q2,\"um\",\"_\",M2)) from other fields including the Q2 \"size_frac_low\" field which is either an int or a float (defined as an integer).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "33792f86-8da9-4cd5-9986-29a80aedfbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observatory ESC68N-water_column has 150 sampling events.\n",
      "136 have no ref_code (i.e. they were not sent for sequencing), \n",
      "0 'sampling' events have a refcode but no 'measured' data with the corresponding source_mat_id: \n",
      "A total of 14 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory Bergen-water_column has 140 sampling events.\n",
      "140 have no ref_code (i.e. they were not sent for sequencing), \n",
      "0 'sampling' events have a refcode but no 'measured' data with the corresponding source_mat_id: \n",
      "A total of 0 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory MBAL4-water_column has 80 sampling events.\n",
      "72 have no ref_code (i.e. they were not sent for sequencing), \n",
      "0 'sampling' events have a refcode but no 'measured' data with the corresponding source_mat_id: \n",
      "A total of 8 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory BPNS-water_column has 320 sampling events.\n",
      "302 have no ref_code (i.e. they were not sent for sequencing), \n",
      "0 'sampling' events have a refcode but no 'measured' data with the corresponding source_mat_id: \n",
      "A total of 18 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory BPNS-soft_sediment has 171 sampling events.\n",
      "163 have no ref_code (i.e. they were not sent for sequencing), \n",
      "0 'sampling' events have a refcode but no 'measured' data with the corresponding source_mat_id: \n",
      "A total of 8 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory ROSKOGO-water_column has 291 sampling events.\n",
      "272 have no ref_code (i.e. they were not sent for sequencing), \n",
      "0 'sampling' events have a refcode but no 'measured' data with the corresponding source_mat_id: \n",
      "A total of 17 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory ROSKOGO-soft_sediment has 155 sampling events.\n",
      "149 have no ref_code (i.e. they were not sent for sequencing), \n",
      "0 'sampling' events have a refcode but no 'measured' data with the corresponding source_mat_id: \n",
      "A total of 6 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory VB-water_column has 824 sampling events.\n",
      "806 have no ref_code (i.e. they were not sent for sequencing), \n",
      "0 'sampling' events have a refcode but no 'measured' data with the corresponding source_mat_id: \n",
      "A total of 18 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory OOB-soft_sediment has 133 sampling events.\n",
      "126 have no ref_code (i.e. they were not sent for sequencing), \n",
      "0 'sampling' events have a refcode but no 'measured' data with the corresponding source_mat_id: \n",
      "A total of 7 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory EMT21-water_column has 265 sampling events.\n",
      "251 have no ref_code (i.e. they were not sent for sequencing), \n",
      "0 'sampling' events have a refcode but no 'measured' data with the corresponding source_mat_id: \n",
      "A total of 14 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory EMT21-soft_sediment has 48 sampling events.\n",
      "48 have no ref_code (i.e. they were not sent for sequencing), \n",
      "0 'sampling' events have a refcode but no 'measured' data with the corresponding source_mat_id: \n",
      "A total of 0 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory PiEGetxo-water_column has 239 sampling events.\n",
      "224 have no ref_code (i.e. they were not sent for sequencing), \n",
      "0 'sampling' events have a refcode but no 'measured' data with the corresponding source_mat_id: \n",
      "A total of 13 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory RFormosa-water_column has 170 sampling events.\n",
      "156 have no ref_code (i.e. they were not sent for sequencing), \n",
      "0 'sampling' events have a refcode but no 'measured' data with the corresponding source_mat_id: \n",
      "A total of 14 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory RFormosa-soft_sediment has 85 sampling events.\n",
      "79 have no ref_code (i.e. they were not sent for sequencing), \n",
      "0 'sampling' events have a refcode but no 'measured' data with the corresponding source_mat_id: \n",
      "A total of 6 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory OSD74-water_column has 170 sampling events.\n",
      "157 have no ref_code (i.e. they were not sent for sequencing), \n",
      "0 'sampling' events have a refcode but no 'measured' data with the corresponding source_mat_id: \n",
      "A total of 13 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory AAOT-water_column has 400 sampling events.\n",
      "382 have no ref_code (i.e. they were not sent for sequencing), \n",
      "0 'sampling' events have a refcode but no 'measured' data with the corresponding source_mat_id: \n",
      "A total of 18 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory NRMCB-water_column has 382 sampling events.\n",
      "364 have no ref_code (i.e. they were not sent for sequencing), \n",
      "0 'sampling' events have a refcode but no 'measured' data with the corresponding source_mat_id: \n",
      "A total of 18 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory NRMCB-soft_sediment has 95 sampling events.\n",
      "86 have no ref_code (i.e. they were not sent for sequencing), \n",
      "0 'sampling' events have a refcode but no 'measured' data with the corresponding source_mat_id: \n",
      "A total of 9 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory HCMR-1-water_column has 144 sampling events.\n",
      "134 have no ref_code (i.e. they were not sent for sequencing), \n",
      "0 'sampling' events have a refcode but no 'measured' data with the corresponding source_mat_id: \n",
      "A total of 10 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory IUIEilat-water_column has 60 sampling events.\n",
      "46 have no ref_code (i.e. they were not sent for sequencing), \n",
      "0 'sampling' events have a refcode but no 'measured' data with the corresponding source_mat_id: \n",
      "A total of 14 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory LMO-water_column has 100 sampling events.\n",
      "100 have no ref_code (i.e. they were not sent for sequencing), \n",
      "0 'sampling' events have a refcode but no 'measured' data with the corresponding source_mat_id: \n",
      "A total of 0 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "There are 227 total ref_codes assigned\n",
      "A total of 2 sequencing event in the Batch 1 & 2 run_information sheets have refcodes that match source_mat_ids in the sample sheets that have duplicate entries\n",
      "Total number of combined sampling events with ref_codes: 225\n",
      "Total number of all_source_mat_ids_from_sheets: 4422 of which 65 were duplicates\n",
      "A total of 0 source_mat_ids in the batch 1 & 2 run information sheets are missing from the observatory sampling sheets\n",
      "Total combined_events 225 + 0 missing events = 225 and should be equal to total number of refcodes assigned in the run information sheets 227 minus number of events ignored 2 because the refcodes match duplicate source_mat_ids in the sampling sheets = 225 + 0 = 227 - 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import difflib\n",
    "import pandas as pd\n",
    "import validators\n",
    "import collections\n",
    "from pprint import pprint\n",
    "\n",
    "######################## CAUTION #######################\n",
    "# It seems the source_mat_ids in the run-sheet do not match the source_mat_ids in the sampling sheets: \n",
    "# CHECK\n",
    "# EMOBON_OOB_So_210608_micro_1\n",
    "# How many source_mat_ids in the run-information-batch-001.csv sheets have not equivalents in the sampling sheets?\n",
    "######################## CAUTION #######################\n",
    "\n",
    "LOGSHEETS_PATH = \"./logsheets\"\n",
    "\n",
    "KNOWN_DUPLICATES = [\"EMOBON_ROSKOGO_Wa_210618_3um_1\", \"EMOBON_PiEGetxo_Wa_210824_3um_blank\"]\n",
    "\n",
    "def get_observatory_data() -> list[str, str, str]:\n",
    "    # Get list of observatory_ids\n",
    "    df = pd.read_csv(\"./governance/logsheets_validated.csv\")\n",
    "    observatory_data = df[[\"observatory_id\", \"water_column\", \"soft_sediment\"]].values.tolist()\n",
    "    return observatory_data\n",
    "\n",
    "def get_refcodes() -> dict[str, str]:\n",
    "    batch1_run_info_path = \"https://raw.githubusercontent.com/emo-bon/sequencing-data/main/shipment/batch-001/run-information-batch-001.csv\"\n",
    "    batch2_run_info_path = \"https://raw.githubusercontent.com/emo-bon/sequencing-data/main/shipment/batch-002/run-information-batch-002.csv\"\n",
    "    # Get list of batch1 <source_mat_id>, <ref_code>'s\n",
    "    df = pd.read_csv(batch1_run_info_path)\n",
    "    refcodes = {}\n",
    "    for i in df[[\"source_material_id\", \"ref_code\"]].values.tolist():\n",
    "        assert i[0] not in refcodes, f\"{source_material_id} maybe duplicated\"\n",
    "        refcodes.update(dict([i]))\n",
    "    # Get list of batch2 <source_mat_id>, <ref_code>'s\n",
    "    df = pd.read_csv(batch2_run_info_path)\n",
    "    b2_refcodes = {}\n",
    "    for i in df[[\"source_material_id\", \"ref_code\"]].values.tolist():\n",
    "        assert i[0] not in refcodes, f\"{source_material_id} maybe duplicated\"\n",
    "        b2_refcodes.update(dict([i]))\n",
    "    refcodes.update(b2_refcodes)\n",
    "    return refcodes\n",
    "\n",
    "def parse_observatory_sample_type(observatory_id: str,\n",
    "                                  sampling_type: str,\n",
    "                                  save_table: bool = False,\n",
    "                                  verbose: bool = True) -> list[dict[str, str]]:\n",
    "        \"\"\"An observatory is an EMBRC station and it has an ID\n",
    "           Each observatory may take either or both of the \"water_column\" and \"soft_sediment\" sampling types\n",
    "           Each sampling type has both a \"sampling\" and \"measured\" sheet\n",
    "\n",
    "           This function returns a list of sampling events each of which is a dict with key/value pairs for each field and value\n",
    "        \"\"\"\n",
    "        \n",
    "        sampling_data_filename = f\"{observatory_id}_{sampling_type}_sampling_validated.csv\"\n",
    "        measured_data_filename = f\"{observatory_id}_{sampling_type}_measured_validated.csv\"\n",
    "    \n",
    "        sampling_data = pd.read_csv(os.path.join(LOGSHEETS_PATH, sampling_data_filename))\n",
    "        measured_data = pd.read_csv(os.path.join(LOGSHEETS_PATH, measured_data_filename))\n",
    "\n",
    "        # Pull out the source_mat_ids:\n",
    "        all_sampling_source_mat_ids = sampling_data[\"source_mat_id\"].values.tolist()\n",
    "        #print(f\"source_mat_ids in {observatory_id}-{sampling_type}: {all_sampling_source_mat_ids}\")\n",
    "    \n",
    "        sampling_events = sampling_data.to_dict(orient=\"records\")\n",
    "        measured_events = measured_data.to_dict(orient=\"records\")\n",
    "    \n",
    "        combined_events = []\n",
    "        source_mat_ids_from_combined_events = []\n",
    "        source_mat_ids_in_sampling_with_refcode_missing_from_measured = []\n",
    "        no_refcode_counter = 0\n",
    "        missing_refcode_counter = 0\n",
    "        missing_measured_but_refcode_present = 0\n",
    "        refcodes_in_run_info = []\n",
    "        duplicates_ignored_counter = 0\n",
    "        for sampling_event in sampling_events:\n",
    "\n",
    "            # Checking consistency\n",
    "            # Does this sampling event have a ref_code\n",
    "            # If yes, then it should have both a sampling and measured sheet\n",
    "            # If no, we can ignore it\n",
    "            event_mat_id = sampling_event[\"source_mat_id\"]\n",
    "\n",
    "            # Two source_mat_id's in the Batch 1 & 2 run_informations match duplicate sampling events\n",
    "            # in the sampling sheets - here we ignore those two:\n",
    "            if event_mat_id in KNOWN_DUPLICATES:\n",
    "                duplicates_ignored_counter += 1\n",
    "                #print(f\"IGNORING DUP: {event_mat_id}\")\n",
    "                continue\n",
    "            \n",
    "            # Hack for HCMR-1 replicates which are \"blank1\" and \"blank2\" in the sampling sheet\n",
    "            # become just blank in the measured and run_information sheet from where we get the refcodes\n",
    "            if event_mat_id == \"EMOBON_HCMR-1_Wa_210917_3um_blank1\":\n",
    "                event_mat_id = \"EMOBON_HCMR-1_Wa_210917_3um_blank\"\n",
    "            if event_mat_id == \"EMOBON_HCMR-1_Wa_210917_0.2um_blank1\":\n",
    "                event_mat_id = \"EMOBON_HCMR-1_Wa_210917_0.2um_blank\"\n",
    "            \n",
    "            try:\n",
    "                refcode = obs_refcodes[event_mat_id]\n",
    "            except KeyError:\n",
    "                no_refcode_counter += 1\n",
    "                # OK so has not been sent to sequencing; ignore\n",
    "                continue\n",
    "            \n",
    "            event_measured = False\n",
    "            for measured_event in measured_events:\n",
    "                \n",
    "                try:\n",
    "                    measured_event[\"source_mat_id\"]\n",
    "                except KeyError:\n",
    "                    print(f\"Key error: {measured_event}\")\n",
    "                    raise KeyError\n",
    "                    # Should not happen\n",
    "                if measured_event[\"source_mat_id\"] == event_mat_id:\n",
    "                    event_measured = copy.deepcopy(measured_event)\n",
    "                    break\n",
    "                    \n",
    "            if not event_measured:\n",
    "                # sampling sheet source_mat_id has ref_code in run_information\n",
    "                # but the corresponding measured sheet lacks the same sources_mat_id\n",
    "                # This shouldn't happen unless the auto-formatting of the 'source_mat_id'\n",
    "                # field in the 'measured' sheet is broken - which is exactly what happened.\n",
    "                missing_measured_but_refcode_present += 1\n",
    "                source_mat_ids_in_sampling_with_refcode_missing_from_measured.append(event_mat_id)\n",
    "                continue\n",
    "            else:\n",
    "                sampling_event[\"ref_code\"] = refcode\n",
    "                if refcode in refcodes_in_run_info:\n",
    "                    print(f\"Error: {refcode} already in refcodes_in_run_info\")\n",
    "                    print(f\"{sampling_event}\")\n",
    "                else:\n",
    "                    refcodes_in_run_info.append(refcode)\n",
    "                # Delete the now duplicated source_mat_id\n",
    "                del event_measured[\"source_mat_id\"]\n",
    "                source_mat_ids_from_combined_events.append(event_mat_id)\n",
    "                sampling_event.update(event_measured)\n",
    "                combined_events.append(sampling_event)\n",
    "\n",
    "    \n",
    "        if verbose:\n",
    "            print(\n",
    "                  f\"Observatory {observatory_id}-{sampling_type} has {len(sampling_events)} sampling events.\\n\"\n",
    "                  f\"{no_refcode_counter} have no ref_code (i.e. they were not sent for sequencing), \\n\"\n",
    "                  f\"{missing_measured_but_refcode_present} 'sampling' events have a refcode but no \"\n",
    "                  f\"'measured' data with the corresponding source_mat_id: \\n\"\n",
    "                  #f\"{source_mat_ids_in_sampling_with_refcode_missing_from_measured} \\n\"\n",
    "                  f\"A total of {len(combined_events)} sampling events with refcode and measured sheet were found.\\n\"\n",
    "                 )\n",
    "    \n",
    "        # Did we find all the sampling events?\n",
    "        se = len(sampling_events)\n",
    "        ce = len(combined_events)\n",
    "        mmbrcp = missing_measured_but_refcode_present\n",
    "        nrc = no_refcode_counter\n",
    "        dc = duplicates_ignored_counter\n",
    "        assert se == (ce + nrc + mmbrcp + dc), \\\n",
    "            f\"Something is a foot: len(sampling_events) {se} != \" \\\n",
    "            f\"(len(combined_events) {ce} + no_refcode_counter {nrc} \" \\\n",
    "            f\"missing_measured_but_refcode_present {mmbrcp}) \" \\\n",
    "            f\"known duplicates ignored was {dc}\"\n",
    "\n",
    "        if len(combined_events) != 0 and save_table:\n",
    "            save_dir = \"./transformed\"\n",
    "            outfile_name = f\"{observatory_id}_{sampling_type}_combined_validated.csv\"\n",
    "            ndf = pd.DataFrame.from_records(combined_events, index=\"source_mat_id\")\n",
    "            ndf.to_csv(os.path.join(save_dir, outfile_name))\n",
    "\n",
    "        return source_mat_ids_from_combined_events, \\\n",
    "               all_sampling_source_mat_ids, \\\n",
    "               missing_measured_but_refcode_present, \\\n",
    "               duplicates_ignored_counter, \\\n",
    "               combined_events \n",
    "\n",
    "observatory_data = get_observatory_data()\n",
    "#pprint(observatory_data)\n",
    "obs_refcodes = get_refcodes()\n",
    "\n",
    "total_combined_events = 0\n",
    "total_missing_measured_but_refcode_present = 0\n",
    "all_source_mat_ids_from_sheets = []\n",
    "source_mat_ids_from_combined_events = []\n",
    "total_duplicates_ignored = 0\n",
    "for observatory_id, water_column, soft_sediment in observatory_data:\n",
    "\n",
    "    if observatory_id == \"Plenzia\": continue # Data not public\n",
    "    if observatory_id == \"UMF\" and soft_sediment: continue # Broken sheet\n",
    "        \n",
    "    if water_column and validators.url(water_column):\n",
    "        \n",
    "        r = parse_observatory_sample_type(observatory_id, \"water_column\", save_table = True)\n",
    "        wc_source_mat_ids_from_combined_events = r[0]\n",
    "        wc_all_sampling_source_mat_ids = r[1]\n",
    "        wc_missing_measured_but_refcode_present = r[2]\n",
    "        wc_duplicates_ignored = r[3]\n",
    "        wc_combined_events = r[4]\n",
    "        \n",
    "        total_combined_events += len(wc_combined_events)\n",
    "        total_missing_measured_but_refcode_present += wc_missing_measured_but_refcode_present\n",
    "        all_source_mat_ids_from_sheets.extend(wc_all_sampling_source_mat_ids)\n",
    "        total_duplicates_ignored += wc_duplicates_ignored\n",
    "        source_mat_ids_from_combined_events.extend(wc_source_mat_ids_from_combined_events)\n",
    "        \n",
    "    if soft_sediment and validators.url(soft_sediment):\n",
    "        r = parse_observatory_sample_type(observatory_id, \"soft_sediment\", save_table = True)\n",
    "        ss_source_mat_ids_from_combined_events= r[0]\n",
    "        ss_all_sampling_source_mat_ids = r[1]\n",
    "        ss_missing_measured_but_refcode_present = r[2]\n",
    "        ss_duplicates_ignored = r[3]\n",
    "        ss_combined_events = r[4]\n",
    "    \n",
    "        total_combined_events += len(ss_combined_events)\n",
    "        total_missing_measured_but_refcode_present += ss_missing_measured_but_refcode_present\n",
    "        all_source_mat_ids_from_sheets.extend(ss_all_sampling_source_mat_ids)\n",
    "        total_duplicates_ignored += ss_duplicates_ignored\n",
    "        source_mat_ids_from_combined_events.extend(ss_source_mat_ids_from_combined_events)\n",
    "\n",
    "print(f\"There are {len(obs_refcodes)} total ref_codes assigned\")\n",
    "# We are ignoring both of the duplicates but it's only 1 record expected missing from total_combined events\n",
    "total_dups_ignored = int(total_duplicates_ignored /2)\n",
    "print(f\"A total of {total_dups_ignored} sequencing event in the Batch 1 & 2 run_information sheets \"\n",
    "      f\"have refcodes that match source_mat_ids in the sample sheets that have duplicate entries\"\n",
    "     )\n",
    "print(f\"Total number of combined sampling events with ref_codes: {total_combined_events}\")\n",
    "duplicates = [source_mat_id for source_mat_id, count in collections.Counter(all_source_mat_ids_from_sheets).items() if count > 1]\n",
    "#if duplicates:\n",
    "    #print(f\"Found {len(duplicates)} source_mat_id duplicates\")\n",
    "    #for dup in duplicates:\n",
    "    #    pprint(dup)\n",
    "print(f\"Total number of all_source_mat_ids_from_sheets: {len(all_source_mat_ids_from_sheets)}\"\n",
    "      f\" of which {len(duplicates)} were duplicates\"\n",
    "     )\n",
    "missing_source_mat_ids =[]\n",
    "for source_mat_id in obs_refcodes: \n",
    "    # source_mat_ids are the keys in the refcode dict of the run_information\n",
    "    # print(f\"refcode from run_information: {refcode}\")\n",
    "\n",
    "    #Hack for HCMR-1\n",
    "    if source_mat_id == \"EMOBON_HCMR-1_Wa_210917_3um_blank\":\n",
    "        source_mat_id = \"EMOBON_HCMR-1_Wa_210917_3um_blank1\"\n",
    "    if source_mat_id == \"EMOBON_HCMR-1_Wa_210917_0.2um_blank\":\n",
    "        source_mat_id = \"EMOBON_HCMR-1_Wa_210917_0.2um_blank1\"\n",
    "    \n",
    "    if source_mat_id not in all_source_mat_ids_from_sheets:\n",
    "        #print(f\"source_mat_id {source_mat_id} is missing from the sampling sheets\")\n",
    "        \n",
    "        # Get close matches to missing source_mat_id\n",
    "        matches = difflib.get_close_matches(source_mat_id, all_source_mat_ids_from_sheets, n=3)\n",
    "        missing_source_mat_ids.append([source_mat_id, matches])\n",
    "        \n",
    "if missing_source_mat_ids:\n",
    "    print(\"\\n\\nThe missing source_mat_ids that are in the run information sheets are:\")\n",
    "    for missing in missing_source_mat_ids:\n",
    "        join = \" \".join(missing[1])\n",
    "        print(f\"Missing source_mat_id is {missing[0]} close matches are \\n\\t {join}\")\n",
    "    \n",
    "    for missing in missing_source_mat_ids:\n",
    "        print(missing)\n",
    "        \n",
    "print(f\"A total of {len(missing_source_mat_ids)} source_mat_ids \"\n",
    "      f\"in the batch 1 & 2 run information sheets are missing from the \"\n",
    "      f\"observatory sampling sheets\")\n",
    "\n",
    "###### CAUTION: THIS SHOULD BE ZERO! ################\n",
    "missing = False\n",
    "counter = 0\n",
    "#pprint(source_mat_ids_from_combined_events)\n",
    "for source_mat_id in source_mat_ids_from_combined_events: \n",
    "    # source_mat_ids are the keys in the refcode dict of the run_information\n",
    "    #print(f\"source_mat_id: {source_mat_id}\")\n",
    "    if source_mat_id not in obs_refcodes:\n",
    "        #print(f\"source_mat_id {source_mat_id} is missing from the run_information\")\n",
    "        counter += 1\n",
    "        missing = True\n",
    "if missing:\n",
    "    print(f\"ERROR: A total of {counter} source_mat_ids \"\n",
    "          f\"in the sampling sheets that also have refcodes are missing from the \"\n",
    "          f\"Batch 1 & 2 run information sheets\")\n",
    "\n",
    "total = total_combined_events + len(missing_source_mat_ids)\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"Total combined_events {total_combined_events} + {len(missing_source_mat_ids)} missing events \"\n",
    "    f\"= {total} and should be equal to total number of refcodes assigned in the run information \"\n",
    "    f\"sheets {len(obs_refcodes)} minus number of events ignored {total_dups_ignored} because the \"\n",
    "    f\"refcodes match duplicate source_mat_ids in the sampling sheets = \"\n",
    "    f\"{total_combined_events} + {len(missing_source_mat_ids)} = {len(obs_refcodes)} - {total_dups_ignored}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846fdf08-db1c-4e88-bdc5-7a0a298307e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
