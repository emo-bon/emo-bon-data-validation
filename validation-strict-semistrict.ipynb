{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae637b6-a712-4ec7-b2cb-4d8c11be9feb",
   "metadata": {},
   "source": [
    "### Pulls from the raw Obseravatory Google Sheets, does \"strict\" and \"semi-strict\" validation depending on constant\n",
    "(Lax is pointless as it should pass and write no errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f2f9e9-b4db-4429-b30a-faa4bbf05c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext mypy_ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "import validators\n",
    "from pathlib import Path, PurePath\n",
    "import pandas as pd\n",
    "from pydantic import ValidationError\n",
    "from pprint import pprint\n",
    "from validation_classes import (\n",
    "    samplingModel,\n",
    "    measuredModel,\n",
    "    samplingModelStrict,\n",
    "    samplingModelSemiStrict,\n",
    ")\n",
    "from typing import Any, Union\n",
    "\n",
    "############################ CAUTION ##################################################\n",
    "STRICT: bool = False  # As defined by Ioulia, dates corrected, NA's removed etc\n",
    "SEMI_STRICT: bool = (\n",
    "    True  # As defined by Ioulia but not checking for mandatory fields\n",
    ")\n",
    "# ints and str coerced to floats when possible\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "def parse_sample_sheets(\n",
    "    sampling_strategy: str,\n",
    "    sheet_type: str,\n",
    "    addresses: list[tuple[str, str]],\n",
    ") -> None:\n",
    "\n",
    "    for observatory in addresses:\n",
    "        observatory_id, sheet_link = observatory\n",
    "        # print(f\"Observatory_id {observatory_id} sheet_link {sheet_link}\")\n",
    "        if not isinstance(sheet_link, str):\n",
    "            # print(f\"This is the sheet_link type {type(sheet_link)}\")\n",
    "            if isinstance(sheet_link, float):\n",
    "                if math.isnan(sheet_link):\n",
    "                    print(\n",
    "                        f\"Observatory {observatory_id} does not do {sampling_strategy}\"\n",
    "                    )\n",
    "                    continue\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Unknown URL value {sheet_link} to observatory {observatory_id}\"\n",
    "                )\n",
    "        else:\n",
    "            if not validators.url(sheet_link):\n",
    "                raise ValueError(f\"URL {sheet_link=} is not valid\")\n",
    "\n",
    "            if observatory_id == \"Plenzia\":\n",
    "                continue  # Sheets not publically available\n",
    "            # UMF soft_sed has two source_mat_ids\n",
    "            if sampling_strategy == \"soft_sediment\" and observatory_id == \"UMF\":\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing {observatory_id}...\")\n",
    "            sampling_sheet_base: str = sheet_link.split(\"/edit\")[0]\n",
    "            sampling_sheet_suffix: str = \"/gviz/tq?tqx=out:csv&sheet=%s\"\n",
    "            sample_sheet_link: str = (\n",
    "                sampling_sheet_base + sampling_sheet_suffix % sheet_type\n",
    "            )\n",
    "            print(f\"Sample sheet link: {sample_sheet_link}\")\n",
    "            df: pd.core.frame.DataFrame = pd.read_csv(\n",
    "                sample_sheet_link, encoding=\"utf-8\"\n",
    "            )\n",
    "            data_records_all: dict[str, str] = df.to_dict(orient=\"records\")\n",
    "\n",
    "            # Many sheets have partially filled rows\n",
    "            # The source_mat_id is manually curated and the PRIMARY_KEY\n",
    "            # Therefore filter records on source_mat_id\n",
    "            def filter_on_source_mat_id(d):\n",
    "                # Bergen has it as source_material_id\n",
    "                try:\n",
    "                    value: Union[str, float, None] = d[\"source_mat_id\"]\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        value: Union[str, float, None] = d[\"source_material_id\"]\n",
    "                    except KeyError:\n",
    "                        raise ValueError(\"Cannot find source_mat_id field\")\n",
    "                if isinstance(value, float):\n",
    "                    if math.isnan(value):\n",
    "                        return False\n",
    "                elif value is None:\n",
    "                    return False\n",
    "                # Remove mis-formatted\n",
    "                elif len(value.split(\"_\")) < 6:\n",
    "                    return False\n",
    "                # Edge case of this otherwise blank entry having 6 \"bits\"\n",
    "                elif value == \"EMOBON_VB_Wa_230509_um_\":\n",
    "                    return False\n",
    "                else:\n",
    "                    return True\n",
    "\n",
    "            data_records_filtered: list[dict[str, str]] = list(\n",
    "                filter(filter_on_source_mat_id, data_records_all)\n",
    "            )\n",
    "\n",
    "            if len(data_records_all) > len(data_records_filtered):\n",
    "                print(\n",
    "                    f\"Discarded {len(data_records_all) - len(data_records_filtered)} records leaving {len(data_records_filtered)}.\"\n",
    "                )\n",
    "\n",
    "            if STRICT:\n",
    "                model_type = f\"{sheet_type}_strict\"\n",
    "            elif SEMI_STRICT:\n",
    "                model_type = f\"{sheet_type}_semistrict\"\n",
    "            else:\n",
    "                model_type = sheet_type\n",
    "\n",
    "            validator = validator_classes[model_type]\n",
    "            validated_rows = []\n",
    "            errors = []  # type is way too complicated to include :)\n",
    "            for row in data_records_filtered:\n",
    "                try:\n",
    "                    vr = validator(**row)\n",
    "                except ValidationError as e:\n",
    "                    if observatory_id == \"Bergen\":\n",
    "                        errors.append([(row[\"source_material_id\"], e.errors())])\n",
    "                    else:\n",
    "                        errors.append([(row[\"source_mat_id\"], e.errors())])\n",
    "                else:\n",
    "                    validated_rows.append(vr.model_dump())\n",
    "\n",
    "            if errors:\n",
    "                # errors is a list of lists where each inner list is a dict of row errors\n",
    "                # where each isof key = source_mat_id and values is list of dicts each of which\n",
    "                # is an error:\n",
    "                total_number_errors: int = sum(\n",
    "                    [len(row[1]) for e in errors for row in e]\n",
    "                )\n",
    "                print(f\"Errors were found... {total_number_errors} in total\")\n",
    "                save_dir_errors: Path = Path(\"./validation_errors\")\n",
    "                # outfile_name_pk: str = f\"{observatory_id}_{sampling_strategy}_{model_type}_ERRORS.pickle\"\n",
    "                # out_path_pk: Path = os.path.join(save_dir, outfile_name_pk)\n",
    "                # with open(out_path_pk, \"wb\") as f:\n",
    "                #    pickle.dump(errors, f, pickle.HIGHEST_PROTOCOL)\n",
    "                outfile_name_log: Path = Path(\n",
    "                    f\"{observatory_id}_{sampling_strategy}_{model_type}_ERRORS.log\"\n",
    "                )\n",
    "                out_path_log: PurePath = PurePath(\n",
    "                    save_dir_errors, outfile_name_log\n",
    "                )\n",
    "                # os.path.join(save_dir, outfile_name_log)\n",
    "                with open(out_path_log, \"w\") as f:\n",
    "                    pprint(errors, f)\n",
    "            else:\n",
    "                assert len(validated_rows) == len(\n",
    "                    data_records_filtered\n",
    "                ), \"Not sure what happenned, but len(validated_rows) != len(data_filtered_records)\"\n",
    "                print(\"All records passed!\")\n",
    "\n",
    "                # for record in validated_rows:\n",
    "                #    for field in record:\n",
    "                #        print(f\"Record {field} has value {record[field]} is type {type(record[field])}\")\n",
    "\n",
    "                if not STRICT and not SEMI_STRICT:\n",
    "                    save_dir_logsheets: Path = Path(\"./logsheets\")\n",
    "                    outfile_name: Path = Path(\n",
    "                        f\"{observatory_id}_{sampling_strategy}_{model_type}_validated.csv\"\n",
    "                    )\n",
    "                    ndf = pd.DataFrame.from_records(\n",
    "                        validated_rows, index=\"source_mat_id\"\n",
    "                    )\n",
    "                    ndf.to_csv(PurePath(save_dir_logsheets, outfile_name))\n",
    "                    print(\n",
    "                        f\"Written {os.path.join(save_dir_logsheets, outfile_name)}\"\n",
    "                    )\n",
    "\n",
    "\n",
    "validator_classes = {\n",
    "    \"sampling\": samplingModel,\n",
    "    \"measured\": measuredModel,\n",
    "    \"sampling_strict\": samplingModelStrict,\n",
    "    \"sampling_semistrict\": samplingModelSemiStrict,\n",
    "}\n",
    "\n",
    "# Get list of all URL links to sampling sheets\n",
    "# NB  you cant use a \"with\" closure here when reading the Pandas df\n",
    "governance_logsheets_validated_csv = \"./governance/logsheets_validated.csv\"\n",
    "df: pd.core.frame.DataFrame = pd.read_csv(governance_logsheets_validated_csv)\n",
    "water_column_sheet_addresses: list[tuple[str, str]] = df[\n",
    "    [\"observatory_id\", \"water_column\"]\n",
    "].values.tolist()\n",
    "soft_sediment_sheet_addresses: list[tuple[str, str]] = df[\n",
    "    [\"observatory_id\", \"soft_sediment\"]\n",
    "].values.tolist()\n",
    "del df\n",
    "\n",
    "parse_sample_sheets(\"water_column\", \"sampling\", water_column_sheet_addresses)\n",
    "parse_sample_sheets(\"soft_sediment\", \"sampling\", soft_sediment_sheet_addresses)\n",
    "\n",
    "# There are no strict or semi-strict sheets for \"measured\"\n",
    "# parse_sample_sheets(\"water_column\", \"measured\", water_column_sheet_addresses)\n",
    "# parse_sample_sheets(\"soft_sediment\", \"measured\", soft_sediment_sheet_addresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e07bc13-7b0b-4b94-84e3-3f2874f551ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext mypy_ipython\n",
    "%mypy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeab05ee-ca50-4ead-b2bf-e4d7fbc597d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
