{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c01fa68b-1e71-4a5d-a68a-77b4d6e810f7",
   "metadata": {},
   "source": [
    "# Pydantic validation framework for the EMO BON observatory and other metadata log sheets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bb619d-5b92-4bee-b1a5-b3f0cb364f70",
   "metadata": {},
   "source": [
    "- **pydantic** Data validation using Python type hints.\n",
    "    - [pypi](https://pypi.org/project/pydantic/)\n",
    "    - [Documentation](https://docs.pydantic.dev/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5bca054-8458-4137-ae94-b960bbe0ad45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD is /srv/scratch/emo-bon-data-validation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Weird stuff from JupyterHub after I moved modules and notebooks around:\n",
    "# For some reasong CWD is /src/scratch even though this notebook is in /srv/scratch/emo-bon-validation\n",
    "# The terminal also show us to be in /srv/scratch/emo-bon-validation\n",
    "# So...\n",
    "if os.getcwd() == \"/srv/scratch\":\n",
    "    os.chdir(\"./emo-bon-data-validation\")\n",
    "print(f\"CWD is {os.getcwd()}\")\n",
    "\n",
    "import importlib\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f0a524-4ac9-4840-ba77-b031296619fa",
   "metadata": {},
   "source": [
    "##### Init the directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d4afacf-09d3-47dc-80cc-e00d3e9232c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    #Init dirs and paths, write csv files\n",
    "    # Init the validation classes dir if needed\n",
    "    # Note that __init__.py will need be edited manually to import the validators\n",
    "    # e.g from .observatories import Model as observatoriesModel\n",
    "    validation_classes_path = \"./validation_classes\"\n",
    "    if True:\n",
    "        if not os.path.exists(validation_classes_path):\n",
    "            os.mkdir(validation_classes_path)\n",
    "            Path(os.path.join(validation_classes_path, \"__init__.py\")).touch()\n",
    "            os.mkdir(raw_files_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e601478-27ea-404a-8371-1405051ccf22",
   "metadata": {},
   "source": [
    "## Governance data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e34558-fb7c-4000-8e8b-24ad71df7eab",
   "metadata": {},
   "source": [
    "#### Read each of the governance CSV files into a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "154ba717-c9a9-43a0-b34e-8b7461952135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18 entries, 0 to 17\n",
      "Data columns (total 9 columns):\n",
      " #   Column                               Non-Null Count  Dtype \n",
      "---  ------                               --------------  ----- \n",
      " 0   EMBRC Node                           18 non-null     object\n",
      " 1   EMBRC Site                           18 non-null     object\n",
      " 2   EMOBON_observatory_id                18 non-null     object\n",
      " 3   Water Column                         17 non-null     object\n",
      " 4   Soft sediment                        7 non-null      object\n",
      " 5   data_quality_control_threshold_date  18 non-null     object\n",
      " 6   data_quality_control_assignee        18 non-null     object\n",
      " 7   rocrate_profile_uri                  18 non-null     object\n",
      " 8   autogenerate                         18 non-null     int64 \n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 1.4+ KB\n",
      "This is info() for None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19 entries, 0 to 18\n",
      "Data columns (total 22 columns):\n",
      " #   Column                            Non-Null Count  Dtype  \n",
      "---  ------                            --------------  -----  \n",
      " 0   country_code                      19 non-null     object \n",
      " 1   country                           19 non-null     object \n",
      " 2   EMOBON_observatory_name           19 non-null     object \n",
      " 3   EMOBON_observatory_id             19 non-null     object \n",
      " 4   startdate                         19 non-null     object \n",
      " 5   enddate                           2 non-null      object \n",
      " 6   Water_Column                      19 non-null     object \n",
      " 7   Soft_Substrates                   19 non-null     object \n",
      " 8   Hard_Substrates                   19 non-null     object \n",
      " 9   water_site_latitude               18 non-null     float64\n",
      " 10  water_site_longitude              18 non-null     float64\n",
      " 11  sediment_site_latitude            9 non-null      float64\n",
      " 12  sediment_site_longtitude          9 non-null      float64\n",
      " 13  hard_substrates_site1_longitude   9 non-null      object \n",
      " 14  hard_substrates_site1_latitude    9 non-null      object \n",
      " 15  hard_substrates_site2_longtitude  3 non-null      float64\n",
      " 16  hard_substrates_site2_latitude    3 non-null      float64\n",
      " 17  contect person                    19 non-null     object \n",
      " 18  contact person email              19 non-null     object \n",
      " 19  ENA_accession_number_umbrella     19 non-null     object \n",
      " 20  ENA_accession_number_project      19 non-null     object \n",
      " 21  EMOBON_core                       19 non-null     object \n",
      "dtypes: float64(6), object(16)\n",
      "memory usage: 3.4+ KB\n",
      "This is info() for None\n"
     ]
    }
   ],
   "source": [
    "github_path = \"https://raw.githubusercontent.com/emo-bon/governance-data/main/\"\n",
    "file_names = [\n",
    "        \"logsheets.csv\",              # contain the URLs of the googlesheets that are the logsheets\n",
    "        \"observatories.csv\"           # contain information about each observatory\n",
    "        #\"organisations.csv\",         # contain information about the organisations in EMO BON\n",
    "        #\"planned_events.csv\"         # contains information about planned EMO BON events (this file is only used by humans, not by any actions) - DONT CARE\n",
    "        #\"ro-crate-metadata.json\"     # IGNORE\n",
    "        ]\n",
    "dfs = {}\n",
    "for f in file_names:\n",
    "    df = pd.read_csv(os.path.join(github_path, f))\n",
    "    print(f\"This is info() for {df.info()}\")\n",
    "    dfs[f] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2484615f-cab2-48a2-8ca7-999bab0051e3",
   "metadata": {},
   "source": [
    "#### Validate Governance tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8dc50e0-2d84-4508-8d82-67b90397c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from validation_classes import observatoriesModel, logsheetsModel\n",
    "validator_class_paths = {\"logsheets.csv\": logsheetsModel, \"observatories.csv\": observatoriesModel}\n",
    "validation_classes_path = \"./validation_classes\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa819fa6-c492-481f-bc81-218057699342",
   "metadata": {},
   "source": [
    "##### Observatories table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af3dfa6-8d4c-4749-b9d1-bc00834e1a3b",
   "metadata": {},
   "source": [
    "The observatories validator mostly changes the column names to make them consistent (and spelled correctly), removes blank strings (\"   \") from cells, and reformats the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9aa2840b-9b25-4155-bd43-34cea9c81de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"observatories.csv\"\n",
    "data = dfs[file_name] # dfs is dict of pandas df's\n",
    "validator = validator_class_paths[file_name]\n",
    "data_records = data.to_dict(orient=\"records\")\n",
    "validated_rows = [validator(**row).model_dump() for row in data_records]\n",
    "\n",
    "#for row in data_records:\n",
    "#    #print(row)\n",
    "#    validator(**row)\n",
    "\n",
    "#for record in validated_rows:\n",
    "#    for field in record:\n",
    "#        print(f\"Record {field} has value {record[field]} is type {type(record[field])}\")\n",
    "\n",
    "ndf = pd.DataFrame.from_records(validated_rows, index=\"observatory_id\")\n",
    "ndf.to_csv(os.path.join(\"governance\", \"observatories_validated.csv\"))\n",
    "\n",
    "# Take a look at ./validation_classes/validated_governance/observatories_validated.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ef5ade-e3f4-4742-b1ba-41233e93e80c",
   "metadata": {},
   "source": [
    "##### Logsheets table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9e1b9e-8a05-4bf2-aceb-55c5c740e77d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_name = \"logsheets.csv\"\n",
    "data = dfs[file_name] # dfs is dict of pandas df's\n",
    "validator = validator_class_paths[file_name]\n",
    "data_records = data.to_dict(orient=\"records\")\n",
    "validated_rows = [validator(**row).model_dump() for row in data_records]\n",
    "        \n",
    "ndf = pd.DataFrame.from_records(validated_rows, index=\"observatory_id\")\n",
    "ndf.to_csv(os.path.join(validation_classes_path, \"governance\", \"logsheets_validated.csv\"))\n",
    "\n",
    "# Take a look at ./validation_classes/validated_governance/logsheets_validated.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fc138c-c96f-4349-843c-ad02276b5f43",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Logsheets from water column and soft sediments sampling and \"measured\" tables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95e6614-c9ff-470d-8363-8f37a08d34ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "    \n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pandas as pd\n",
    "import pydantic\n",
    "from validation_classes import samplingModel, measuredModel\n",
    "\n",
    "def parse_sample_sheets(sampling_strategy: str, sheet_type: str, addresses: pd.core.frame.DataFrame) -> None: \n",
    "    for observatory in addresses:\n",
    "        observatory_id, sheet_link = observatory\n",
    "        #print(f\"Observatory_id {observatory_id} sheet_link {sheet_link}\")\n",
    "        if not isinstance(sheet_link, str):\n",
    "            #print(f\"This is the sheet_link type {type(sheet_link)}\")\n",
    "            if isinstance(sheet_link, float): \n",
    "                if math.isnan(sheet_link):\n",
    "                    print(f\"Observatory {observatory_id} lacks valid sheet URL {sheet_link}\")\n",
    "                    continue\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown link {sheet_link} to observatory {observatory_id}\")\n",
    "        else:\n",
    "\n",
    "            if observatory_id == \"Plenzia\": continue # Sheets not publically available\n",
    "            # UMF soft_sed has two source_mat_ids\n",
    "            if sampling_strategy == \"soft_sediment\" and observatory_id == \"UMF\":\n",
    "                continue\n",
    "            \n",
    "            #if observatory_id in [\"BPNS\", \"Bergen\", \"ESC68N\", \"MBAL4\",\n",
    "            #                     \"VB\", \"ROSKOGO\"]:\n",
    "            #[\"Bergen\", \"MBAL4\", \"ESC68N\", \n",
    "            #               \"BPNS\", \"VB\", \"ROSKOGO\",\n",
    "            #               \"EMT21\", \"PiEGetxo\", \"RFormosa\",\n",
    "            #              \"OSD74\", \"AAOT\", \"NRMCB\",\n",
    "            #              \"HCMR-1\", \"IUIEilat\", \"UMF\"]:\n",
    "            #    continue\n",
    "    \n",
    "            #if observatory_id != \"BPNS\":\n",
    "            #    continue\n",
    "    \n",
    "            print(f\"Processing {observatory_id}...\")\n",
    "            sampling_sheet_base = sheet_link.split(\"/edit\")[0]\n",
    "            sampling_sheet_suffix = \"/gviz/tq?tqx=out:csv&sheet=%s\"\n",
    "            sample_sheet_link = sampling_sheet_base + sampling_sheet_suffix % sheet_type\n",
    "            print(f\"Sample sheet link: {sample_sheet_link}\")\n",
    "            df = pd.read_csv(sample_sheet_link, encoding='utf-8')\n",
    "            data_records_all = df.to_dict(orient=\"records\")\n",
    "    \n",
    "            # Many sheets have partially filled rows\n",
    "            # The source_mat_id is manually curated and the PRIMARY_KEY\n",
    "            # Therefore filter records on source_mat_id\n",
    "            def filter_on_source_mat_id(d):\n",
    "                # Bergen has it as source_material_id\n",
    "                try:\n",
    "                    value = d[\"source_mat_id\"]\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        value = d[\"source_material_id\"]\n",
    "                    except KeyError:\n",
    "                        raise ValueError(\"Cannot find source_mat_id field\")\n",
    "                if isinstance(value, float):\n",
    "                    if math.isnan(value):\n",
    "                        return False\n",
    "                elif value is None:\n",
    "                    return False\n",
    "                # Remove mis-formatted\n",
    "                elif len(value.split(\"_\")) < 6:\n",
    "                    return False\n",
    "                #Edge case of this otherwise blank entry having 6 \"bits\"\n",
    "                elif value == \"EMOBON_VB_Wa_230509_um_\":\n",
    "                    return False \n",
    "                else:\n",
    "                    return True\n",
    "                \n",
    "            data_records_filtered = list(filter(filter_on_source_mat_id, data_records_all))\n",
    "    \n",
    "            if len(data_records_all) > len(data_records_filtered):\n",
    "                print(f\"Discarded {len(data_records_all) - len(data_records_filtered)} records leaving {len(data_records_filtered)}.\")\n",
    "            \n",
    "            validator = validator_classes[sheet_type]\n",
    "            validated_rows = [validator(**row).model_dump() for row in data_records_filtered]\n",
    "    \n",
    "            #for record in validated_rows:\n",
    "            #    for field in record:\n",
    "            #        print(f\"Record {field} has value {record[field]} is type {type(record[field])}\")\n",
    "    \n",
    "            save_dir = \"./logsheets\"\n",
    "            outfile_name = f\"{observatory_id}_{sampling_strategy}_{sheet_type}_validated.csv\"\n",
    "            ndf = pd.DataFrame.from_records(validated_rows, index=\"source_mat_id\")\n",
    "            ndf.to_csv(os.path.join(save_dir, outfile_name))\n",
    "            print(f\"Written {os.path.join(save_dir, outfile_name)}\")\n",
    "\n",
    "\n",
    "validator_classes = {\"sampling\": samplingModel, \"measured\": measuredModel}\n",
    "# Get list of all URL links to sampling sheets\n",
    "# NB  you cant use a \"with\" closure here when reading the Pandas df\n",
    "governance_logsheets_validated_csv = \"./governance/logsheets_validated.csv\"\n",
    "df = pd.read_csv(governance_logsheets_validated_csv)\n",
    "water_column_sheet_addresses = df[[\"observatory_id\", \"water_column\"]].values.tolist()\n",
    "soft_sediment_sheet_addresses  = df[[\"observatory_id\", \"soft_sediment\"]].values.tolist()\n",
    "del df\n",
    "    \n",
    "parse_sample_sheets(\"water_column\", \"sampling\", water_column_sheet_addresses)\n",
    "parse_sample_sheets(\"soft_sediment\", \"sampling\", soft_sediment_sheet_addresses)\n",
    "parse_sample_sheets(\"water_column\", \"measured\", water_column_sheet_addresses)\n",
    "parse_sample_sheets(\"soft_sediment\", \"measured\", soft_sediment_sheet_addresses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2e4fbb-ff53-4058-8fc9-90dfef6423b4",
   "metadata": {},
   "source": [
    "# Refactoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ec0e469-3c57-4f9c-a41d-9a343a235bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Processing ESC68N...\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/11_Eu0W1-sDiuzKx1cIl6YuxjRHmWezN6u9v3Ly8JZ3A/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Discarded 114 records leaving 150.\n",
      "Errors were found... 2319 in total\n",
      "Processing Bergen...\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1HuXHiUJICZrmCrJ4EZDyU5aSCMzDAc1cy_tne5YVPTg/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Discarded 32 records leaving 108.\n",
      "Errors were found... 1672 in total\n",
      "Processing MBAL4...\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1xfrqraPa0auQ1O-C9RUo68RhxrPCDWkVMCAUbj79AZI/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Discarded 2 records leaving 78.\n",
      "Errors were found... 935 in total\n",
      "Processing BPNS...\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1mEi4Bd2YR63WD0j54FQ6QkzcUw_As9Wilue9kaXO2DE/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Discarded 40 records leaving 280.\n",
      "Errors were found... 4635 in total\n",
      "Processing ROSKOGO...\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1BCu2pbuIS4f-yrw5Kw4uzfO9v6ryZJBtSF5WBLWG_-E/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Discarded 57 records leaving 234.\n",
      "Errors were found... 3560 in total\n",
      "Processing VB...\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1kSlTcNfXaCQv7fIKVbBnt5PmRW3_H7Trryh4FJHyLkE/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Discarded 88 records leaving 764.\n",
      "Errors were found... 12011 in total\n",
      "Observatory OOB lacks valid sheet URL nan\n",
      "Processing EMT21...\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1RutVpduwL1mEed5jC2zLbs0DgkzAwZdsJ27fg3kb0Yg/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Discarded 62 records leaving 188.\n",
      "Errors were found... 2657 in total\n",
      "Processing PiEGetxo...\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1KGcishLP5eAn6ZCnixn5wZIez1Y9MgmYoch2s7aefKM/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Errors were found... 3399 in total\n",
      "Processing RFormosa...\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/15f_yqrhvOr1MW-SHuv0JiKf0HRDy81uGZapJANxL54k/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Errors were found... 2439 in total\n",
      "Processing OSD74...\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1VATpzmhwmk8sYkhEjCbDUZHRiYuNGpSRefhXthZRbfQ/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Discarded 28 records leaving 170.\n",
      "Errors were found... 2642 in total\n",
      "Processing AAOT...\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1hvLkBwiKTGTJDx19m_8e7qJ2lm9bwLLeVztMpxTLqnk/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Discarded 268 records leaving 320.\n",
      "Errors were found... 4412 in total\n",
      "Processing NRMCB...\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1F4AWv_seI-DQJ_Gp_N2ziwvGyjnc4KC92GGNXrJRek4/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Errors were found... 5965 in total\n",
      "Processing HCMR-1...\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/13DcVK2mzSxMJoFydSBaIMmj7Td1_JapEvcY2bmZTyLc/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Errors were found... 1917 in total\n",
      "Processing IUIEilat...\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1qIwi1nZu4OBiriCzn1fEG1fyG8-3wSVvwzsMiH6JTwk/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Errors were found... 690 in total\n",
      "Processing UMF...\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1-1VsUUbRtKselxu-y2BghRIrJdaf74SbmvK42ntvd20/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Discarded 86 records leaving 154.\n",
      "Errors were found... 2168 in total\n",
      "Processing LMO...\n",
      "Sample sheet link: https://docs.google.com/spreadsheets/d/1AvQMYcS0tdNMw6Er8zUarQg1a_wrshhnkTS6RuI1FJQ/gviz/tq?tqx=out:csv&sheet=sampling\n",
      "Discarded 138 records leaving 100.\n",
      "Errors were found... 1666 in total\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "    \n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pydantic import ValidationError\n",
    "from pprint import pprint\n",
    "from validation_classes import samplingModel, measuredModel, samplingModelStrict\n",
    "\n",
    "############################ CAUTION #############################################\n",
    "STRICT      = True # As defined by Ioulia\n",
    "SEMI_STRICT = True # As defined by Ioulia but not checking for Mandatory fields\n",
    "##################################################################################\n",
    "\n",
    "def parse_sample_sheets(sampling_strategy: str,\n",
    "                        sheet_type: str,\n",
    "                        addresses: pd.core.frame.DataFrame,\n",
    "                       ) -> None:\n",
    "\n",
    "    ################# temp ###################\n",
    "    #addresses = [addresses[0]]\n",
    "    #print(addresses)\n",
    "    ################ temp ######################\n",
    "    \n",
    "    \n",
    "    for observatory in addresses:\n",
    "        observatory_id, sheet_link = observatory\n",
    "        #print(f\"Observatory_id {observatory_id} sheet_link {sheet_link}\")\n",
    "        if not isinstance(sheet_link, str):\n",
    "            #print(f\"This is the sheet_link type {type(sheet_link)}\")\n",
    "            if isinstance(sheet_link, float): \n",
    "                if math.isnan(sheet_link):\n",
    "                    print(f\"Observatory {observatory_id} lacks valid sheet URL {sheet_link}\")\n",
    "                    continue\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown link {sheet_link} to observatory {observatory_id}\")\n",
    "        else:\n",
    "\n",
    "            if observatory_id == \"Plenzia\": continue # Sheets not publically available\n",
    "            # UMF soft_sed has two source_mat_ids\n",
    "            if sampling_strategy == \"soft_sediment\" and observatory_id == \"UMF\":\n",
    "                continue\n",
    "            \n",
    "            #if observatory_id in [\"BPNS\", \"Bergen\", \"ESC68N\", \"MBAL4\",\n",
    "            #                     \"VB\", \"ROSKOGO\"]:\n",
    "            #[\"Bergen\", \"MBAL4\", \"ESC68N\", \n",
    "            #               \"BPNS\", \"VB\", \"ROSKOGO\",\n",
    "            #               \"EMT21\", \"PiEGetxo\", \"RFormosa\",\n",
    "            #              \"OSD74\", \"AAOT\", \"NRMCB\",\n",
    "            #              \"HCMR-1\", \"IUIEilat\", \"UMF\"]:\n",
    "            #    continue\n",
    "    \n",
    "            #if observatory_id != \"BPNS\":\n",
    "            #    continue\n",
    "    \n",
    "            print(f\"Processing {observatory_id}...\")\n",
    "            sampling_sheet_base = sheet_link.split(\"/edit\")[0]\n",
    "            sampling_sheet_suffix = \"/gviz/tq?tqx=out:csv&sheet=%s\"\n",
    "            sample_sheet_link = sampling_sheet_base + sampling_sheet_suffix % sheet_type\n",
    "            print(f\"Sample sheet link: {sample_sheet_link}\")\n",
    "            df = pd.read_csv(sample_sheet_link, encoding='utf-8')\n",
    "            data_records_all = df.to_dict(orient=\"records\")\n",
    "    \n",
    "            # Many sheets have partially filled rows\n",
    "            # The source_mat_id is manually curated and the PRIMARY_KEY\n",
    "            # Therefore filter records on source_mat_id\n",
    "            def filter_on_source_mat_id(d):\n",
    "                # Bergen has it as source_material_id\n",
    "                try:\n",
    "                    value = d[\"source_mat_id\"]\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        value = d[\"source_material_id\"]\n",
    "                    except KeyError:\n",
    "                        raise ValueError(\"Cannot find source_mat_id field\")\n",
    "                if isinstance(value, float):\n",
    "                    if math.isnan(value):\n",
    "                        return False\n",
    "                elif value is None:\n",
    "                    return False\n",
    "                # Remove mis-formatted\n",
    "                elif len(value.split(\"_\")) < 6:\n",
    "                    return False\n",
    "                #Edge case of this otherwise blank entry having 6 \"bits\"\n",
    "                elif value == \"EMOBON_VB_Wa_230509_um_\":\n",
    "                    return False \n",
    "                else:\n",
    "                    return True\n",
    "                \n",
    "            data_records_filtered = list(filter(filter_on_source_mat_id, data_records_all))\n",
    "    \n",
    "            if len(data_records_all) > len(data_records_filtered):\n",
    "                print(f\"Discarded {len(data_records_all) - len(data_records_filtered)} records leaving {len(data_records_filtered)}.\")\n",
    "\n",
    "            if STRICT:\n",
    "                model_type = f\"{sheet_type}_strict\"\n",
    "            else:\n",
    "                model_type = sheet_type\n",
    "\n",
    "            validator = validator_classes[model_type]\n",
    "\n",
    "            #validated_rows = [validator(**row).model_dump() for row in data_records_filtered]\n",
    "            validated_rows = []\n",
    "            errors: List[List[str:List[Dict]]] = [] # where each error is the inner Dict\n",
    "            for row in data_records_filtered:\n",
    "                try:\n",
    "                    vr = validator(**row)\n",
    "                except ValidationError as e:\n",
    "                    if observatory_id == \"Bergen\":\n",
    "                        errors.append([(row[\"source_material_id\"], e.errors())])\n",
    "                    else:\n",
    "                        errors.append([(row[\"source_mat_id\"], e.errors())])\n",
    "                else:\n",
    "                    validated_rows.append(vr)\n",
    "\n",
    "            if errors:\n",
    "                # errors is a list of lists where each inner list is a dict of row errors\n",
    "                # where each isof key = source_mat_id and values is list of dicts each of which\n",
    "                # is an error:\n",
    "                #List[List[str:List[Dict]]]\n",
    "                total_number_errors = sum([len(row[1]) for e in errors for row in e])\n",
    "                print(f\"Errors were found... {total_number_errors} in total\")\n",
    "                save_dir = \"./validation_errors\"\n",
    "                outfile_name_pk = f\"{observatory_id}_{sampling_strategy}_{model_type}_ERRORS.pickle\"\n",
    "                out_path_pk = os.path.join(save_dir, outfile_name_pk)\n",
    "                with open(out_path_pk, \"wb\") as f:\n",
    "                    pickle.dump(errors, f, pickle.HIGHEST_PROTOCOL)\n",
    "                outfile_name_log = f\"{observatory_id}_{sampling_strategy}_{model_type}_ERRORS.log\"\n",
    "                out_path_log = os.path.join(save_dir, outfile_name_log)                \n",
    "                with open(out_path_log, \"w\") as f:\n",
    "                    pprint(errors, f)\n",
    "\n",
    "            else:\n",
    "                assert len(validated_rows) == len(data_records_filtered), \"Not sure what happenned, but len(validated_rows) != len(data_filtered_records)\"\n",
    "            \n",
    "                #for record in validated_rows:\n",
    "                #    for field in record:\n",
    "                #        print(f\"Record {field} has value {record[field]} is type {type(record[field])}\")\n",
    "    \n",
    "                save_dir = \"./logsheets\"\n",
    "                outfile_name = f\"{observatory_id}_{sampling_strategy}_{model_type}_validated.csv\"\n",
    "                ndf = pd.DataFrame.from_records(validated_rows, index=\"source_mat_id\")\n",
    "                ndf.to_csv(os.path.join(save_dir, outfile_name))\n",
    "                print(f\"Written {os.path.join(save_dir, outfile_name)}\")\n",
    "\n",
    "\n",
    "validator_classes = {\"sampling\": samplingModel, \"measured\": measuredModel, \"sampling_strict\": samplingModelStrict}\n",
    "\n",
    "# Get list of all URL links to sampling sheets\n",
    "# NB  you cant use a \"with\" closure here when reading the Pandas df\n",
    "governance_logsheets_validated_csv = \"./governance/logsheets_validated.csv\"\n",
    "df = pd.read_csv(governance_logsheets_validated_csv)\n",
    "water_column_sheet_addresses = df[[\"observatory_id\", \"water_column\"]].values.tolist()\n",
    "soft_sediment_sheet_addresses  = df[[\"observatory_id\", \"soft_sediment\"]].values.tolist()\n",
    "del df\n",
    "\n",
    "parse_sample_sheets(\"water_column\", \"sampling\", water_column_sheet_addresses)\n",
    "\n",
    "#parse_sample_sheets(\"water_column\", \"sampling\", water_column_sheet_addresses)\n",
    "#parse_sample_sheets(\"soft_sediment\", \"sampling\", soft_sediment_sheet_addresses)\n",
    "#parse_sample_sheets(\"water_column\", \"measured\", water_column_sheet_addresses)\n",
    "#parse_sample_sheets(\"soft_sediment\", \"measured\", soft_sediment_sheet_addresses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e277b03b-8374-4ca9-9aee-d1ffae707bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33792f86-8da9-4cd5-9986-29a80aedfbfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
