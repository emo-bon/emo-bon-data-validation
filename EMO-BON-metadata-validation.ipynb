{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c01fa68b-1e71-4a5d-a68a-77b4d6e810f7",
   "metadata": {},
   "source": [
    "# Pydantic validation framework for the EMO BON observatory and other metadata log sheets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bb619d-5b92-4bee-b1a5-b3f0cb364f70",
   "metadata": {},
   "source": [
    "- **pydantic** Data validation using Python type hints.\n",
    "    - [pypi](https://pypi.org/project/pydantic/)\n",
    "    - [Documentation](https://docs.pydantic.dev/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5bca054-8458-4137-ae94-b960bbe0ad45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD is /srv/scratch/emo-bon-data-validation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Weird stuff from JupyterHub after I moved modules and notebooks around:\n",
    "# For some reasong CWD is /src/scratch even though this notebook is in /srv/scratch/emo-bon-validation\n",
    "# The terminal also show us to be in /srv/scratch/emo-bon-validation\n",
    "# So...\n",
    "if os.getcwd() == \"/srv/scratch\":\n",
    "    os.chdir(\"./emo-bon-data-validation\")\n",
    "print(f\"CWD is {os.getcwd()}\")\n",
    "\n",
    "import importlib\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f0a524-4ac9-4840-ba77-b031296619fa",
   "metadata": {},
   "source": [
    "##### Init the directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d4afacf-09d3-47dc-80cc-e00d3e9232c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    #Init dirs and paths, write csv files\n",
    "    # Init the validation classes dir if needed\n",
    "    # Note that __init__.py will need be edited manually to import the validators\n",
    "    # e.g from .observatories import Model as observatoriesModel\n",
    "    validation_classes_path = \"./validation_classes\"\n",
    "    if True:\n",
    "        if not os.path.exists(validation_classes_path):\n",
    "            os.mkdir(validation_classes_path)\n",
    "            Path(os.path.join(validation_classes_path, \"__init__.py\")).touch()\n",
    "            os.mkdir(raw_files_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e601478-27ea-404a-8371-1405051ccf22",
   "metadata": {},
   "source": [
    "## Governance data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e34558-fb7c-4000-8e8b-24ad71df7eab",
   "metadata": {},
   "source": [
    "#### Read each of the governance CSV files into a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "154ba717-c9a9-43a0-b34e-8b7461952135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18 entries, 0 to 17\n",
      "Data columns (total 9 columns):\n",
      " #   Column                               Non-Null Count  Dtype \n",
      "---  ------                               --------------  ----- \n",
      " 0   EMBRC Node                           18 non-null     object\n",
      " 1   EMBRC Site                           18 non-null     object\n",
      " 2   EMOBON_observatory_id                18 non-null     object\n",
      " 3   Water Column                         17 non-null     object\n",
      " 4   Soft sediment                        7 non-null      object\n",
      " 5   data_quality_control_threshold_date  18 non-null     object\n",
      " 6   data_quality_control_assignee        18 non-null     object\n",
      " 7   rocrate_profile_uri                  18 non-null     object\n",
      " 8   autogenerate                         18 non-null     int64 \n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 1.4+ KB\n",
      "This is info() for None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19 entries, 0 to 18\n",
      "Data columns (total 22 columns):\n",
      " #   Column                            Non-Null Count  Dtype  \n",
      "---  ------                            --------------  -----  \n",
      " 0   country_code                      19 non-null     object \n",
      " 1   country                           19 non-null     object \n",
      " 2   EMOBON_observatory_name           19 non-null     object \n",
      " 3   EMOBON_observatory_id             19 non-null     object \n",
      " 4   startdate                         19 non-null     object \n",
      " 5   enddate                           2 non-null      object \n",
      " 6   Water_Column                      19 non-null     object \n",
      " 7   Soft_Substrates                   19 non-null     object \n",
      " 8   Hard_Substrates                   19 non-null     object \n",
      " 9   water_site_latitude               18 non-null     float64\n",
      " 10  water_site_longitude              18 non-null     float64\n",
      " 11  sediment_site_latitude            9 non-null      float64\n",
      " 12  sediment_site_longtitude          9 non-null      float64\n",
      " 13  hard_substrates_site1_longitude   9 non-null      object \n",
      " 14  hard_substrates_site1_latitude    9 non-null      object \n",
      " 15  hard_substrates_site2_longtitude  3 non-null      float64\n",
      " 16  hard_substrates_site2_latitude    3 non-null      float64\n",
      " 17  contect person                    19 non-null     object \n",
      " 18  contact person email              19 non-null     object \n",
      " 19  ENA_accession_number_umbrella     19 non-null     object \n",
      " 20  ENA_accession_number_project      19 non-null     object \n",
      " 21  EMOBON_core                       19 non-null     object \n",
      "dtypes: float64(6), object(16)\n",
      "memory usage: 3.4+ KB\n",
      "This is info() for None\n"
     ]
    }
   ],
   "source": [
    "github_path = \"https://raw.githubusercontent.com/emo-bon/governance-data/main/\"\n",
    "file_names = [\n",
    "        \"logsheets.csv\",              # contain the URLs of the googlesheets that are the logsheets\n",
    "        \"observatories.csv\"           # contain information about each observatory\n",
    "        #\"organisations.csv\",         # contain information about the organisations in EMO BON\n",
    "        #\"planned_events.csv\"         # contains information about planned EMO BON events (this file is only used by humans, not by any actions) - DONT CARE\n",
    "        #\"ro-crate-metadata.json\"     # IGNORE\n",
    "        ]\n",
    "dfs = {}\n",
    "for f in file_names:\n",
    "    df = pd.read_csv(os.path.join(github_path, f))\n",
    "    print(f\"This is info() for {df.info()}\")\n",
    "    dfs[f] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2484615f-cab2-48a2-8ca7-999bab0051e3",
   "metadata": {},
   "source": [
    "#### Validate Governance tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8dc50e0-2d84-4508-8d82-67b90397c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from validation_classes import observatoriesModel, logsheetsModel\n",
    "validator_class_paths = {\"logsheets.csv\": logsheetsModel, \"observatories.csv\": observatoriesModel}\n",
    "validation_classes_path = \"./validation_classes\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa819fa6-c492-481f-bc81-218057699342",
   "metadata": {},
   "source": [
    "##### Observatories table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af3dfa6-8d4c-4749-b9d1-bc00834e1a3b",
   "metadata": {},
   "source": [
    "The observatories validator mostly changes the column names to make them consistent (and spelled correctly), removes blank strings (\"   \") from cells, and reformats the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9aa2840b-9b25-4155-bd43-34cea9c81de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"observatories.csv\"\n",
    "data = dfs[file_name] # dfs is dict of pandas df's\n",
    "validator = validator_class_paths[file_name]\n",
    "data_records = data.to_dict(orient=\"records\")\n",
    "validated_rows = [validator(**row).model_dump() for row in data_records]\n",
    "\n",
    "#for row in data_records:\n",
    "#    #print(row)\n",
    "#    validator(**row)\n",
    "\n",
    "#for record in validated_rows:\n",
    "#    for field in record:\n",
    "#        print(f\"Record {field} has value {record[field]} is type {type(record[field])}\")\n",
    "\n",
    "ndf = pd.DataFrame.from_records(validated_rows, index=\"observatory_id\")\n",
    "ndf.to_csv(os.path.join(\"governance\", \"observatories_validated.csv\"))\n",
    "\n",
    "# Take a look at ./validation_classes/validated_governance/observatories_validated.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ef5ade-e3f4-4742-b1ba-41233e93e80c",
   "metadata": {},
   "source": [
    "##### Logsheets table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9e1b9e-8a05-4bf2-aceb-55c5c740e77d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_name = \"logsheets.csv\"\n",
    "data = dfs[file_name] # dfs is dict of pandas df's\n",
    "validator = validator_class_paths[file_name]\n",
    "data_records = data.to_dict(orient=\"records\")\n",
    "validated_rows = [validator(**row).model_dump() for row in data_records]\n",
    "        \n",
    "ndf = pd.DataFrame.from_records(validated_rows, index=\"observatory_id\")\n",
    "ndf.to_csv(os.path.join(validation_classes_path, \"governance\", \"logsheets_validated.csv\"))\n",
    "\n",
    "# Take a look at ./validation_classes/validated_governance/logsheets_validated.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fc138c-c96f-4349-843c-ad02276b5f43",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Logsheets from water column and soft sediments sampling and \"measured\" tables\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b0343f-e36b-4685-ac17-e90a9e53f12d",
   "metadata": {},
   "source": [
    "### Version 1 - pulls from Google Sheets only does \"lax\" validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95e6614-c9ff-470d-8363-8f37a08d34ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "    \n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pandas as pd\n",
    "import pydantic\n",
    "from validation_classes import samplingModel, measuredModel\n",
    "\n",
    "def parse_sample_sheets(sampling_strategy: str, sheet_type: str, addresses: pd.core.frame.DataFrame) -> None: \n",
    "    for observatory in addresses:\n",
    "        observatory_id, sheet_link = observatory\n",
    "        #print(f\"Observatory_id {observatory_id} sheet_link {sheet_link}\")\n",
    "        if not isinstance(sheet_link, str):\n",
    "            #print(f\"This is the sheet_link type {type(sheet_link)}\")\n",
    "            if isinstance(sheet_link, float): \n",
    "                if math.isnan(sheet_link):\n",
    "                    print(f\"Observatory {observatory_id} lacks valid sheet URL {sheet_link}\")\n",
    "                    continue\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown link {sheet_link} to observatory {observatory_id}\")\n",
    "        else:\n",
    "\n",
    "            if observatory_id == \"Plenzia\": continue # Sheets not publically available\n",
    "            # UMF soft_sed has two source_mat_ids\n",
    "            if sampling_strategy == \"soft_sediment\" and observatory_id == \"UMF\":\n",
    "                continue\n",
    "            \n",
    "            #if observatory_id in [\"BPNS\", \"Bergen\", \"ESC68N\", \"MBAL4\",\n",
    "            #                     \"VB\", \"ROSKOGO\"]:\n",
    "            #[\"Bergen\", \"MBAL4\", \"ESC68N\", \n",
    "            #               \"BPNS\", \"VB\", \"ROSKOGO\",\n",
    "            #               \"EMT21\", \"PiEGetxo\", \"RFormosa\",\n",
    "            #              \"OSD74\", \"AAOT\", \"NRMCB\",\n",
    "            #              \"HCMR-1\", \"IUIEilat\", \"UMF\"]:\n",
    "            #    continue\n",
    "    \n",
    "            #if observatory_id != \"BPNS\":\n",
    "            #    continue\n",
    "    \n",
    "            print(f\"Processing {observatory_id}...\")\n",
    "            sampling_sheet_base = sheet_link.split(\"/edit\")[0]\n",
    "            sampling_sheet_suffix = \"/gviz/tq?tqx=out:csv&sheet=%s\"\n",
    "            sample_sheet_link = sampling_sheet_base + sampling_sheet_suffix % sheet_type\n",
    "            print(f\"Sample sheet link: {sample_sheet_link}\")\n",
    "            df = pd.read_csv(sample_sheet_link, encoding='utf-8')\n",
    "            data_records_all = df.to_dict(orient=\"records\")\n",
    "    \n",
    "            # Many sheets have partially filled rows\n",
    "            # The source_mat_id is manually curated and the PRIMARY_KEY\n",
    "            # Therefore filter records on source_mat_id\n",
    "            def filter_on_source_mat_id(d):\n",
    "                # Bergen has it as source_material_id\n",
    "                try:\n",
    "                    value = d[\"source_mat_id\"]\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        value = d[\"source_material_id\"]\n",
    "                    except KeyError:\n",
    "                        raise ValueError(\"Cannot find source_mat_id field\")\n",
    "                if isinstance(value, float):\n",
    "                    if math.isnan(value):\n",
    "                        return False\n",
    "                elif value is None:\n",
    "                    return False\n",
    "                # Remove mis-formatted\n",
    "                elif len(value.split(\"_\")) < 6:\n",
    "                    return False\n",
    "                #Edge case of this otherwise blank entry having 6 \"bits\"\n",
    "                elif value == \"EMOBON_VB_Wa_230509_um_\":\n",
    "                    return False \n",
    "                else:\n",
    "                    return True\n",
    "                \n",
    "            data_records_filtered = list(filter(filter_on_source_mat_id, data_records_all))\n",
    "    \n",
    "            if len(data_records_all) > len(data_records_filtered):\n",
    "                print(f\"Discarded {len(data_records_all) - len(data_records_filtered)} records leaving {len(data_records_filtered)}.\")\n",
    "            \n",
    "            validator = validator_classes[sheet_type]\n",
    "            validated_rows = [validator(**row).model_dump() for row in data_records_filtered]\n",
    "    \n",
    "            #for record in validated_rows:\n",
    "            #    for field in record:\n",
    "            #        print(f\"Record {field} has value {record[field]} is type {type(record[field])}\")\n",
    "    \n",
    "            save_dir = \"./logsheets\"\n",
    "            outfile_name = f\"{observatory_id}_{sampling_strategy}_{sheet_type}_validated.csv\"\n",
    "            ndf = pd.DataFrame.from_records(validated_rows, index=\"source_mat_id\")\n",
    "            ndf.to_csv(os.path.join(save_dir, outfile_name))\n",
    "            print(f\"Written {os.path.join(save_dir, outfile_name)}\")\n",
    "\n",
    "\n",
    "validator_classes = {\"sampling\": samplingModel, \"measured\": measuredModel}\n",
    "# Get list of all URL links to sampling sheets\n",
    "# NB  you cant use a \"with\" closure here when reading the Pandas df\n",
    "governance_logsheets_validated_csv = \"./governance/logsheets_validated.csv\"\n",
    "df = pd.read_csv(governance_logsheets_validated_csv)\n",
    "water_column_sheet_addresses = df[[\"observatory_id\", \"water_column\"]].values.tolist()\n",
    "soft_sediment_sheet_addresses  = df[[\"observatory_id\", \"soft_sediment\"]].values.tolist()\n",
    "del df\n",
    "    \n",
    "parse_sample_sheets(\"water_column\", \"sampling\", water_column_sheet_addresses)\n",
    "parse_sample_sheets(\"soft_sediment\", \"sampling\", soft_sediment_sheet_addresses)\n",
    "parse_sample_sheets(\"water_column\", \"measured\", water_column_sheet_addresses)\n",
    "parse_sample_sheets(\"soft_sediment\", \"measured\", soft_sediment_sheet_addresses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2e4fbb-ff53-4058-8fc9-90dfef6423b4",
   "metadata": {},
   "source": [
    "### Version 2: pulls from Google Sheets, does \"lax\" (default), \"strict\" and \"semi-strict\" validation depending on constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec0e469-3c57-4f9c-a41d-9a343a235bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "    \n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pydantic import ValidationError\n",
    "from pprint import pprint\n",
    "from validation_classes import samplingModel, measuredModel, samplingModelStrict, samplingModelSemiStrict\n",
    "\n",
    "############################ CAUTION #############################################\n",
    "STRICT      = False  # As defined by Ioulia, dates corrected, NA's removed etc\n",
    "SEMI_STRICT = True   # As defined by Ioulia but not checking for mandatory fields\n",
    "                     # ints and str coerced to floats when possible\n",
    "##################################################################################\n",
    "\n",
    "def parse_sample_sheets(sampling_strategy: str,\n",
    "                        sheet_type: str,\n",
    "                        addresses: pd.core.frame.DataFrame,\n",
    "                       ) -> None:\n",
    "    \n",
    "    for observatory in addresses:\n",
    "        observatory_id, sheet_link = observatory\n",
    "        #print(f\"Observatory_id {observatory_id} sheet_link {sheet_link}\")\n",
    "        if not isinstance(sheet_link, str):\n",
    "            #print(f\"This is the sheet_link type {type(sheet_link)}\")\n",
    "            if isinstance(sheet_link, float): \n",
    "                if math.isnan(sheet_link):\n",
    "                    print(f\"Observatory {observatory_id} lacks valid sheet URL {sheet_link}\")\n",
    "                    continue\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown link {sheet_link} to observatory {observatory_id}\")\n",
    "        else:\n",
    "\n",
    "            if observatory_id == \"Plenzia\": continue # Sheets not publically available\n",
    "            # UMF soft_sed has two source_mat_ids\n",
    "            if sampling_strategy == \"soft_sediment\" and observatory_id == \"UMF\":\n",
    "                continue\n",
    "    \n",
    "            print(f\"Processing {observatory_id}...\")\n",
    "            sampling_sheet_base = sheet_link.split(\"/edit\")[0]\n",
    "            sampling_sheet_suffix = \"/gviz/tq?tqx=out:csv&sheet=%s\"\n",
    "            sample_sheet_link = sampling_sheet_base + sampling_sheet_suffix % sheet_type\n",
    "            print(f\"Sample sheet link: {sample_sheet_link}\")\n",
    "            df = pd.read_csv(sample_sheet_link, encoding='utf-8')\n",
    "            data_records_all = df.to_dict(orient=\"records\")\n",
    "    \n",
    "            # Many sheets have partially filled rows\n",
    "            # The source_mat_id is manually curated and the PRIMARY_KEY\n",
    "            # Therefore filter records on source_mat_id\n",
    "            def filter_on_source_mat_id(d):\n",
    "                # Bergen has it as source_material_id\n",
    "                try:\n",
    "                    value = d[\"source_mat_id\"]\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        value = d[\"source_material_id\"]\n",
    "                    except KeyError:\n",
    "                        raise ValueError(\"Cannot find source_mat_id field\")\n",
    "                if isinstance(value, float):\n",
    "                    if math.isnan(value):\n",
    "                        return False\n",
    "                elif value is None:\n",
    "                    return False\n",
    "                # Remove mis-formatted\n",
    "                elif len(value.split(\"_\")) < 6:\n",
    "                    return False\n",
    "                #Edge case of this otherwise blank entry having 6 \"bits\"\n",
    "                elif value == \"EMOBON_VB_Wa_230509_um_\":\n",
    "                    return False \n",
    "                else:\n",
    "                    return True\n",
    "                \n",
    "            data_records_filtered = list(filter(filter_on_source_mat_id, data_records_all))\n",
    "    \n",
    "            if len(data_records_all) > len(data_records_filtered):\n",
    "                print(f\"Discarded {len(data_records_all) - len(data_records_filtered)} records leaving {len(data_records_filtered)}.\")\n",
    "\n",
    "            if STRICT:\n",
    "                model_type = f\"{sheet_type}_strict\"\n",
    "            elif SEMI_STRICT:\n",
    "                model_type = f\"{sheet_type}_semistrict\"\n",
    "            else:\n",
    "                model_type = sheet_type\n",
    "\n",
    "            validator = validator_classes[model_type]\n",
    "\n",
    "            #validated_rows = [validator(**row).model_dump() for row in data_records_filtered]\n",
    "            validated_rows = []\n",
    "            errors: List[List[str:List[Dict]]] = [] # where each error is the inner Dict\n",
    "            for row in data_records_filtered:\n",
    "                try:\n",
    "                    vr = validator(**row)\n",
    "                except ValidationError as e:\n",
    "                    if observatory_id == \"Bergen\":\n",
    "                        errors.append([(row[\"source_material_id\"], e.errors())])\n",
    "                    else:\n",
    "                        errors.append([(row[\"source_mat_id\"], e.errors())])\n",
    "                else:\n",
    "                    validated_rows.append(vr.model_dump())\n",
    "\n",
    "            if errors:\n",
    "                # errors is a list of lists where each inner list is a dict of row errors\n",
    "                # where each isof key = source_mat_id and values is list of dicts each of which\n",
    "                # is an error:\n",
    "                #List[List[str:List[Dict]]]\n",
    "                total_number_errors = sum([len(row[1]) for e in errors for row in e])\n",
    "                print(f\"Errors were found... {total_number_errors} in total\")\n",
    "                save_dir = \"./validation_errors\"\n",
    "                outfile_name_pk = f\"{observatory_id}_{sampling_strategy}_{model_type}_ERRORS.pickle\"\n",
    "                out_path_pk = os.path.join(save_dir, outfile_name_pk)\n",
    "                with open(out_path_pk, \"wb\") as f:\n",
    "                    pickle.dump(errors, f, pickle.HIGHEST_PROTOCOL)\n",
    "                outfile_name_log = f\"{observatory_id}_{sampling_strategy}_{model_type}_ERRORS.log\"\n",
    "                out_path_log = os.path.join(save_dir, outfile_name_log)                \n",
    "                with open(out_path_log, \"w\") as f:\n",
    "                    pprint(errors, f)\n",
    "            else:\n",
    "                assert len(validated_rows) == len(data_records_filtered), \"Not sure what happenned, but len(validated_rows) != len(data_filtered_records)\"\n",
    "                print(\"All records passed!\")\n",
    "            \n",
    "                #for record in validated_rows:\n",
    "                #    for field in record:\n",
    "                #        print(f\"Record {field} has value {record[field]} is type {type(record[field])}\")\n",
    "\n",
    "                if not STRICT and not SEMI_STRICT:\n",
    "                    save_dir = \"./logsheets\"\n",
    "                    outfile_name = f\"{observatory_id}_{sampling_strategy}_{model_type}_validated.csv\"\n",
    "                    ndf = pd.DataFrame.from_records(validated_rows, index=\"source_mat_id\")\n",
    "                    ndf.to_csv(os.path.join(save_dir, outfile_name))\n",
    "                    print(f\"Written {os.path.join(save_dir, outfile_name)}\")\n",
    "\n",
    "\n",
    "validator_classes = {\"sampling\": samplingModel,\n",
    "                     \"measured\": measuredModel,\n",
    "                     \"sampling_strict\": samplingModelStrict,\n",
    "                     \"sampling_semistrict\": samplingModelSemiStrict\n",
    "                    }\n",
    "\n",
    "# Get list of all URL links to sampling sheets\n",
    "# NB  you cant use a \"with\" closure here when reading the Pandas df\n",
    "governance_logsheets_validated_csv = \"./governance/logsheets_validated.csv\"\n",
    "df = pd.read_csv(governance_logsheets_validated_csv)\n",
    "water_column_sheet_addresses = df[[\"observatory_id\", \"water_column\"]].values.tolist()\n",
    "soft_sediment_sheet_addresses  = df[[\"observatory_id\", \"soft_sediment\"]].values.tolist()\n",
    "del df\n",
    "\n",
    "parse_sample_sheets(\"water_column\", \"sampling\", water_column_sheet_addresses)\n",
    "\n",
    "#parse_sample_sheets(\"water_column\", \"sampling\", water_column_sheet_addresses)\n",
    "#parse_sample_sheets(\"soft_sediment\", \"sampling\", soft_sediment_sheet_addresses)\n",
    "#parse_sample_sheets(\"water_column\", \"measured\", water_column_sheet_addresses)\n",
    "#parse_sample_sheets(\"soft_sediment\", \"measured\", soft_sediment_sheet_addresses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dbec45-35c7-41f6-a245-8a81cf547cf9",
   "metadata": {},
   "source": [
    "### Version 3: pulls sheets from Github after curation, does \"lax\", \"strict\", and \"semi-strict\" validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b732a61-afa4-4281-8acd-1a8ad3f46671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ESC68N... water_sampling.csv\n",
      "Discarded 124 records leaving 120.\n",
      "All records passed!\n",
      "Written ./logsheets_github/ESC68N_water_sampling_github_validated.csv\n",
      "Processing Bergen... water_sampling.csv\n",
      "Observatory Bergen does not have a transformed water_sampling.csv on GH\n",
      "Processing MBAL4... water_sampling.csv\n",
      "Discarded 162 records leaving 80.\n",
      "All records passed!\n",
      "Written ./logsheets_github/MBAL4_water_sampling_github_validated.csv\n",
      "Processing BPNS... water_sampling.csv\n",
      "All records passed!\n",
      "Written ./logsheets_github/BPNS_water_sampling_github_validated.csv\n",
      "Processing ROSKOGO... water_sampling.csv\n",
      "All records passed!\n",
      "Written ./logsheets_github/ROSKOGO_water_sampling_github_validated.csv\n",
      "Processing VB... water_sampling.csv\n",
      "Discarded 296 records leaving 554.\n",
      "All records passed!\n",
      "Written ./logsheets_github/VB_water_sampling_github_validated.csv\n",
      "Observatory OOB does not have a water sampling strategy.\n",
      "Processing EMT21... water_sampling.csv\n",
      "Discarded 17 records leaving 188.\n",
      "All records passed!\n",
      "Written ./logsheets_github/EMT21_water_sampling_github_validated.csv\n",
      "Processing PiEGetxo... water_sampling.csv\n",
      "Discarded 6 records leaving 264.\n",
      "All records passed!\n",
      "Written ./logsheets_github/PiEGetxo_water_sampling_github_validated.csv\n",
      "Processing RFormosa... water_sampling.csv\n",
      "All records passed!\n",
      "Written ./logsheets_github/RFormosa_water_sampling_github_validated.csv\n",
      "Processing OSD74... water_sampling.csv\n",
      "Discarded 28 records leaving 150.\n",
      "All records passed!\n",
      "Written ./logsheets_github/OSD74_water_sampling_github_validated.csv\n",
      "Processing AAOT... water_sampling.csv\n",
      "Discarded 268 records leaving 320.\n",
      "All records passed!\n",
      "Written ./logsheets_github/AAOT_water_sampling_github_validated.csv\n",
      "Processing NRMCB... water_sampling.csv\n",
      "All records passed!\n",
      "Written ./logsheets_github/NRMCB_water_sampling_github_validated.csv\n",
      "Processing HCMR-1... water_sampling.csv\n",
      "All records passed!\n",
      "Written ./logsheets_github/HCMR-1_water_sampling_github_validated.csv\n",
      "Processing IUIEilat... water_sampling.csv\n",
      "All records passed!\n",
      "Written ./logsheets_github/IUIEilat_water_sampling_github_validated.csv\n",
      "Processing UMF... water_sampling.csv\n",
      "Discarded 86 records leaving 139.\n",
      "All records passed!\n",
      "Written ./logsheets_github/UMF_water_sampling_github_validated.csv\n",
      "Processing LMO... water_sampling.csv\n",
      "Discarded 148 records leaving 70.\n",
      "All records passed!\n",
      "Written ./logsheets_github/LMO_water_sampling_github_validated.csv\n",
      "Observatory ESC68N does not have a sediment sampling strategy.\n",
      "Observatory Bergen does not have a sediment sampling strategy.\n",
      "Observatory MBAL4 does not have a sediment sampling strategy.\n",
      "Processing BPNS... sediment_sampling.csv\n",
      "All records passed!\n",
      "Written ./logsheets_github/BPNS_sediment_sampling_github_validated.csv\n",
      "Processing ROSKOGO... sediment_sampling.csv\n",
      "Discarded 80 records leaving 122.\n",
      "All records passed!\n",
      "Written ./logsheets_github/ROSKOGO_sediment_sampling_github_validated.csv\n",
      "Observatory VB does not have a sediment sampling strategy.\n",
      "Processing OOB... sediment_sampling.csv\n",
      "Discarded 93 records leaving 128.\n",
      "All records passed!\n",
      "Written ./logsheets_github/OOB_sediment_sampling_github_validated.csv\n",
      "Processing EMT21... sediment_sampling.csv\n",
      "Discarded 137 records leaving 40.\n",
      "All records passed!\n",
      "Written ./logsheets_github/EMT21_sediment_sampling_github_validated.csv\n",
      "Observatory PiEGetxo does not have a sediment sampling strategy.\n",
      "Observatory Plenzia does not have a sediment sampling strategy.\n",
      "Processing RFormosa... sediment_sampling.csv\n",
      "All records passed!\n",
      "Written ./logsheets_github/RFormosa_sediment_sampling_github_validated.csv\n",
      "Observatory OSD74 does not have a sediment sampling strategy.\n",
      "Observatory AAOT does not have a sediment sampling strategy.\n",
      "Processing NRMCB... sediment_sampling.csv\n",
      "All records passed!\n",
      "Written ./logsheets_github/NRMCB_sediment_sampling_github_validated.csv\n",
      "Observatory HCMR-1 does not have a sediment sampling strategy.\n",
      "Observatory IUIEilat does not have a sediment sampling strategy.\n",
      "Observatory LMO does not have a sediment sampling strategy.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "    \n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "from urllib.request import HTTPError\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "from pydantic import ValidationError\n",
    "from pprint import pprint\n",
    "from validation_classes import (samplingModelGithub,               # lax validator for EMO-BON Github repository\n",
    "                                samplingModelGithubStrict,         # strict validator for EMO-BON Github repository\n",
    "                                samplingModelGithubSemiStrict      # semi-strict validator for EMO-BON Github repository                  \n",
    "                               )\n",
    "class SamplingStrategy(Enum):\n",
    "    WATER    = \"water\"    # Originally water_column\n",
    "    SEDIMENT = \"sediment\" # Originally soft_sediment\n",
    "\n",
    "class SheetType(Enum):\n",
    "    SAMPLING = \"sampling\"\n",
    "    MEASURED = \"measured\"\n",
    "\n",
    "# Not all observatories have \"transformed\" sheets on GH, but may have \"raw\"\n",
    "# Of course the types in the fields are different to difficult to validate\n",
    "# with a single validator - best just to ignore the raw sheets\n",
    "USE_RAW = False\n",
    "\n",
    "############################ CAUTION #############################################\n",
    "# As defined by Ioulia, dates corrected, NA's removed etc\n",
    "STRICT      = False\n",
    "\n",
    "# As defined by Ioulia but not checking for mandatory fields\n",
    "# ints and str coerced to floats when possible\n",
    "SEMI_STRICT = False  \n",
    "##################################################################################\n",
    "\n",
    "def get_sheet_from_github(observatory_id: SheetType,\n",
    "                          sampling_strategy: SamplingStrategy, \n",
    "                          sheet_type: str) -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    Here we pull the \"sampling\" or \"measured\" sheets from Github. These are the curated\n",
    "    sheets downloaded by the Github actions and hopefully do not have the errors that\n",
    "    the CSV's pulled directly from Google Sheets had (e.g. the word \"blank\" magically\n",
    "    disappering from the \"replicate\" field.\n",
    "\n",
    "    Github paths look like:\n",
    "    https://raw.githubusercontent.com/emo-bon/observatory-umf-crate/main/logsheets/transformed/sediment_measured.csv\n",
    "    https://raw.githubusercontent.com/emo-bon/observatory-bergen-crate/main/logsheets/raw/water_sampling.csv\n",
    "    \"\"\"\n",
    "\n",
    "    prefix     = \"https://raw.githubusercontent.com/emo-bon\"\n",
    "    obs_name   = f\"observatory-{observatory_id}-crate\"\n",
    "    inter_path = \"main/logsheets\"\n",
    "    dir_path   = \"transformed\"\n",
    "    sheet_name = f\"{sampling_strategy}_{sheet_type}.csv\"\n",
    "\n",
    "    print(f\"Processing {observatory_id}... {sheet_name}\")\n",
    "    github_addr = os.path.join(prefix, obs_name, inter_path, dir_path, sheet_name)\n",
    "    try:\n",
    "        df = pd.read_csv(github_addr)\n",
    "    except HTTPError:\n",
    "        # Some observatories don't yet have transformed sheets\n",
    "        if USE_RAW:\n",
    "            # Try for the raw sheets\n",
    "            print(\"Unable to find 'transformed' sheet, reading the 'raw' sheet\")\n",
    "            dir_path   = \"raw\"\n",
    "            github_addr = os.path.join(prefix, obs_name, inter_path, dir_path, sheet_name)\n",
    "            try:\n",
    "                df = pd.read_csv(github_addr)\n",
    "            except HTTPError:\n",
    "                raise ValueError(\"Unable to find transformed or raw sheet\")\n",
    "        else:\n",
    "            print(f\"Observatory {observatory_id} does not have a transformed {sheet_name} on GH\")\n",
    "            return None\n",
    "            \n",
    "    return df\n",
    "\n",
    "def filter_on_source_mat_id(d):\n",
    "    # Bergen has it as source_material_id on Google and Github\n",
    "    try:\n",
    "        value = d[\"source_mat_id\"]\n",
    "    except KeyError:\n",
    "        try:\n",
    "            value = d[\"source_material_id\"]\n",
    "        except KeyError:\n",
    "            raise ValueError(\"Cannot find source_mat_id field\")\n",
    "    if isinstance(value, float):\n",
    "        if math.isnan(value):\n",
    "            return False\n",
    "    elif value is None:\n",
    "        return False\n",
    "    # Remove mis-formatted\n",
    "    elif len(value.split(\"_\")) < 6:\n",
    "        return False\n",
    "    #Edge case of this otherwise blank entry having 6 \"bits\"\n",
    "    elif value == \"EMOBON_VB_Wa_230509_um_\":\n",
    "        return False \n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def parse_sample_sheets(sampling_strategy: str,\n",
    "                        sheet_type: str,\n",
    "                        addresses: pd.core.frame.DataFrame,\n",
    "                       ) -> None:\n",
    "    \n",
    "    for observatory in addresses:\n",
    "        observatory_id, sheet_link = observatory\n",
    "        #print(f\"ObSservatory_id {observatory_id} sheet_link {sheet_link}\")\n",
    "        if not isinstance(sheet_link, str):\n",
    "            #print(f\"This is the sheet_link type {type(sheet_link)}\")\n",
    "            if isinstance(sheet_link, float):\n",
    "                # Only OOB doesnt do water_column\n",
    "                # But most do not do soft-sediments\n",
    "                if math.isnan(sheet_link):\n",
    "                    print(f\"Observatory {observatory_id} does not have a {sampling_strategy} sampling strategy.\")\n",
    "                    continue\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown value \\'{sheet_link}\\' in {sampling_strategy} cell of {observatory_id}\")\n",
    "        else:\n",
    "\n",
    "            if observatory_id == \"Plenzia\": continue # Sheets not publically available\n",
    "\n",
    "            ################ CAUTION ##################\n",
    "            #if not observatory_id in [\"AAOT\"]: continue\n",
    "            \n",
    "            # UMF soft_sed has two source_mat_ids\n",
    "            if sampling_strategy == \"sediment\" and observatory_id == \"UMF\":\n",
    "                continue\n",
    "    \n",
    "            df = get_sheet_from_github(observatory_id, sampling_strategy, sheet_type)\n",
    "            if df is None:\n",
    "                continue\n",
    "            data_records_all = df.to_dict(orient=\"records\")\n",
    "    \n",
    "            # Many sheets have partially filled rows\n",
    "            # The source_mat_id is manually curated and the PRIMARY_KEY\n",
    "            # Therefore filter records on source_mat_id\n",
    "                \n",
    "            data_records_filtered = list(filter(filter_on_source_mat_id, data_records_all))\n",
    "    \n",
    "            if len(data_records_all) > len(data_records_filtered):\n",
    "                print(f\"Discarded {len(data_records_all) - len(data_records_filtered)} records leaving {len(data_records_filtered)}.\")\n",
    "\n",
    "            ################ CAUTION ##############\n",
    "            #continue\n",
    "\n",
    "            if STRICT:\n",
    "                model_type = f\"{sheet_type}_github_strict\"\n",
    "            elif SEMI_STRICT:\n",
    "                model_type = f\"{sheet_type}_github_semistrict\"\n",
    "            else:\n",
    "                model_type = f\"{sheet_type}_github\"\n",
    "\n",
    "            validator = validator_classes[model_type]\n",
    "            #print(f\"Using {validator} from {model_type}\")\n",
    "\n",
    "            #validated_rows = [validator(**row).model_dump() for row in data_records_filtered]\n",
    "            validated_rows = []\n",
    "            errors: List[List[str:List[Dict]]] = [] # where each error is the inner Dict\n",
    "            for row in data_records_filtered:\n",
    "                try:\n",
    "                    vr = validator(**row)\n",
    "                except ValidationError as e:\n",
    "                    if observatory_id == \"Bergen\":\n",
    "                        errors.append([(row[\"source_material_id\"], e.errors())])\n",
    "                    else:\n",
    "                        errors.append([(row[\"source_mat_id\"], e.errors())])\n",
    "                else:\n",
    "                    validated_rows.append(vr.model_dump())\n",
    "\n",
    "            if errors:\n",
    "                # errors is a list of lists where each inner list is a dict of row errors\n",
    "                # where each isof key = source_mat_id and values is list of dicts each of which\n",
    "                # is an error:\n",
    "                #List[List[str:List[Dict]]]\n",
    "                total_number_errors = sum([len(row[1]) for e in errors for row in e])\n",
    "                print(f\"Errors were found... {total_number_errors} in total\")\n",
    "                save_dir = \"./validation_errors_github\"\n",
    "                #outfile_name_pk = f\"{observatory_id}_{sampling_strategy}_{model_type}_ERRORS.pickle\"\n",
    "                #out_path_pk = os.path.join(save_dir, outfile_name_pk)\n",
    "                #with open(out_path_pk, \"wb\") as f:\n",
    "                #    pickle.dump(errors, f, pickle.HIGHEST_PROTOCOL)\n",
    "                outfile_name_log = f\"{observatory_id}_{sampling_strategy}_{model_type}_ERRORS.log\"\n",
    "                out_path_log = os.path.join(save_dir, outfile_name_log)                \n",
    "                with open(out_path_log, \"w\") as f:\n",
    "                    pprint(errors, f)\n",
    "            else:\n",
    "                assert len(validated_rows) == len(data_records_filtered), \\\n",
    "                    \"Not sure what happenned, but len(validated_rows) != len(data_filtered_records)\"\n",
    "                print(\"All records passed!\")\n",
    "            \n",
    "                #for record in validated_rows:\n",
    "                #    for field in record:\n",
    "                #        print(f\"Record {field} has value {record[field]} is type {type(record[field])}\")\n",
    "\n",
    "                if not STRICT and not SEMI_STRICT:\n",
    "                    save_dir = \"./logsheets_github\"\n",
    "                    outfile_name = f\"{observatory_id}_{sampling_strategy}_{model_type}_validated.csv\"\n",
    "                    ndf = pd.DataFrame.from_records(validated_rows, index=\"source_mat_id\")\n",
    "                    ndf.to_csv(os.path.join(save_dir, outfile_name))\n",
    "                    print(f\"Written {os.path.join(save_dir, outfile_name)}\")\n",
    "\n",
    "validator_classes = {\"sampling_github\"           : samplingModelGithub, \n",
    "                     \"sampling_github_strict\"    : samplingModelGithubStrict,\n",
    "                     \"sampling_github_semistrict\": samplingModelGithubSemiStrict\n",
    "                    }\n",
    "\n",
    "# Get list of all URL links to sampling sheets\n",
    "# NB  you cant use a \"with\" closure here when reading the Pandas df\n",
    "governance_logsheets_validated_csv = \"./governance/logsheets_validated.csv\"\n",
    "df = pd.read_csv(governance_logsheets_validated_csv)\n",
    "water_column_sheet_addresses = df[[\"observatory_id\", \"water_column\"]].values.tolist()\n",
    "soft_sediment_sheet_addresses  = df[[\"observatory_id\", \"soft_sediment\"]].values.tolist()\n",
    "del df\n",
    "\n",
    "parse_sample_sheets(\"water\", \"sampling\", water_column_sheet_addresses)\n",
    "parse_sample_sheets(\"sediment\", \"sampling\", soft_sediment_sheet_addresses)\n",
    "#parse_sample_sheets(\"water\", \"measured\", water_column_sheet_addresses)\n",
    "#parse_sample_sheets(\"sediment\", \"measured\", soft_sediment_sheet_addresses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c0a641-6fa8-48d5-b3d3-0785beaf2f68",
   "metadata": {},
   "source": [
    "# Create meta-table of all logsheets.sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33792f86-8da9-4cd5-9986-29a80aedfbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observatory ESC68N-water_column has 150 sampling events, of which 136 have missing no ref_code (not sent for sequencing), and 0 have a refcode but not 'measured' data. A total of 14 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory Bergen-water_column has 108 sampling events, of which 108 have missing no ref_code (not sent for sequencing), and 0 have a refcode but not 'measured' data. A total of 0 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory MBAL4-water_column has 78 sampling events, of which 74 have missing no ref_code (not sent for sequencing), and 0 have a refcode but not 'measured' data. A total of 4 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory BPNS-water_column has 280 sampling events, of which 271 have missing no ref_code (not sent for sequencing), and 0 have a refcode but not 'measured' data. A total of 9 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory BPNS-soft_sediment has 144 sampling events, of which 136 have missing no ref_code (not sent for sequencing), and 0 have a refcode but not 'measured' data. A total of 8 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory ROSKOGO-water_column has 234 sampling events, of which 225 have missing no ref_code (not sent for sequencing), and 0 have a refcode but not 'measured' data. A total of 9 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory ROSKOGO-soft_sediment has 155 sampling events, of which 149 have missing no ref_code (not sent for sequencing), and 0 have a refcode but not 'measured' data. A total of 6 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory VB-water_column has 764 sampling events, of which 755 have missing no ref_code (not sent for sequencing), and 0 have a refcode but not 'measured' data. A total of 9 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory OOB-soft_sediment has 133 sampling events, of which 126 have missing no ref_code (not sent for sequencing), and 0 have a refcode but not 'measured' data. A total of 7 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory EMT21-water_column has 188 sampling events, of which 181 have missing no ref_code (not sent for sequencing), and 0 have a refcode but not 'measured' data. A total of 7 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory EMT21-soft_sediment has 48 sampling events, of which 48 have missing no ref_code (not sent for sequencing), and 0 have a refcode but not 'measured' data. A total of 0 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory PiEGetxo-water_column has 239 sampling events, of which 232 have missing no ref_code (not sent for sequencing), and 0 have a refcode but not 'measured' data. A total of 7 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory RFormosa-water_column has 170 sampling events, of which 163 have missing no ref_code (not sent for sequencing), and 0 have a refcode but not 'measured' data. A total of 7 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory RFormosa-soft_sediment has 85 sampling events, of which 79 have missing no ref_code (not sent for sequencing), and 0 have a refcode but not 'measured' data. A total of 6 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory OSD74-water_column has 170 sampling events, of which 164 have missing no ref_code (not sent for sequencing), and 0 have a refcode but not 'measured' data. A total of 6 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory AAOT-water_column has 320 sampling events, of which 311 have missing no ref_code (not sent for sequencing), and 0 have a refcode but not 'measured' data. A total of 9 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory NRMCB-water_column has 382 sampling events, of which 373 have missing no ref_code (not sent for sequencing), and 0 have a refcode but not 'measured' data. A total of 9 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory NRMCB-soft_sediment has 95 sampling events, of which 85 have missing no ref_code (not sent for sequencing), and 0 have a refcode but not 'measured' data. A total of 10 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory HCMR-1-water_column has 144 sampling events, of which 140 have missing no ref_code (not sent for sequencing), and 0 have a refcode but not 'measured' data. A total of 4 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory IUIEilat-water_column has 60 sampling events, of which 53 have missing no ref_code (not sent for sequencing), and 0 have a refcode but not 'measured' data. A total of 7 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "Observatory LMO-water_column has 100 sampling events, of which 100 have missing no ref_code (not sent for sequencing), and 0 have a refcode but not 'measured' data. A total of 0 sampling events with refcode and measured sheet were found.\n",
      "\n",
      "'There are 227 total ref_codes assigned'\n",
      "'Total number of combined sampling events with ref_codes: 138'\n",
      "'Total number of all_source_mat_ids_from_sheets: 4047'\n",
      "A total of 90 source_mat_ids in the batch 1 & 2 run information sheets are missing from the observatory sampling sheets\n",
      "Total combined_events 138 + 90 = 228 and should be equal to total number of refcodes assigned in the run information sheets 227\n",
      "'The missing source_mat_ids that are in the run information sheets are:'\n",
      "Missing source_mat_id is EMOBON_BPNS_Wa_210701_0.2um_2 close matches are \n",
      "\t EMOBON_BPNS_Wa_210701_200um_2 EMOBON_BPNS_Wa_210701_3um_2 EMOBON_BPNS_Wa_210701_NAum_2\n",
      "Missing source_mat_id is EMOBON_BPNS_Wa_210701_0.2um_1 close matches are \n",
      "\t EMOBON_BPNS_Wa_210701_200um_1 EMOBON_BPNS_Wa_210701_3um_1 EMOBON_BPNS_Wa_210701_NAum_1\n",
      "Missing source_mat_id is EMOBON_BPNS_Wa_210825_0.2um_1 close matches are \n",
      "\t EMOBON_BPNS_Wa_210825_200um_1 EMOBON_BPNS_Wa_210825_3um_1 EMOBON_BPNS_Wa_210825_NAum_1\n",
      "Missing source_mat_id is EMOBON_BPNS_Wa_210825_0.2um_2 close matches are \n",
      "\t EMOBON_BPNS_Wa_210825_200um_2 EMOBON_BPNS_Wa_210825_3um_2 EMOBON_BPNS_Wa_210825_NAum_2\n",
      "Missing source_mat_id is EMOBON_ROSKOGO_Wa_210618_0.2um_1 close matches are \n",
      "\t EMOBON_ROSKOGO_Wa_210618_200um_1 EMOBON_ROSKOGO_Wa_210618_200um_1 EMOBON_ROSKOGO_Wa_210618_3um_1\n",
      "Missing source_mat_id is EMOBON_ROSKOGO_Wa_210618_0.2um_2 close matches are \n",
      "\t EMOBON_ROSKOGO_Wa_210618_200um_2 EMOBON_ROSKOGO_Wa_210618_3um_2 EMOBON_ROSKOGO_Wa_210618_200um_4\n",
      "Missing source_mat_id is EMOBON_ROSKOGO_Wa_210802_0.2um_1 close matches are \n",
      "\t EMOBON_ROSKOGO_Wa_210802_200um_1 EMOBON_ROSKOGO_Wa_210802_3um_1 EMOBON_ROSKOGO_Wa_210802_200um_4\n",
      "Missing source_mat_id is EMOBON_ROSKOGO_Wa_210802_0.2um_2 close matches are \n",
      "\t EMOBON_ROSKOGO_Wa_210802_200um_2 EMOBON_ROSKOGO_Wa_210802_3um_2 EMOBON_ROSKOGO_Wa_210802_200um_4\n",
      "Missing source_mat_id is EMOBON_VB_Wa_210621_0.2um_1 close matches are \n",
      "\t EMOBON_VB_Wa_210621_3um_1 EMOBON_VB_Wa_210621_NAum_1 EMOBON_VB_Wa_210621_NAum_1\n",
      "Missing source_mat_id is EMOBON_VB_Wa_210621_0.2um_2 close matches are \n",
      "\t EMOBON_VB_Wa_210621_3um_2 EMOBON_VB_Wa_210621_NAum_2 EMOBON_VB_Wa_210621_NAum_2\n",
      "Missing source_mat_id is EMOBON_VB_Wa_210823_0.2um_1 close matches are \n",
      "\t EMOBON_VB_Wa_210823_3um_1 EMOBON_VB_Wa_210823_NAum_1 EMOBON_VB_Wa_210823_NAum_1\n",
      "Missing source_mat_id is EMOBON_VB_Wa_210823_0.2um_2 close matches are \n",
      "\t EMOBON_VB_Wa_210823_3um_2 EMOBON_VB_Wa_210823_NAum_2 EMOBON_VB_Wa_210823_NAum_2\n",
      "Missing source_mat_id is EMOBON_EMT21_Wa_210825_0.2um_1 close matches are \n",
      "\t EMOBON_EMT21_Wa_210825_200um_1 EMOBON_EMT21_Wa_210825_3um_1 EMOBON_EMT21_Wa_210825_NAum_1\n",
      "Missing source_mat_id is EMOBON_EMT21_Wa_210825_0.2um_2 close matches are \n",
      "\t EMOBON_EMT21_Wa_210825_200um_2 EMOBON_EMT21_Wa_210825_3um_2 EMOBON_EMT21_Wa_210825_NAum_2\n",
      "Missing source_mat_id is EMOBON_PiEGetxo_Wa_210824_0.2um_1 close matches are \n",
      "\t EMOBON_PiEGetxo_Wa_210824_200um_1 EMOBON_PiEGetxo_Wa_210824_3um_1 EMOBON_PiEGetxo_Wa_221024_200um_1\n",
      "Missing source_mat_id is EMOBON_PiEGetxo_Wa_210824_0.2um_2 close matches are \n",
      "\t EMOBON_PiEGetxo_Wa_210824_200um_2 EMOBON_PiEGetxo_Wa_210824_3um_2 EMOBON_PiEGetxo_Wa_221024_200um_2\n",
      "Missing source_mat_id is EMOBON_PiEGetxo_Wa_210824_0.2um_blank close matches are \n",
      "\t EMOBON_PiEGetxo_Wa_210824_200um_blank EMOBON_PiEGetxo_Wa_210824_200um_blank EMOBON_PiEGetxo_Wa_210824_3um_blank\n",
      "Missing source_mat_id is EMOBON_RFormosa_Wa_210805_0.2um_1 close matches are \n",
      "\t EMOBON_RFormosa_Wa_210805_200um_1 EMOBON_RFormosa_Wa_210805_3um_1 EMOBON_RFormosa_Wa_210805_200um_4\n",
      "Missing source_mat_id is EMOBON_RFormosa_Wa_210805_0.2um_2 close matches are \n",
      "\t EMOBON_RFormosa_Wa_210805_200um_2 EMOBON_RFormosa_Wa_210805_3um_2 EMOBON_RFormosa_Wa_210805_200um_4\n",
      "Missing source_mat_id is EMOBON_OSD74_Wa_210831_0.2um_1 close matches are \n",
      "\t EMOBON_OSD74_Wa_210831_200um_1 EMOBON_OSD74_Wa_210831_3um_1 EMOBON_OSD74_Wa_210831_200um_4\n",
      "Missing source_mat_id is EMOBON_OSD74_Wa_210831_0.2um_2 close matches are \n",
      "\t EMOBON_OSD74_Wa_210831_200um_2 EMOBON_OSD74_Wa_210831_3um_2 EMOBON_OSD74_Wa_210831_200um_4\n",
      "Missing source_mat_id is EMOBON_AAOT_Wa_210622_0.2um_1 close matches are \n",
      "\t EMOBON_AAOT_Wa_210622_200um_1 EMOBON_AAOT_Wa_210622_3um_1 EMOBON_AAOT_Wa_210622_2000um_1\n",
      "Missing source_mat_id is EMOBON_AAOT_Wa_210622_0.2um_2 close matches are \n",
      "\t EMOBON_AAOT_Wa_210622_200um_2 EMOBON_AAOT_Wa_210622_3um_2 EMOBON_AAOT_Wa_210622_2000um_2\n",
      "Missing source_mat_id is EMOBON_AAOT_Wa_210809_0.2um_1 close matches are \n",
      "\t EMOBON_AAOT_Wa_210809_200um_1 EMOBON_AAOT_Wa_210809_3um_1 EMOBON_AAOT_Wa_210809_2000um_1\n",
      "Missing source_mat_id is EMOBON_AAOT_Wa_210809_0.2um_2 close matches are \n",
      "\t EMOBON_AAOT_Wa_210809_200um_2 EMOBON_AAOT_Wa_210809_3um_2 EMOBON_AAOT_Wa_210809_2000um_2\n",
      "Missing source_mat_id is EMOBON_NRMCB_Wa_210621_0.2um_1 close matches are \n",
      "\t EMOBON_NRMCB_Wa_210621_200um_1 EMOBON_NRMCB_Wa_210621_3um_1 EMOBON_NRMCB_Wa_210621_NAum_1\n",
      "Missing source_mat_id is EMOBON_NRMCB_Wa_210621_0.2um_2 close matches are \n",
      "\t EMOBON_NRMCB_Wa_210621_200um_2 EMOBON_NRMCB_Wa_210621_3um_2 EMOBON_NRMCB_Wa_210621_NAum_2\n",
      "Missing source_mat_id is EMOBON_NRMCB_Wa_210831_0.2um_1 close matches are \n",
      "\t EMOBON_NRMCB_Wa_210831_3um_1 EMOBON_NRMCB_Wa_210831_NAum_1 EMOBON_NRMCB_Wa_210831_NAum_1\n",
      "Missing source_mat_id is EMOBON_NRMCB_Wa_210831_0.2um_2 close matches are \n",
      "\t EMOBON_NRMCB_Wa_210831_3um_2 EMOBON_NRMCB_Wa_210831_NAum_2 EMOBON_NRMCB_Wa_210831_NAum_2\n",
      "Missing source_mat_id is EMOBON_NRMCB_Wa_210831_0.2um_blank close matches are \n",
      "\t EMOBON_NRMCB_Wa_210831_3um_blank EMOBON_NRMCB_Wa_210831_NAum_blank EMOBON_NRMCB_Wa_210831_NAum_blank\n",
      "Missing source_mat_id is EMOBON_HCMR-1_Wa_210628_0.2um_1 close matches are \n",
      "\t EMOBON_HCMR-1_Wa_210628_200um_1 EMOBON_HCMR-1_Wa_210628_3um_1 EMOBON_HCMR-1_Wa_210628_200um_4\n",
      "Missing source_mat_id is EMOBON_HCMR-1_Wa_210628_0.2um_2 close matches are \n",
      "\t EMOBON_HCMR-1_Wa_210628_200um_2 EMOBON_HCMR-1_Wa_210628_3um_2 EMOBON_HCMR-1_Wa_210628_200um_4\n",
      "Missing source_mat_id is EMOBON_HCMR-1_Wa_210917_3um_blank close matches are \n",
      "\t EMOBON_HCMR-1_Wa_210917_3um_blank2 EMOBON_HCMR-1_Wa_210917_3um_blank1 EMOBON_HCMR-1_Wa_210917_200um_blank2\n",
      "Missing source_mat_id is EMOBON_HCMR-1_Wa_210917_0.2um_1 close matches are \n",
      "\t EMOBON_HCMR-1_Wa_210917_200um_1 EMOBON_HCMR-1_Wa_210917_3um_1 EMOBON_HCMR-1_Wa_210917_200um_4\n",
      "Missing source_mat_id is EMOBON_HCMR-1_Wa_210917_0.2um_2 close matches are \n",
      "\t EMOBON_HCMR-1_Wa_210917_200um_2 EMOBON_HCMR-1_Wa_210917_3um_2 EMOBON_HCMR-1_Wa_210917_200um_4\n",
      "Missing source_mat_id is EMOBON_HCMR-1_Wa_210917_0.2um_blank close matches are \n",
      "\t EMOBON_HCMR-1_Wa_210917_200um_blank2 EMOBON_HCMR-1_Wa_210917_200um_blank1 EMOBON_HCMR-1_Wa_210917_3um_blank2\n",
      "Missing source_mat_id is EMOBON_IUIEilat1_Wa_210829_0.2um_1 close matches are \n",
      "\t EMOBON_IUIEilat1_Wa_210829_200um_1 EMOBON_IUIEilat1_Wa_210829_3um_1 EMOBON_IUIEilat1_Wa_210829_200um_4\n",
      "Missing source_mat_id is EMOBON_IUIEilat1_Wa_210829_0.2um_2 close matches are \n",
      "\t EMOBON_IUIEilat1_Wa_210829_200um_2 EMOBON_IUIEilat1_Wa_210829_3um_2 EMOBON_IUIEilat1_Wa_210829_200um_4\n",
      "Missing source_mat_id is EMOBON_IUIEilat1_Wa_211018_0.2um_1 close matches are \n",
      "\t EMOBON_IUIEilat1_Wa_211018_200um_1 EMOBON_IUIEilat1_Wa_211018_3um_1 EMOBON_IUIEilat1_Wa_211018_200um_4\n",
      "Missing source_mat_id is EMOBON_IUIEilat1_Wa_211018_0.2um_2 close matches are \n",
      "\t EMOBON_IUIEilat1_Wa_211018_200um_2 EMOBON_IUIEilat1_Wa_211018_3um_2 EMOBON_IUIEilat1_Wa_211018_200um_4\n",
      "Missing source_mat_id is EMOBON_IUIEilat1_Wa_211219_0.2um_1 close matches are \n",
      "\t EMOBON_IUIEilat1_Wa_211219_200um_1 EMOBON_IUIEilat1_Wa_211219_3um_1 EMOBON_IUIEilat1_Wa_211219_200um_4\n",
      "Missing source_mat_id is EMOBON_IUIEilat1_Wa_211219_0.2um_2 close matches are \n",
      "\t EMOBON_IUIEilat1_Wa_211219_200um_2 EMOBON_IUIEilat1_Wa_211219_3um_2 EMOBON_IUIEilat1_Wa_211219_200um_4\n",
      "Missing source_mat_id is EMOBON_IUIEilat1_Wa_211219_0.2um_blank close matches are \n",
      "\t EMOBON_IUIEilat1_Wa_211219_200um_blank EMOBON_IUIEilat1_Wa_211219_3um_blank EMOBON_IUIEilat1_Wa_210829_200um_blank\n",
      "Missing source_mat_id is EMOBON_BPNS_Wa_211028_0.2um_1 close matches are \n",
      "\t EMOBON_BPNS_Wa_211028_200um_1 EMOBON_BPNS_Wa_211028_3um_1 EMOBON_BPNS_Wa_211028_NAum_1\n",
      "Missing source_mat_id is EMOBON_BPNS_Wa_211028_0.2um_2 close matches are \n",
      "\t EMOBON_BPNS_Wa_211028_200um_2 EMOBON_BPNS_Wa_211028_3um_2 EMOBON_BPNS_Wa_211028_NAum_2\n",
      "Missing source_mat_id is EMOBON_BPNS_Wa_211028_0.2um_blank close matches are \n",
      "\t EMOBON_BPNS_Wa_211028_200um_blank EMOBON_BPNS_Wa_211028_3um_blank EMOBON_BPNS_Wa_211028_NAum_blank\n",
      "Missing source_mat_id is EMOBON_BPNS_Wa_211223_0.2um_1 close matches are \n",
      "\t EMOBON_BPNS_Wa_211223_200um_1 EMOBON_BPNS_Wa_211223_3um_1 EMOBON_BPNS_Wa_211223_NAum_1\n",
      "Missing source_mat_id is EMOBON_BPNS_Wa_211223_0.2um_2 close matches are \n",
      "\t EMOBON_BPNS_Wa_211223_200um_2 EMOBON_BPNS_Wa_211223_3um_2 EMOBON_BPNS_Wa_211223_NAum_2\n",
      "Missing source_mat_id is EMOBON_EMT21_Wa_211020_0.2um_1 close matches are \n",
      "\t EMOBON_EMT21_Wa_211020_200um_1 EMOBON_EMT21_Wa_211020_3um_1 EMOBON_EMT21_Wa_211020_NAum_1\n",
      "Missing source_mat_id is EMOBON_EMT21_Wa_211020_0.2um_2 close matches are \n",
      "\t EMOBON_EMT21_Wa_211020_200um_2 EMOBON_EMT21_Wa_211020_3um_2 EMOBON_EMT21_Wa_211020_NAum_2\n",
      "Missing source_mat_id is EMOBON_EMT21_Wa_211216_0.2um_1 close matches are \n",
      "\t EMOBON_EMT21_Wa_211216_200um_1 EMOBON_EMT21_Wa_211216_3um_1 EMOBON_EMT21_Wa_211216_NAum_1\n",
      "Missing source_mat_id is EMOBON_EMT21_Wa_211216_0.2um_2 close matches are \n",
      "\t EMOBON_EMT21_Wa_211216_200um_2 EMOBON_EMT21_Wa_211216_3um_2 EMOBON_EMT21_Wa_211216_NAum_2\n",
      "Missing source_mat_id is EMOBON_EMT21_Wa_211216_0.2um_blank close matches are \n",
      "\t EMOBON_EMT21_Wa_211216_200um_blank EMOBON_EMT21_Wa_211216_3um_blank EMOBON_EMT21_Wa_211216_NAum_blank\n",
      "Missing source_mat_id is EMOBON_MBAL4_Wa_211103_0.2um_1 close matches are \n",
      "\t EMOBON_MBAL4_Wa_211103_200um_1 EMOBON_MBAL4_Wa_211103_3um_1 EMOBON_MBAL4_Wa_211103_200um_4\n",
      "Missing source_mat_id is EMOBON_MBAL4_Wa_211103_0.2um_2 close matches are \n",
      "\t EMOBON_MBAL4_Wa_211103_200um_2 EMOBON_MBAL4_Wa_211103_3um_2 EMOBON_MBAL4_Wa_211103_200um_4\n",
      "Missing source_mat_id is EMOBON_MBAL4_Wa_211216_0.2um_1 close matches are \n",
      "\t EMOBON_MBAL4_Wa_211216_200um_1 EMOBON_MBAL4_Wa_211216_3um_1 EMOBON_MBAL4_Wa_211216_200um_4\n",
      "Missing source_mat_id is EMOBON_MBAL4_Wa_211216_0.2um_2 close matches are \n",
      "\t EMOBON_MBAL4_Wa_211216_200um_2 EMOBON_MBAL4_Wa_211216_3um_2 EMOBON_MBAL4_Wa_211216_200um_4\n",
      "Missing source_mat_id is EMOBON_AAOT_Wa_211015_0.2um_1 close matches are \n",
      "\t EMOBON_AAOT_Wa_211015_200um_1 EMOBON_AAOT_Wa_211015_3um_1 EMOBON_AAOT_Wa_211015_2000um_1\n",
      "Missing source_mat_id is EMOBON_AAOT_Wa_211015_0.2um_2 close matches are \n",
      "\t EMOBON_AAOT_Wa_211015_200um_2 EMOBON_AAOT_Wa_211015_3um_2 EMOBON_AAOT_Wa_211015_2000um_2\n",
      "Missing source_mat_id is EMOBON_AAOT_Wa_211214_0.2um_1 close matches are \n",
      "\t EMOBON_AAOT_Wa_211214_200um_1 EMOBON_AAOT_Wa_211214_3um_1 EMOBON_AAOT_Wa_211214_2000um_1\n",
      "Missing source_mat_id is EMOBON_AAOT_Wa_211214_0.2um_2 close matches are \n",
      "\t EMOBON_AAOT_Wa_211214_200um_2 EMOBON_AAOT_Wa_211214_3um_2 EMOBON_AAOT_Wa_211214_2000um_2\n",
      "Missing source_mat_id is EMOBON_AAOT_Wa_211214_0.2um_blank close matches are \n",
      "\t EMOBON_AAOT_Wa_211214_200um_blank EMOBON_AAOT_Wa_211214_3um_blank EMOBON_AAOT_Wa_211214_2000um_blank\n",
      "Missing source_mat_id is EMOBON_VB_Wa_211018_0.2um_1 close matches are \n",
      "\t EMOBON_VB_Wa_211018_3um_1 EMOBON_VB_Wa_211018_NAum_1 EMOBON_VB_Wa_211018_NAum_1\n",
      "Missing source_mat_id is EMOBON_VB_Wa_211018_0.2um_2 close matches are \n",
      "\t EMOBON_VB_Wa_211018_3um_2 EMOBON_VB_Wa_211018_NAum_2 EMOBON_VB_Wa_211018_NAum_2\n",
      "Missing source_mat_id is EMOBON_VB_Wa_211018_0.2um_blank close matches are \n",
      "\t EMOBON_VB_Wa_211018_3um_blank EMOBON_VB_Wa_211018_NAum_blank EMOBON_VB_Wa_211018_NAum_blank\n",
      "Missing source_mat_id is EMOBON_VB_Wa_211217_0.2um_1 close matches are \n",
      "\t EMOBON_VB_Wa_211217_3um_1 EMOBON_VB_Wa_211217_NAum_1 EMOBON_VB_Wa_211217_NAum_1\n",
      "Missing source_mat_id is EMOBON_VB_Wa_211217_0.2um_2 close matches are \n",
      "\t EMOBON_VB_Wa_211217_3um_2 EMOBON_VB_Wa_211217_NAum_2 EMOBON_VB_Wa_211217_NAum_2\n",
      "Missing source_mat_id is EMOBON_ROSKOGO_Wa_211014_0.2um_1 close matches are \n",
      "\t EMOBON_ROSKOGO_Wa_211014_200um_1 EMOBON_ROSKOGO_Wa_211014_3um_1 EMOBON_ROSKOGO_Wa_211014_200um_4\n",
      "Missing source_mat_id is EMOBON_ROSKOGO_Wa_211014_0.2um_2 close matches are \n",
      "\t EMOBON_ROSKOGO_Wa_211014_200um_2 EMOBON_ROSKOGO_Wa_211014_3um_2 EMOBON_ROSKOGO_Wa_211014_200um_4\n",
      "Missing source_mat_id is EMOBON_ROSKOGO_Wa_211014_0.2um_blank close matches are \n",
      "\t EMOBON_ROSKOGO_Wa_211014_um_blank EMOBON_ROSKOGO_Wa_211014_3um_blank EMOBON_ROSKOGO_Wa_230414_um_blank\n",
      "Missing source_mat_id is EMOBON_ROSKOGO_Wa_211213_0.2um_1 close matches are \n",
      "\t EMOBON_ROSKOGO_Wa_211213_200um_1 EMOBON_ROSKOGO_Wa_211213_3um_1 EMOBON_ROSKOGO_Wa_211213_200um_4\n",
      "Missing source_mat_id is EMOBON_ROSKOGO_Wa_211213_0.2um_2 close matches are \n",
      "\t EMOBON_ROSKOGO_Wa_211213_200um_2 EMOBON_ROSKOGO_Wa_211213_3um_2 EMOBON_ROSKOGO_Wa_211213_200um_4\n",
      "Missing source_mat_id is EMOBON_PiEGetxo_Wa_211027_0.2um_1 close matches are \n",
      "\t EMOBON_PiEGetxo_Wa_211027_um_1 EMOBON_PiEGetxo_Wa_211027_um_1 EMOBON_PiEGetxo_Wa_211027_um_1\n",
      "Missing source_mat_id is EMOBON_PiEGetxo_Wa_211027_0.2um_2 close matches are \n",
      "\t EMOBON_PiEGetxo_Wa_211027_um_2 EMOBON_PiEGetxo_Wa_211027_um_2 EMOBON_PiEGetxo_Wa_211027_um_2\n",
      "Missing source_mat_id is EMOBON_PiEGetxo_Wa_211222_0.2um_1 close matches are \n",
      "\t EMOBON_PiEGetxo_Wa_211222_um_1 EMOBON_PiEGetxo_Wa_211222_um_1 EMOBON_PiEGetxo_Wa_211222_200um_1\n",
      "Missing source_mat_id is EMOBON_PiEGetxo_Wa_211222_0.2um_2 close matches are \n",
      "\t EMOBON_PiEGetxo_Wa_211222_um_2 EMOBON_PiEGetxo_Wa_211222_um_2 EMOBON_PiEGetxo_Wa_211222_200um_2\n",
      "Missing source_mat_id is EMOBON_RFormosa_Wa_211022_0.2um_1 close matches are \n",
      "\t EMOBON_RFormosa_Wa_211022_200um_1 EMOBON_RFormosa_Wa_211022_3um_1 EMOBON_RFormosa_Wa_211220_200um_1\n",
      "Missing source_mat_id is EMOBON_RFormosa_Wa_211022_0.2um_2 close matches are \n",
      "\t EMOBON_RFormosa_Wa_211022_200um_2 EMOBON_RFormosa_Wa_211022_3um_2 EMOBON_RFormosa_Wa_211220_200um_2\n",
      "Missing source_mat_id is EMOBON_RFormosa_Wa_211220_0.2um_1 close matches are \n",
      "\t EMOBON_RFormosa_Wa_211220_200um_1 EMOBON_RFormosa_Wa_211220_3um_1 EMOBON_RFormosa_Wa_211220_200um_4\n",
      "Missing source_mat_id is EMOBON_RFormosa_Wa_211220_0.2um_2 close matches are \n",
      "\t EMOBON_RFormosa_Wa_211220_200um_2 EMOBON_RFormosa_Wa_211220_3um_2 EMOBON_RFormosa_Wa_211220_200um_4\n",
      "Missing source_mat_id is EMOBON_RFormosa_Wa_211220_0.2um_blank close matches are \n",
      "\t EMOBON_RFormosa_Wa_211220_200um_blank EMOBON_RFormosa_Wa_211220_3um_blank EMOBON_RFormosa_Wa_211022_200um_blank\n",
      "Missing source_mat_id is EMOBON_NRMCB_Wa_211026_0.2um_1 close matches are \n",
      "\t EMOBON_NRMCB_Wa_211026_200um_1 EMOBON_NRMCB_Wa_211026_3um_1 EMOBON_NRMCB_Wa_211026_NAum_1\n",
      "Missing source_mat_id is EMOBON_NRMCB_Wa_211026_0.2um_2 close matches are \n",
      "\t EMOBON_NRMCB_Wa_211026_200um_2 EMOBON_NRMCB_Wa_211026_3um_2 EMOBON_NRMCB_Wa_211026_NAum_2\n",
      "Missing source_mat_id is EMOBON_NRMCB_Wa_211221_0.2um_1 close matches are \n",
      "\t EMOBON_NRMCB_Wa_211221_200um_1 EMOBON_NRMCB_Wa_211221_3um_1 EMOBON_NRMCB_Wa_211221_NAum_1\n",
      "Missing source_mat_id is EMOBON_NRMCB_Wa_211221_0.2um_2 close matches are \n",
      "\t EMOBON_NRMCB_Wa_211221_200um_2 EMOBON_NRMCB_Wa_211221_3um_2 EMOBON_NRMCB_Wa_211221_NAum_2\n",
      "Missing source_mat_id is EMOBON_OSD74_Wa_211028_0.2um_1 close matches are \n",
      "\t EMOBON_OSD74_Wa_211028_200um_1 EMOBON_OSD74_Wa_211028_3um_1 EMOBON_OSD74_Wa_211218_200um_1\n",
      "Missing source_mat_id is EMOBON_OSD74_Wa_211028_0.2um_2 close matches are \n",
      "\t EMOBON_OSD74_Wa_211028_200um_2 EMOBON_OSD74_Wa_211028_3um_2 EMOBON_OSD74_Wa_211218_200um_2\n",
      "Missing source_mat_id is EMOBON_OSD74_Wa_211218_0.2um_1 close matches are \n",
      "\t EMOBON_OSD74_Wa_211218_200um_1 EMOBON_OSD74_Wa_211218_3um_1 EMOBON_OSD74_Wa_211218_200um_4\n",
      "Missing source_mat_id is EMOBON_OSD74_Wa_211218_0.2um_2 close matches are \n",
      "\t EMOBON_OSD74_Wa_211218_200um_2 EMOBON_OSD74_Wa_211218_3um_2 EMOBON_OSD74_Wa_211218_200um_4\n",
      "Missing source_mat_id is EMOBON_OSD74_Wa_211218_0.2um_blank close matches are \n",
      "\t EMOBON_OSD74_Wa_211218_200um_blank EMOBON_OSD74_Wa_211218_3um_blank EMOBON_OSD74_Wa_211028_200um_blank\n",
      "['EMOBON_BPNS_Wa_210701_0.2um_2', ['EMOBON_BPNS_Wa_210701_200um_2', 'EMOBON_BPNS_Wa_210701_3um_2', 'EMOBON_BPNS_Wa_210701_NAum_2']]\n",
      "['EMOBON_BPNS_Wa_210701_0.2um_1', ['EMOBON_BPNS_Wa_210701_200um_1', 'EMOBON_BPNS_Wa_210701_3um_1', 'EMOBON_BPNS_Wa_210701_NAum_1']]\n",
      "['EMOBON_BPNS_Wa_210825_0.2um_1', ['EMOBON_BPNS_Wa_210825_200um_1', 'EMOBON_BPNS_Wa_210825_3um_1', 'EMOBON_BPNS_Wa_210825_NAum_1']]\n",
      "['EMOBON_BPNS_Wa_210825_0.2um_2', ['EMOBON_BPNS_Wa_210825_200um_2', 'EMOBON_BPNS_Wa_210825_3um_2', 'EMOBON_BPNS_Wa_210825_NAum_2']]\n",
      "['EMOBON_ROSKOGO_Wa_210618_0.2um_1', ['EMOBON_ROSKOGO_Wa_210618_200um_1', 'EMOBON_ROSKOGO_Wa_210618_200um_1', 'EMOBON_ROSKOGO_Wa_210618_3um_1']]\n",
      "['EMOBON_ROSKOGO_Wa_210618_0.2um_2', ['EMOBON_ROSKOGO_Wa_210618_200um_2', 'EMOBON_ROSKOGO_Wa_210618_3um_2', 'EMOBON_ROSKOGO_Wa_210618_200um_4']]\n",
      "['EMOBON_ROSKOGO_Wa_210802_0.2um_1', ['EMOBON_ROSKOGO_Wa_210802_200um_1', 'EMOBON_ROSKOGO_Wa_210802_3um_1', 'EMOBON_ROSKOGO_Wa_210802_200um_4']]\n",
      "['EMOBON_ROSKOGO_Wa_210802_0.2um_2', ['EMOBON_ROSKOGO_Wa_210802_200um_2', 'EMOBON_ROSKOGO_Wa_210802_3um_2', 'EMOBON_ROSKOGO_Wa_210802_200um_4']]\n",
      "['EMOBON_VB_Wa_210621_0.2um_1', ['EMOBON_VB_Wa_210621_3um_1', 'EMOBON_VB_Wa_210621_NAum_1', 'EMOBON_VB_Wa_210621_NAum_1']]\n",
      "['EMOBON_VB_Wa_210621_0.2um_2', ['EMOBON_VB_Wa_210621_3um_2', 'EMOBON_VB_Wa_210621_NAum_2', 'EMOBON_VB_Wa_210621_NAum_2']]\n",
      "['EMOBON_VB_Wa_210823_0.2um_1', ['EMOBON_VB_Wa_210823_3um_1', 'EMOBON_VB_Wa_210823_NAum_1', 'EMOBON_VB_Wa_210823_NAum_1']]\n",
      "['EMOBON_VB_Wa_210823_0.2um_2', ['EMOBON_VB_Wa_210823_3um_2', 'EMOBON_VB_Wa_210823_NAum_2', 'EMOBON_VB_Wa_210823_NAum_2']]\n",
      "['EMOBON_EMT21_Wa_210825_0.2um_1', ['EMOBON_EMT21_Wa_210825_200um_1', 'EMOBON_EMT21_Wa_210825_3um_1', 'EMOBON_EMT21_Wa_210825_NAum_1']]\n",
      "['EMOBON_EMT21_Wa_210825_0.2um_2', ['EMOBON_EMT21_Wa_210825_200um_2', 'EMOBON_EMT21_Wa_210825_3um_2', 'EMOBON_EMT21_Wa_210825_NAum_2']]\n",
      "['EMOBON_PiEGetxo_Wa_210824_0.2um_1', ['EMOBON_PiEGetxo_Wa_210824_200um_1', 'EMOBON_PiEGetxo_Wa_210824_3um_1', 'EMOBON_PiEGetxo_Wa_221024_200um_1']]\n",
      "['EMOBON_PiEGetxo_Wa_210824_0.2um_2', ['EMOBON_PiEGetxo_Wa_210824_200um_2', 'EMOBON_PiEGetxo_Wa_210824_3um_2', 'EMOBON_PiEGetxo_Wa_221024_200um_2']]\n",
      "['EMOBON_PiEGetxo_Wa_210824_0.2um_blank', ['EMOBON_PiEGetxo_Wa_210824_200um_blank', 'EMOBON_PiEGetxo_Wa_210824_200um_blank', 'EMOBON_PiEGetxo_Wa_210824_3um_blank']]\n",
      "['EMOBON_RFormosa_Wa_210805_0.2um_1', ['EMOBON_RFormosa_Wa_210805_200um_1', 'EMOBON_RFormosa_Wa_210805_3um_1', 'EMOBON_RFormosa_Wa_210805_200um_4']]\n",
      "['EMOBON_RFormosa_Wa_210805_0.2um_2', ['EMOBON_RFormosa_Wa_210805_200um_2', 'EMOBON_RFormosa_Wa_210805_3um_2', 'EMOBON_RFormosa_Wa_210805_200um_4']]\n",
      "['EMOBON_OSD74_Wa_210831_0.2um_1', ['EMOBON_OSD74_Wa_210831_200um_1', 'EMOBON_OSD74_Wa_210831_3um_1', 'EMOBON_OSD74_Wa_210831_200um_4']]\n",
      "['EMOBON_OSD74_Wa_210831_0.2um_2', ['EMOBON_OSD74_Wa_210831_200um_2', 'EMOBON_OSD74_Wa_210831_3um_2', 'EMOBON_OSD74_Wa_210831_200um_4']]\n",
      "['EMOBON_AAOT_Wa_210622_0.2um_1', ['EMOBON_AAOT_Wa_210622_200um_1', 'EMOBON_AAOT_Wa_210622_3um_1', 'EMOBON_AAOT_Wa_210622_2000um_1']]\n",
      "['EMOBON_AAOT_Wa_210622_0.2um_2', ['EMOBON_AAOT_Wa_210622_200um_2', 'EMOBON_AAOT_Wa_210622_3um_2', 'EMOBON_AAOT_Wa_210622_2000um_2']]\n",
      "['EMOBON_AAOT_Wa_210809_0.2um_1', ['EMOBON_AAOT_Wa_210809_200um_1', 'EMOBON_AAOT_Wa_210809_3um_1', 'EMOBON_AAOT_Wa_210809_2000um_1']]\n",
      "['EMOBON_AAOT_Wa_210809_0.2um_2', ['EMOBON_AAOT_Wa_210809_200um_2', 'EMOBON_AAOT_Wa_210809_3um_2', 'EMOBON_AAOT_Wa_210809_2000um_2']]\n",
      "['EMOBON_NRMCB_Wa_210621_0.2um_1', ['EMOBON_NRMCB_Wa_210621_200um_1', 'EMOBON_NRMCB_Wa_210621_3um_1', 'EMOBON_NRMCB_Wa_210621_NAum_1']]\n",
      "['EMOBON_NRMCB_Wa_210621_0.2um_2', ['EMOBON_NRMCB_Wa_210621_200um_2', 'EMOBON_NRMCB_Wa_210621_3um_2', 'EMOBON_NRMCB_Wa_210621_NAum_2']]\n",
      "['EMOBON_NRMCB_Wa_210831_0.2um_1', ['EMOBON_NRMCB_Wa_210831_3um_1', 'EMOBON_NRMCB_Wa_210831_NAum_1', 'EMOBON_NRMCB_Wa_210831_NAum_1']]\n",
      "['EMOBON_NRMCB_Wa_210831_0.2um_2', ['EMOBON_NRMCB_Wa_210831_3um_2', 'EMOBON_NRMCB_Wa_210831_NAum_2', 'EMOBON_NRMCB_Wa_210831_NAum_2']]\n",
      "['EMOBON_NRMCB_Wa_210831_0.2um_blank', ['EMOBON_NRMCB_Wa_210831_3um_blank', 'EMOBON_NRMCB_Wa_210831_NAum_blank', 'EMOBON_NRMCB_Wa_210831_NAum_blank']]\n",
      "['EMOBON_HCMR-1_Wa_210628_0.2um_1', ['EMOBON_HCMR-1_Wa_210628_200um_1', 'EMOBON_HCMR-1_Wa_210628_3um_1', 'EMOBON_HCMR-1_Wa_210628_200um_4']]\n",
      "['EMOBON_HCMR-1_Wa_210628_0.2um_2', ['EMOBON_HCMR-1_Wa_210628_200um_2', 'EMOBON_HCMR-1_Wa_210628_3um_2', 'EMOBON_HCMR-1_Wa_210628_200um_4']]\n",
      "['EMOBON_HCMR-1_Wa_210917_3um_blank', ['EMOBON_HCMR-1_Wa_210917_3um_blank2', 'EMOBON_HCMR-1_Wa_210917_3um_blank1', 'EMOBON_HCMR-1_Wa_210917_200um_blank2']]\n",
      "['EMOBON_HCMR-1_Wa_210917_0.2um_1', ['EMOBON_HCMR-1_Wa_210917_200um_1', 'EMOBON_HCMR-1_Wa_210917_3um_1', 'EMOBON_HCMR-1_Wa_210917_200um_4']]\n",
      "['EMOBON_HCMR-1_Wa_210917_0.2um_2', ['EMOBON_HCMR-1_Wa_210917_200um_2', 'EMOBON_HCMR-1_Wa_210917_3um_2', 'EMOBON_HCMR-1_Wa_210917_200um_4']]\n",
      "['EMOBON_HCMR-1_Wa_210917_0.2um_blank', ['EMOBON_HCMR-1_Wa_210917_200um_blank2', 'EMOBON_HCMR-1_Wa_210917_200um_blank1', 'EMOBON_HCMR-1_Wa_210917_3um_blank2']]\n",
      "['EMOBON_IUIEilat1_Wa_210829_0.2um_1', ['EMOBON_IUIEilat1_Wa_210829_200um_1', 'EMOBON_IUIEilat1_Wa_210829_3um_1', 'EMOBON_IUIEilat1_Wa_210829_200um_4']]\n",
      "['EMOBON_IUIEilat1_Wa_210829_0.2um_2', ['EMOBON_IUIEilat1_Wa_210829_200um_2', 'EMOBON_IUIEilat1_Wa_210829_3um_2', 'EMOBON_IUIEilat1_Wa_210829_200um_4']]\n",
      "['EMOBON_IUIEilat1_Wa_211018_0.2um_1', ['EMOBON_IUIEilat1_Wa_211018_200um_1', 'EMOBON_IUIEilat1_Wa_211018_3um_1', 'EMOBON_IUIEilat1_Wa_211018_200um_4']]\n",
      "['EMOBON_IUIEilat1_Wa_211018_0.2um_2', ['EMOBON_IUIEilat1_Wa_211018_200um_2', 'EMOBON_IUIEilat1_Wa_211018_3um_2', 'EMOBON_IUIEilat1_Wa_211018_200um_4']]\n",
      "['EMOBON_IUIEilat1_Wa_211219_0.2um_1', ['EMOBON_IUIEilat1_Wa_211219_200um_1', 'EMOBON_IUIEilat1_Wa_211219_3um_1', 'EMOBON_IUIEilat1_Wa_211219_200um_4']]\n",
      "['EMOBON_IUIEilat1_Wa_211219_0.2um_2', ['EMOBON_IUIEilat1_Wa_211219_200um_2', 'EMOBON_IUIEilat1_Wa_211219_3um_2', 'EMOBON_IUIEilat1_Wa_211219_200um_4']]\n",
      "['EMOBON_IUIEilat1_Wa_211219_0.2um_blank', ['EMOBON_IUIEilat1_Wa_211219_200um_blank', 'EMOBON_IUIEilat1_Wa_211219_3um_blank', 'EMOBON_IUIEilat1_Wa_210829_200um_blank']]\n",
      "['EMOBON_BPNS_Wa_211028_0.2um_1', ['EMOBON_BPNS_Wa_211028_200um_1', 'EMOBON_BPNS_Wa_211028_3um_1', 'EMOBON_BPNS_Wa_211028_NAum_1']]\n",
      "['EMOBON_BPNS_Wa_211028_0.2um_2', ['EMOBON_BPNS_Wa_211028_200um_2', 'EMOBON_BPNS_Wa_211028_3um_2', 'EMOBON_BPNS_Wa_211028_NAum_2']]\n",
      "['EMOBON_BPNS_Wa_211028_0.2um_blank', ['EMOBON_BPNS_Wa_211028_200um_blank', 'EMOBON_BPNS_Wa_211028_3um_blank', 'EMOBON_BPNS_Wa_211028_NAum_blank']]\n",
      "['EMOBON_BPNS_Wa_211223_0.2um_1', ['EMOBON_BPNS_Wa_211223_200um_1', 'EMOBON_BPNS_Wa_211223_3um_1', 'EMOBON_BPNS_Wa_211223_NAum_1']]\n",
      "['EMOBON_BPNS_Wa_211223_0.2um_2', ['EMOBON_BPNS_Wa_211223_200um_2', 'EMOBON_BPNS_Wa_211223_3um_2', 'EMOBON_BPNS_Wa_211223_NAum_2']]\n",
      "['EMOBON_EMT21_Wa_211020_0.2um_1', ['EMOBON_EMT21_Wa_211020_200um_1', 'EMOBON_EMT21_Wa_211020_3um_1', 'EMOBON_EMT21_Wa_211020_NAum_1']]\n",
      "['EMOBON_EMT21_Wa_211020_0.2um_2', ['EMOBON_EMT21_Wa_211020_200um_2', 'EMOBON_EMT21_Wa_211020_3um_2', 'EMOBON_EMT21_Wa_211020_NAum_2']]\n",
      "['EMOBON_EMT21_Wa_211216_0.2um_1', ['EMOBON_EMT21_Wa_211216_200um_1', 'EMOBON_EMT21_Wa_211216_3um_1', 'EMOBON_EMT21_Wa_211216_NAum_1']]\n",
      "['EMOBON_EMT21_Wa_211216_0.2um_2', ['EMOBON_EMT21_Wa_211216_200um_2', 'EMOBON_EMT21_Wa_211216_3um_2', 'EMOBON_EMT21_Wa_211216_NAum_2']]\n",
      "['EMOBON_EMT21_Wa_211216_0.2um_blank', ['EMOBON_EMT21_Wa_211216_200um_blank', 'EMOBON_EMT21_Wa_211216_3um_blank', 'EMOBON_EMT21_Wa_211216_NAum_blank']]\n",
      "['EMOBON_MBAL4_Wa_211103_0.2um_1', ['EMOBON_MBAL4_Wa_211103_200um_1', 'EMOBON_MBAL4_Wa_211103_3um_1', 'EMOBON_MBAL4_Wa_211103_200um_4']]\n",
      "['EMOBON_MBAL4_Wa_211103_0.2um_2', ['EMOBON_MBAL4_Wa_211103_200um_2', 'EMOBON_MBAL4_Wa_211103_3um_2', 'EMOBON_MBAL4_Wa_211103_200um_4']]\n",
      "['EMOBON_MBAL4_Wa_211216_0.2um_1', ['EMOBON_MBAL4_Wa_211216_200um_1', 'EMOBON_MBAL4_Wa_211216_3um_1', 'EMOBON_MBAL4_Wa_211216_200um_4']]\n",
      "['EMOBON_MBAL4_Wa_211216_0.2um_2', ['EMOBON_MBAL4_Wa_211216_200um_2', 'EMOBON_MBAL4_Wa_211216_3um_2', 'EMOBON_MBAL4_Wa_211216_200um_4']]\n",
      "['EMOBON_AAOT_Wa_211015_0.2um_1', ['EMOBON_AAOT_Wa_211015_200um_1', 'EMOBON_AAOT_Wa_211015_3um_1', 'EMOBON_AAOT_Wa_211015_2000um_1']]\n",
      "['EMOBON_AAOT_Wa_211015_0.2um_2', ['EMOBON_AAOT_Wa_211015_200um_2', 'EMOBON_AAOT_Wa_211015_3um_2', 'EMOBON_AAOT_Wa_211015_2000um_2']]\n",
      "['EMOBON_AAOT_Wa_211214_0.2um_1', ['EMOBON_AAOT_Wa_211214_200um_1', 'EMOBON_AAOT_Wa_211214_3um_1', 'EMOBON_AAOT_Wa_211214_2000um_1']]\n",
      "['EMOBON_AAOT_Wa_211214_0.2um_2', ['EMOBON_AAOT_Wa_211214_200um_2', 'EMOBON_AAOT_Wa_211214_3um_2', 'EMOBON_AAOT_Wa_211214_2000um_2']]\n",
      "['EMOBON_AAOT_Wa_211214_0.2um_blank', ['EMOBON_AAOT_Wa_211214_200um_blank', 'EMOBON_AAOT_Wa_211214_3um_blank', 'EMOBON_AAOT_Wa_211214_2000um_blank']]\n",
      "['EMOBON_VB_Wa_211018_0.2um_1', ['EMOBON_VB_Wa_211018_3um_1', 'EMOBON_VB_Wa_211018_NAum_1', 'EMOBON_VB_Wa_211018_NAum_1']]\n",
      "['EMOBON_VB_Wa_211018_0.2um_2', ['EMOBON_VB_Wa_211018_3um_2', 'EMOBON_VB_Wa_211018_NAum_2', 'EMOBON_VB_Wa_211018_NAum_2']]\n",
      "['EMOBON_VB_Wa_211018_0.2um_blank', ['EMOBON_VB_Wa_211018_3um_blank', 'EMOBON_VB_Wa_211018_NAum_blank', 'EMOBON_VB_Wa_211018_NAum_blank']]\n",
      "['EMOBON_VB_Wa_211217_0.2um_1', ['EMOBON_VB_Wa_211217_3um_1', 'EMOBON_VB_Wa_211217_NAum_1', 'EMOBON_VB_Wa_211217_NAum_1']]\n",
      "['EMOBON_VB_Wa_211217_0.2um_2', ['EMOBON_VB_Wa_211217_3um_2', 'EMOBON_VB_Wa_211217_NAum_2', 'EMOBON_VB_Wa_211217_NAum_2']]\n",
      "['EMOBON_ROSKOGO_Wa_211014_0.2um_1', ['EMOBON_ROSKOGO_Wa_211014_200um_1', 'EMOBON_ROSKOGO_Wa_211014_3um_1', 'EMOBON_ROSKOGO_Wa_211014_200um_4']]\n",
      "['EMOBON_ROSKOGO_Wa_211014_0.2um_2', ['EMOBON_ROSKOGO_Wa_211014_200um_2', 'EMOBON_ROSKOGO_Wa_211014_3um_2', 'EMOBON_ROSKOGO_Wa_211014_200um_4']]\n",
      "['EMOBON_ROSKOGO_Wa_211014_0.2um_blank', ['EMOBON_ROSKOGO_Wa_211014_um_blank', 'EMOBON_ROSKOGO_Wa_211014_3um_blank', 'EMOBON_ROSKOGO_Wa_230414_um_blank']]\n",
      "['EMOBON_ROSKOGO_Wa_211213_0.2um_1', ['EMOBON_ROSKOGO_Wa_211213_200um_1', 'EMOBON_ROSKOGO_Wa_211213_3um_1', 'EMOBON_ROSKOGO_Wa_211213_200um_4']]\n",
      "['EMOBON_ROSKOGO_Wa_211213_0.2um_2', ['EMOBON_ROSKOGO_Wa_211213_200um_2', 'EMOBON_ROSKOGO_Wa_211213_3um_2', 'EMOBON_ROSKOGO_Wa_211213_200um_4']]\n",
      "['EMOBON_PiEGetxo_Wa_211027_0.2um_1', ['EMOBON_PiEGetxo_Wa_211027_um_1', 'EMOBON_PiEGetxo_Wa_211027_um_1', 'EMOBON_PiEGetxo_Wa_211027_um_1']]\n",
      "['EMOBON_PiEGetxo_Wa_211027_0.2um_2', ['EMOBON_PiEGetxo_Wa_211027_um_2', 'EMOBON_PiEGetxo_Wa_211027_um_2', 'EMOBON_PiEGetxo_Wa_211027_um_2']]\n",
      "['EMOBON_PiEGetxo_Wa_211222_0.2um_1', ['EMOBON_PiEGetxo_Wa_211222_um_1', 'EMOBON_PiEGetxo_Wa_211222_um_1', 'EMOBON_PiEGetxo_Wa_211222_200um_1']]\n",
      "['EMOBON_PiEGetxo_Wa_211222_0.2um_2', ['EMOBON_PiEGetxo_Wa_211222_um_2', 'EMOBON_PiEGetxo_Wa_211222_um_2', 'EMOBON_PiEGetxo_Wa_211222_200um_2']]\n",
      "['EMOBON_RFormosa_Wa_211022_0.2um_1', ['EMOBON_RFormosa_Wa_211022_200um_1', 'EMOBON_RFormosa_Wa_211022_3um_1', 'EMOBON_RFormosa_Wa_211220_200um_1']]\n",
      "['EMOBON_RFormosa_Wa_211022_0.2um_2', ['EMOBON_RFormosa_Wa_211022_200um_2', 'EMOBON_RFormosa_Wa_211022_3um_2', 'EMOBON_RFormosa_Wa_211220_200um_2']]\n",
      "['EMOBON_RFormosa_Wa_211220_0.2um_1', ['EMOBON_RFormosa_Wa_211220_200um_1', 'EMOBON_RFormosa_Wa_211220_3um_1', 'EMOBON_RFormosa_Wa_211220_200um_4']]\n",
      "['EMOBON_RFormosa_Wa_211220_0.2um_2', ['EMOBON_RFormosa_Wa_211220_200um_2', 'EMOBON_RFormosa_Wa_211220_3um_2', 'EMOBON_RFormosa_Wa_211220_200um_4']]\n",
      "['EMOBON_RFormosa_Wa_211220_0.2um_blank', ['EMOBON_RFormosa_Wa_211220_200um_blank', 'EMOBON_RFormosa_Wa_211220_3um_blank', 'EMOBON_RFormosa_Wa_211022_200um_blank']]\n",
      "['EMOBON_NRMCB_Wa_211026_0.2um_1', ['EMOBON_NRMCB_Wa_211026_200um_1', 'EMOBON_NRMCB_Wa_211026_3um_1', 'EMOBON_NRMCB_Wa_211026_NAum_1']]\n",
      "['EMOBON_NRMCB_Wa_211026_0.2um_2', ['EMOBON_NRMCB_Wa_211026_200um_2', 'EMOBON_NRMCB_Wa_211026_3um_2', 'EMOBON_NRMCB_Wa_211026_NAum_2']]\n",
      "['EMOBON_NRMCB_Wa_211221_0.2um_1', ['EMOBON_NRMCB_Wa_211221_200um_1', 'EMOBON_NRMCB_Wa_211221_3um_1', 'EMOBON_NRMCB_Wa_211221_NAum_1']]\n",
      "['EMOBON_NRMCB_Wa_211221_0.2um_2', ['EMOBON_NRMCB_Wa_211221_200um_2', 'EMOBON_NRMCB_Wa_211221_3um_2', 'EMOBON_NRMCB_Wa_211221_NAum_2']]\n",
      "['EMOBON_OSD74_Wa_211028_0.2um_1', ['EMOBON_OSD74_Wa_211028_200um_1', 'EMOBON_OSD74_Wa_211028_3um_1', 'EMOBON_OSD74_Wa_211218_200um_1']]\n",
      "['EMOBON_OSD74_Wa_211028_0.2um_2', ['EMOBON_OSD74_Wa_211028_200um_2', 'EMOBON_OSD74_Wa_211028_3um_2', 'EMOBON_OSD74_Wa_211218_200um_2']]\n",
      "['EMOBON_OSD74_Wa_211218_0.2um_1', ['EMOBON_OSD74_Wa_211218_200um_1', 'EMOBON_OSD74_Wa_211218_3um_1', 'EMOBON_OSD74_Wa_211218_200um_4']]\n",
      "['EMOBON_OSD74_Wa_211218_0.2um_2', ['EMOBON_OSD74_Wa_211218_200um_2', 'EMOBON_OSD74_Wa_211218_3um_2', 'EMOBON_OSD74_Wa_211218_200um_4']]\n",
      "['EMOBON_OSD74_Wa_211218_0.2um_blank', ['EMOBON_OSD74_Wa_211218_200um_blank', 'EMOBON_OSD74_Wa_211218_3um_blank', 'EMOBON_OSD74_Wa_211028_200um_blank']]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import difflib\n",
    "import pandas as pd\n",
    "import validators\n",
    "from pprint import pprint\n",
    "\n",
    "######################## CAUTION #######################\n",
    "# It seems the source_mat_ids in the run-sheet do not match the source_mat_ids in the sampling sheets: \n",
    "# CHECK\n",
    "# EMOBON_OOB_So_210608_micro_1\n",
    "# How many source_mat_ids in the run-information-batch-001.csv sheets have not equivalents in the sampling sheets?\n",
    "######################## CAUTION #######################\n",
    "\n",
    "LOGSHEETS_PREFIX = \"./logsheets\"\n",
    "\n",
    "def get_observatory_data() -> list[str, str, str]:\n",
    "    # Get list of observatory_ids\n",
    "    df = pd.read_csv(\"./governance/logsheets_validated.csv\")\n",
    "    observatory_data = df[[\"observatory_id\", \"water_column\", \"soft_sediment\"]].values.tolist()\n",
    "    return observatory_data\n",
    "\n",
    "def get_refcodes() -> dict[str, str]:\n",
    "    batch1_run_info_path = \"https://raw.githubusercontent.com/emo-bon/sequencing-data/main/shipment/batch-001/run-information-batch-001.csv\"\n",
    "    batch2_run_info_path = \"https://raw.githubusercontent.com/emo-bon/sequencing-data/main/shipment/batch-002/run-information-batch-002.csv\"\n",
    "    # Get list of batch1 <source_mat_id>, <ref_code>'s\n",
    "    df = pd.read_csv(batch1_run_info_path)\n",
    "    refcodes = {}\n",
    "    for i in df[[\"source_material_id\", \"ref_code\"]].values.tolist():\n",
    "        assert i[0] not in refcodes, f\"{source_material_id} maybe duplicated\"\n",
    "        refcodes.update(dict([i]))\n",
    "    # Get list of batch2 <source_mat_id>, <ref_code>'s\n",
    "    df = pd.read_csv(batch2_run_info_path)\n",
    "    b2_refcodes = {}\n",
    "    for i in df[[\"source_material_id\", \"ref_code\"]].values.tolist():\n",
    "        assert i[0] not in refcodes, f\"{source_material_id} maybe duplicated\"\n",
    "        b2_refcodes.update(dict([i]))\n",
    "    refcodes.update(b2_refcodes)\n",
    "    return refcodes\n",
    "\n",
    "def parse_observatory_sample_type(observatory_id: str, sampling_type: str, save_table: bool = False) -> list[dict[str, str]]:\n",
    "        \"\"\"An observatory is an EMBRC station and it has an ID\n",
    "           Each observatory may take either or both of the \"water_column\" and \"soft_sediment\" sampling types\n",
    "           Each sampling type has both a \"sampling\" and \"measured\" sheet\n",
    "\n",
    "           This function returns a list of sampling events each of which is a dict with key/value pairs for each field and value\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        sampling_data_filename = f\"{observatory_id}_{sampling_type}_sampling_validated.csv\"\n",
    "        measured_data_filename = f\"{observatory_id}_{sampling_type}_measured_validated.csv\"\n",
    "    \n",
    "        sampling_data = pd.read_csv(os.path.join(LOGSHEETS_PREFIX, sampling_data_filename))\n",
    "        measured_data = pd.read_csv(os.path.join(LOGSHEETS_PREFIX, measured_data_filename))\n",
    "\n",
    "        # Pull out the source_mat_ids:\n",
    "        all_sampling_source_mat_ids = sampling_data[\"source_mat_id\"].values.tolist()\n",
    "        #print(f\"source_mat_ids in {observatory_id}-{sampling_type}: {all_sampling_source_mat_ids}\")\n",
    "    \n",
    "        sampling_events = sampling_data.to_dict(orient=\"records\")\n",
    "        measured_events = measured_data.to_dict(orient=\"records\")\n",
    "    \n",
    "        combined_events = []\n",
    "        source_mat_ids_from_combined_events = []\n",
    "        no_refcode_counter = 0\n",
    "        missing_refcode_counter = 0\n",
    "        missing_measured_but_refcode_present = 0\n",
    "        for sampling_event in sampling_events:\n",
    "\n",
    "            # Checking consistency\n",
    "            # Does this sampling event have a ref_code\n",
    "            # If yes, then it should have both a sampling and measured sheet\n",
    "            # If no, we can ignore it\n",
    "            event_mat_id = sampling_event[\"source_mat_id\"]\n",
    "            try:\n",
    "                refcode = obs_refcodes[event_mat_id]\n",
    "            except KeyError:\n",
    "                no_refcode_counter += 1\n",
    "                # OK so has not been sent to sequencing; ignore\n",
    "                continue\n",
    "            \n",
    "            event_measured = False\n",
    "            for measured_event in measured_events:\n",
    "                try:\n",
    "                    measured_event[\"source_mat_id\"]\n",
    "                except KeyError:\n",
    "                    print(f\"Key error: {measured_event}\")\n",
    "                    # Should not happen\n",
    "                if measured_event[\"source_mat_id\"] == event_mat_id:\n",
    "                    event_measured = copy.deepcopy(measured_event)\n",
    "                    break\n",
    "                    \n",
    "            if not event_measured:\n",
    "                missing_measured_but_refcode_present += 1\n",
    "                continue\n",
    "            else:\n",
    "                sampling_event[\"ref_code\"] = refcode\n",
    "                # Delete the now duplicated source_mat_id\n",
    "                del event_measured[\"source_mat_id\"]\n",
    "                source_mat_ids_from_combined_events.append(event_mat_id)\n",
    "                sampling_event.update(event_measured)\n",
    "                combined_events.append(sampling_event)\n",
    "    \n",
    "        print(\n",
    "              f\"Observatory {observatory_id}-{sampling_type} has {len(sampling_events)} sampling events, \"\n",
    "              f\"of which {no_refcode_counter} have missing no ref_code (not sent for sequencing), \"\n",
    "              f\"and {missing_measured_but_refcode_present} have a refcode but not 'measured' data. \"\n",
    "              f\"A total of {len(combined_events)} sampling events with refcode and measured sheet were found.\\n\"\n",
    "             )\n",
    "    \n",
    "        # Did we find all the sampling events?\n",
    "        assert len(sampling_events) == (len(combined_events) + no_refcode_counter), \\\n",
    "            \"Something is a foot: len(sampling_events) != (len(combined_events) + no_refcode_counter)\"\n",
    "\n",
    "\n",
    "        if save_table:\n",
    "            save_dir = \"./transformed\"\n",
    "            outfile_name = f\"{observatory_id}_{sampling_type}_combined_validated.csv\"\n",
    "            ndf = pd.DataFrame.from_records(combined_events, index=\"source_mat_id\")\n",
    "            ndf.to_csv(os.path.join(save_dir, outfile_name))\n",
    "\n",
    "        return source_mat_ids_from_combined_events, \\\n",
    "               all_sampling_source_mat_ids, \\\n",
    "               missing_measured_but_refcode_present, \\\n",
    "               combined_events \n",
    "\n",
    "observatory_data = get_observatory_data()\n",
    "#pprint(observatory_data)\n",
    "obs_refcodes = get_refcodes()\n",
    "\n",
    "total_combined_events = 0\n",
    "total_missing_measured_but_refcode_present = 0\n",
    "all_source_mat_ids_from_sheets = []\n",
    "source_mat_ids_from_combined_events = []\n",
    "for observatory_id, water_column, soft_sediment in observatory_data:\n",
    "\n",
    "    if observatory_id == \"Plenzia\": continue # Data not public\n",
    "    if observatory_id == \"UMF\" and soft_sediment: continue # Broken sheet\n",
    "        \n",
    "    if water_column and validators.url(water_column):\n",
    "        \n",
    "        r = parse_observatory_sample_type(observatory_id, \"water_column\", save_table = False)\n",
    "        wc_source_mat_ids_from_combined_events = r[0]\n",
    "        wc_all_sampling_source_mat_ids = r[1]\n",
    "        wc_missing_measured_but_refcode_present = r[2]\n",
    "        wc_combined_events = r[3]\n",
    "        #pprint(wc_source_mat_ids_from_combined_events)\n",
    "        \n",
    "        total_combined_events += len(wc_combined_events)\n",
    "        total_missing_measured_but_refcode_present += wc_missing_measured_but_refcode_present\n",
    "        all_source_mat_ids_from_sheets.extend(wc_all_sampling_source_mat_ids)\n",
    "        source_mat_ids_from_combined_events.extend(wc_source_mat_ids_from_combined_events)\n",
    "        \n",
    "        \n",
    "    if soft_sediment and validators.url(soft_sediment):\n",
    "        r = parse_observatory_sample_type(observatory_id, \"soft_sediment\", save_table = False)\n",
    "        ss_source_mat_ids_from_combined_events= r[0]\n",
    "        ss_all_sampling_source_mat_ids = r[1]\n",
    "        ss_missing_measured_but_refcode_present = r[2]\n",
    "        ss_combined_events = r[3]\n",
    "    \n",
    "        total_combined_events += len(ss_combined_events)\n",
    "        total_missing_measured_but_refcode_present += ss_missing_measured_but_refcode_present\n",
    "        all_source_mat_ids_from_sheets.extend(ss_all_sampling_source_mat_ids)\n",
    "        source_mat_ids_from_combined_events.extend(ss_source_mat_ids_from_combined_events)\n",
    "\n",
    "pprint(f\"There are {len(obs_refcodes)} total ref_codes assigned\")\n",
    "pprint(f\"Total number of combined sampling events with ref_codes: {total_combined_events}\")\n",
    "# Did we find all the ref_codes?\n",
    "#assert len(obs_refcodes) == (total_combined_events + total_missing_measured_but_refcode_present), \\\n",
    "#    \"Something is a foot: len(obs_refcodes) != (len(total_combined_events) + total_missing_measured_but_refcode_present)\"\n",
    "\n",
    "pprint(f\"Total number of all_source_mat_ids_from_sheets: {len(all_source_mat_ids_from_sheets)}\")\n",
    "\n",
    "missing_source_mat_ids =[]\n",
    "for source_mat_id in obs_refcodes: \n",
    "    # source_mat_ids are the keys in the refcode dict of the run_information\n",
    "    # print(f\"refcode from run_information: {refcode}\")\n",
    "    if source_mat_id not in all_source_mat_ids_from_sheets:\n",
    "        #print(f\"source_mat_id {source_mat_id} is missing from the sampling sheets\")\n",
    "        \n",
    "        # Get close matches to missing source_mat_id\n",
    "        matches = difflib.get_close_matches(source_mat_id, all_source_mat_ids_from_sheets, n=3)\n",
    "        missing_source_mat_ids.append([source_mat_id, matches])\n",
    "        \n",
    "print(f\"A total of {len(missing_source_mat_ids)} source_mat_ids \"\n",
    "      f\"in the batch 1 & 2 run information sheets are missing from the \"\n",
    "      f\"observatory sampling sheets\")\n",
    "\n",
    "###### CAUTION: THIS SHOULD BE ZERO! ################\n",
    "missing = False\n",
    "counter = 0\n",
    "#pprint(source_mat_ids_from_combined_events)\n",
    "for source_mat_id in source_mat_ids_from_combined_events: \n",
    "    # source_mat_ids are the keys in the refcode dict of the run_information\n",
    "    #print(f\"source_mat_id: {source_mat_id}\")\n",
    "    if source_mat_id not in obs_refcodes:\n",
    "        #print(f\"source_mat_id {source_mat_id} is missing from the run_information\")\n",
    "        counter += 1\n",
    "        missing = True\n",
    "if missing:\n",
    "    print(f\"A total of {counter} source_mat_ids \"\n",
    "          f\"in the sampling sheets that also have refcodes are missing from the \"\n",
    "          f\"Batch 1 & 2 run information sheets\")\n",
    "\n",
    "total = total_combined_events + len(missing_source_mat_ids)\n",
    "print(\n",
    "    f\"Total combined_events {total_combined_events} + {len(missing_source_mat_ids)} \"\n",
    "    f\"= {total} and should be equal to total number of refcodes assigned in the run information \"\n",
    "    f\"sheets {len(obs_refcodes)}\"\n",
    "    )\n",
    "\n",
    "#pprint(\"The missing source_mat_ids that are in the run information sheets are:\")\n",
    "#for missing in missing_source_mat_ids:\n",
    "#    join = \" \".join(missing[1])\n",
    "#    print(f\"Missing source_mat_id is {missing[0]} close matches are \\n\\t {join}\")\n",
    "\n",
    "#for missing in missing_source_mat_ids:\n",
    "#    print(missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fc6d98-8f88-4d4e-ac88-ea6690d00066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
